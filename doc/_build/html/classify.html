

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>nltk.classify &mdash; NLTK vr2 documentation</title>
    <link rel="stylesheet" href="_static/default.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '',
        VERSION:     'r2',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="NLTK vr2 documentation" href="index.html" />
    <link rel="up" title="API Documentation" href="api.html" />
    <link rel="next" title="nltk.corpus" href="corpus.html" />
    <link rel="prev" title="nltk.ccg" href="ccg.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="corpus.html" title="nltk.corpus"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="ccg.html" title="nltk.ccg"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">NLTK vr2 documentation</a> &raquo;</li>
          <li><a href="api.html" accesskey="U">API Documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="module-nltk.classify">
<span id="nltk-classify"></span><h1>nltk.classify<a class="headerlink" href="#module-nltk.classify" title="Permalink to this headline">¶</a></h1>
<p>Classes and interfaces for labeling tokens with category labels (or
X{class labels}).  Typically, labels are represented with strings
(such as C{&#8216;health&#8217;} or C{&#8216;sports&#8217;}).  Classifiers can be used to
perform a wide range of classification tasks.  For example,
classifiers can be used...</p>
<blockquote>
<div><ul class="simple">
<li>to classify documents by topic.</li>
<li>to classify ambiguous words by which word sense is intended.</li>
<li>to classify acoustic signals by which phoneme they represent.</li>
<li>to classify sentences by their author.</li>
</ul>
</div></blockquote>
<div class="section" id="features">
<h2>Features<a class="headerlink" href="#features" title="Permalink to this headline">¶</a></h2>
<p>In order to decide which category label is appropriate for a given
token, classifiers examine one or more &#8216;features&#8217; of the token.  These
X{features} are typically chosen by hand, and indicate which aspects
of the token are relevant to the classification decision.  For
example, a document classifier might use a separate feature for each
word, recording how often that word occured in the document.</p>
</div>
<div class="section" id="featuresets">
<h2>Featuresets<a class="headerlink" href="#featuresets" title="Permalink to this headline">¶</a></h2>
<p>The features describing a token are encoded using a X{featureset},
which is a dictionary that maps from X{feature names} to X{feature
values}.  Feature names are unique strings that indicate what aspect
of the token is encoded by the feature.  Examples include
C{&#8216;prevword&#8217;}, for a feature whose value is the previous word; and
C{&#8216;contains-word(library)&#8217;} for a feature that is true when a document
contains the word C{&#8216;library&#8217;}.  Feature values are typically
booleans, numbers, or strings, depending on which feature they
describe.</p>
<p>Featuresets are typically constructed using a X{feature detector}
(also known as a X{feature extractor}).  A feature detector is a
function that takes a token (and sometimes information about its
context) as its input, and returns a featureset describing that token.
For example, the following feature detector converts a document
(stored as a list of words) to a featureset describing the set of
words included in the document:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="c"># Define a feature detector function.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">document_features</span><span class="p">(</span><span class="n">document</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="nb">dict</span><span class="p">([(</span><span class="s">&#39;contains-word(</span><span class="si">%s</span><span class="s">)&#39;</span> <span class="o">%</span> <span class="n">w</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">document</span><span class="p">])</span>
</pre></div>
</div>
<p>Feature detectors are typically applied to each token before it is fed
to the classifier:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">Classify</span> <span class="n">each</span> <span class="n">Gutenberg</span> <span class="n">document</span><span class="o">.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="nb">file</span> <span class="ow">in</span> <span class="n">gutenberg</span><span class="o">.</span><span class="n">files</span><span class="p">():</span>
<span class="gp">... </span>    <span class="n">doc</span> <span class="o">=</span> <span class="n">gutenberg</span><span class="o">.</span><span class="n">tokenized</span><span class="p">(</span><span class="nb">file</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">print</span> <span class="n">doc_name</span><span class="p">,</span> <span class="n">classifier</span><span class="o">.</span><span class="n">classify</span><span class="p">(</span><span class="n">document_features</span><span class="p">(</span><span class="n">doc</span><span class="p">))</span>
</pre></div>
</div>
<p>The parameters that a feature detector expects will vary, depending on
the task and the needs of the feature detector.  For example, a
feature detector for word sense disambiguation (WSD) might take as its
input a sentence, and the index of a word that should be classified,
and return a featureset for that word.  The following feature detector
for WSD includes features describing the left and right contexts of
the target word:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">wsd_features</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">featureset</span> <span class="o">=</span> <span class="p">{}</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="o">-</span><span class="mi">3</span><span class="p">),</span> <span class="n">index</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">featureset</span><span class="p">[</span><span class="s">&#39;left-context(</span><span class="si">%s</span><span class="s">)&#39;</span> <span class="o">%</span> <span class="n">sentence</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">True</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">index</span><span class="o">+</span><span class="mi">3</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">))):</span>
<span class="gp">... </span>        <span class="n">featureset</span><span class="p">[</span><span class="s">&#39;right-context(</span><span class="si">%s</span><span class="s">)&#39;</span> <span class="o">%</span> <span class="n">sentence</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">True</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">featureset</span>
</pre></div>
</div>
</div>
<div class="section" id="training-classifiers">
<h2>Training Classifiers<a class="headerlink" href="#training-classifiers" title="Permalink to this headline">¶</a></h2>
<p>Most classifiers are built by training them on a list of hand-labeled
examples, known as the X{training set}.  Training sets are represented
as lists of C{(featuredict, label)} tuples.</p>
<dl class="class">
<dt id="nltk.classify.ClassifierI">
<em class="property">class </em><tt class="descclassname">nltk.classify.</tt><tt class="descname">ClassifierI</tt><a class="headerlink" href="#nltk.classify.ClassifierI" title="Permalink to this definition">¶</a></dt>
<dd><p>A processing interface for labeling tokens with a single category
label (or X{class}).  Labels are typically C{string}s or
C{integer}s, but can be any immutable type.  The set of labels
that the classifier chooses from must be fixed and finite.</p>
<dl class="docutils">
<dt>Subclasses must define:</dt>
<dd><ul class="first last simple">
<li>L{labels()}</li>
<li>either L{classify()} or L{batch_classify()} (or both)</li>
</ul>
</dd>
<dt>Subclasses may define:</dt>
<dd><ul class="first last simple">
<li>either L{prob_classify()} or L{batch_prob_classify()} (or both)</li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="nltk.classify.ClassifierI.batch_classify">
<tt class="descname">batch_classify</tt><big>(</big><em>featuresets</em><big>)</big><a class="headerlink" href="#nltk.classify.ClassifierI.batch_classify" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply L{self.classify()} to each element of C{featuresets}.  I.e.:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">classify</span><span class="p">(</span><span class="n">fs</span><span class="p">)</span> <span class="k">for</span> <span class="n">fs</span> <span class="ow">in</span> <span class="n">featuresets</span><span class="p">]</span>
</pre></div>
</div>
<p>&#64;rtype: C{list} of I{label}</p>
</dd></dl>

<dl class="method">
<dt id="nltk.classify.ClassifierI.batch_prob_classify">
<tt class="descname">batch_prob_classify</tt><big>(</big><em>featuresets</em><big>)</big><a class="headerlink" href="#nltk.classify.ClassifierI.batch_prob_classify" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply L{self.prob_classify()} to each element of C{featuresets}.  I.e.:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">prob_classify</span><span class="p">(</span><span class="n">fs</span><span class="p">)</span> <span class="k">for</span> <span class="n">fs</span> <span class="ow">in</span> <span class="n">featuresets</span><span class="p">]</span>
</pre></div>
</div>
<p>&#64;rtype: C{list} of L{ProbDistI &lt;nltk.probability.ProbDistI&gt;}</p>
</dd></dl>

<dl class="method">
<dt id="nltk.classify.ClassifierI.classify">
<tt class="descname">classify</tt><big>(</big><em>featureset</em><big>)</big><a class="headerlink" href="#nltk.classify.ClassifierI.classify" title="Permalink to this definition">¶</a></dt>
<dd><p>&#64;return: the most appropriate label for the given featureset.
&#64;rtype: label</p>
</dd></dl>

<dl class="method">
<dt id="nltk.classify.ClassifierI.labels">
<tt class="descname">labels</tt><big>(</big><big>)</big><a class="headerlink" href="#nltk.classify.ClassifierI.labels" title="Permalink to this definition">¶</a></dt>
<dd><p>&#64;return: the list of category labels used by this classifier.
&#64;rtype: C{list} of (immutable)</p>
</dd></dl>

<dl class="method">
<dt id="nltk.classify.ClassifierI.prob_classify">
<tt class="descname">prob_classify</tt><big>(</big><em>featureset</em><big>)</big><a class="headerlink" href="#nltk.classify.ClassifierI.prob_classify" title="Permalink to this definition">¶</a></dt>
<dd><dl class="docutils">
<dt>&#64;return: a probability distribution over labels for the given</dt>
<dd>featureset.</dd>
</dl>
<p>&#64;rtype: L{ProbDistI &lt;nltk.probability.ProbDistI&gt;}</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.classify.MultiClassifierI">
<em class="property">class </em><tt class="descclassname">nltk.classify.</tt><tt class="descname">MultiClassifierI</tt><a class="headerlink" href="#nltk.classify.MultiClassifierI" title="Permalink to this definition">¶</a></dt>
<dd><p>A processing interface for labeling tokens with zero or more
category labels (or X{labels}).  Labels are typically C{string}s
or C{integer}s, but can be any immutable type.  The set of labels
that the multi-classifier chooses from must be fixed and finite.</p>
<dl class="docutils">
<dt>Subclasses must define:</dt>
<dd><ul class="first last simple">
<li>L{labels()}</li>
<li>either L{classify()} or L{batch_classify()} (or both)</li>
</ul>
</dd>
<dt>Subclasses may define:</dt>
<dd><ul class="first last simple">
<li>either L{prob_classify()} or L{batch_prob_classify()} (or both)</li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="nltk.classify.MultiClassifierI.batch_classify">
<tt class="descname">batch_classify</tt><big>(</big><em>featuresets</em><big>)</big><a class="headerlink" href="#nltk.classify.MultiClassifierI.batch_classify" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply L{self.classify()} to each element of C{featuresets}.  I.e.:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">classify</span><span class="p">(</span><span class="n">fs</span><span class="p">)</span> <span class="k">for</span> <span class="n">fs</span> <span class="ow">in</span> <span class="n">featuresets</span><span class="p">]</span>
</pre></div>
</div>
<p>&#64;rtype: C{list} of (C{set} of I{label})</p>
</dd></dl>

<dl class="method">
<dt id="nltk.classify.MultiClassifierI.batch_prob_classify">
<tt class="descname">batch_prob_classify</tt><big>(</big><em>featuresets</em><big>)</big><a class="headerlink" href="#nltk.classify.MultiClassifierI.batch_prob_classify" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply L{self.prob_classify()} to each element of C{featuresets}.  I.e.:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">prob_classify</span><span class="p">(</span><span class="n">fs</span><span class="p">)</span> <span class="k">for</span> <span class="n">fs</span> <span class="ow">in</span> <span class="n">featuresets</span><span class="p">]</span>
</pre></div>
</div>
<p>&#64;rtype: C{list} of L{ProbDistI &lt;nltk.probability.ProbDistI&gt;}</p>
</dd></dl>

<dl class="method">
<dt id="nltk.classify.MultiClassifierI.classify">
<tt class="descname">classify</tt><big>(</big><em>featureset</em><big>)</big><a class="headerlink" href="#nltk.classify.MultiClassifierI.classify" title="Permalink to this definition">¶</a></dt>
<dd><p>&#64;return: the most appropriate set of labels for the given featureset.
&#64;rtype: C{set} of I{label}</p>
</dd></dl>

<dl class="method">
<dt id="nltk.classify.MultiClassifierI.labels">
<tt class="descname">labels</tt><big>(</big><big>)</big><a class="headerlink" href="#nltk.classify.MultiClassifierI.labels" title="Permalink to this definition">¶</a></dt>
<dd><p>&#64;return: the list of category labels used by this classifier.
&#64;rtype: C{list} of (immutable)</p>
</dd></dl>

<dl class="method">
<dt id="nltk.classify.MultiClassifierI.prob_classify">
<tt class="descname">prob_classify</tt><big>(</big><em>featureset</em><big>)</big><a class="headerlink" href="#nltk.classify.MultiClassifierI.prob_classify" title="Permalink to this definition">¶</a></dt>
<dd><dl class="docutils">
<dt>&#64;return: a probability distribution over sets of labels for the</dt>
<dd>given featureset.</dd>
</dl>
<p>&#64;rtype: L{ProbDistI &lt;nltk.probability.ProbDistI&gt;}</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.classify.NaiveBayesClassifier">
<em class="property">class </em><tt class="descclassname">nltk.classify.</tt><tt class="descname">NaiveBayesClassifier</tt><big>(</big><em>label_probdist</em>, <em>feature_probdist</em><big>)</big><a class="headerlink" href="#nltk.classify.NaiveBayesClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>A Naive Bayes classifier.  Naive Bayes classifiers are
paramaterized by two probability distributions:</p>
<blockquote>
<div><ul class="simple">
<li>P(label) gives the probability that an input will receive each
label, given no information about the input&#8217;s features.</li>
<li>P(fname=fval|label) gives the probability that a given feature
(fname) will receive a given value (fval), given that the
label (label).</li>
</ul>
</div></blockquote>
<p>If the classifier encounters an input with a feature that has
never been seen with any label, then rather than assigning a
probability of 0 to all labels, it will ignore that feature.</p>
<p>The feature value &#8216;None&#8217; is reserved for unseen feature values;
you generally should not use &#8216;None&#8217; as a feature value for one of
your own features.</p>
<dl class="method">
<dt id="nltk.classify.NaiveBayesClassifier.most_informative_features">
<tt class="descname">most_informative_features</tt><big>(</big><em>n=100</em><big>)</big><a class="headerlink" href="#nltk.classify.NaiveBayesClassifier.most_informative_features" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a list of the &#8216;most informative&#8217; features used by this
classifier.  For the purpose of this function, the
informativeness of a feature C{(fname,fval)} is equal to the
highest value of P(fname=fval|label), for any label, divided by
the lowest value of P(fname=fval|label), for any label:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="nb">max</span><span class="p">[</span> <span class="n">P</span><span class="p">(</span><span class="n">fname</span><span class="o">=</span><span class="n">fval</span><span class="o">|</span><span class="n">label1</span><span class="p">)</span> <span class="o">/</span> <span class="n">P</span><span class="p">(</span><span class="n">fname</span><span class="o">=</span><span class="n">fval</span><span class="o">|</span><span class="n">label2</span><span class="p">)</span> <span class="p">]</span>
</pre></div>
</div>
</dd></dl>

<dl class="staticmethod">
<dt id="nltk.classify.NaiveBayesClassifier.train">
<em class="property">static </em><tt class="descname">train</tt><big>(</big><em>labeled_featuresets</em>, <em>estimator=&lt;class 'nltk.probability.ELEProbDist'&gt;</em><big>)</big><a class="headerlink" href="#nltk.classify.NaiveBayesClassifier.train" title="Permalink to this definition">¶</a></dt>
<dd><dl class="docutils">
<dt>&#64;param labeled_featuresets: A list of classified featuresets,</dt>
<dd>i.e., a list of tuples C{(featureset, label)}.</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="nltk.classify.config_megam">
<tt class="descclassname">nltk.classify.</tt><tt class="descname">config_megam</tt><big>(</big><em>bin=None</em><big>)</big><a class="headerlink" href="#nltk.classify.config_megam" title="Permalink to this definition">¶</a></dt>
<dd><p>Configure NLTK&#8217;s interface to the C{megam} maxent optimization
package.</p>
<dl class="docutils">
<dt>&#64;param bin: The full path to the C{megam} binary.  If not specified,</dt>
<dd>then nltk will search the system for a C{megam} binary; and if
one is not found, it will raise a C{LookupError} exception.</dd>
</dl>
<p>&#64;type bin: C{string}</p>
</dd></dl>

<dl class="function">
<dt id="nltk.classify.rte_classifier">
<tt class="descclassname">nltk.classify.</tt><tt class="descname">rte_classifier</tt><big>(</big><em>trainer</em>, <em>features=&lt;function rte_features at 0x3c5f8c0&gt;</em><big>)</big><a class="headerlink" href="#nltk.classify.rte_classifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Classify RTEPairs</p>
</dd></dl>

<dl class="class">
<dt id="nltk.classify.RTEFeatureExtractor">
<em class="property">class </em><tt class="descclassname">nltk.classify.</tt><tt class="descname">RTEFeatureExtractor</tt><big>(</big><em>rtepair</em>, <em>stop=True</em>, <em>lemmatize=False</em><big>)</big><a class="headerlink" href="#nltk.classify.RTEFeatureExtractor" title="Permalink to this definition">¶</a></dt>
<dd><p>This builds a bag of words for both the text and the hypothesis after
throwing away some stopwords, then calculates overlap and difference.</p>
<dl class="method">
<dt id="nltk.classify.RTEFeatureExtractor.hyp_extra">
<tt class="descname">hyp_extra</tt><big>(</big><em>toktype</em>, <em>debug=True</em><big>)</big><a class="headerlink" href="#nltk.classify.RTEFeatureExtractor.hyp_extra" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the extraneous material in the hypothesis.</p>
<p>&#64;param toktype: distinguish Named Entities from ordinary words
&#64;type toktype: &#8216;ne&#8217; or &#8216;word&#8217;</p>
</dd></dl>

<dl class="method">
<dt id="nltk.classify.RTEFeatureExtractor.overlap">
<tt class="descname">overlap</tt><big>(</big><em>toktype</em>, <em>debug=False</em><big>)</big><a class="headerlink" href="#nltk.classify.RTEFeatureExtractor.overlap" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the overlap between text and hypothesis.</p>
<p>&#64;param toktype: distinguish Named Entities from ordinary words
&#64;type toktype: &#8216;ne&#8217; or &#8216;word&#8217;</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.classify.MaxentClassifier">
<em class="property">class </em><tt class="descclassname">nltk.classify.</tt><tt class="descname">MaxentClassifier</tt><big>(</big><em>encoding</em>, <em>weights</em>, <em>logarithmic=True</em><big>)</big><a class="headerlink" href="#nltk.classify.MaxentClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>A maximum entropy classifier (also known as a X{conditional
exponential classifier}).  This classifier is parameterized by a
set of X{weights}, which are used to combine the joint-features
that are generated from a featureset by an X{encoding}.  In
particular, the encoding maps each C{(featureset, label)} pair to
a vector.  The probability of each label is then computed using
the following equation:</p>
<div class="highlight-python"><pre>                          dotprod(weights, encode(fs,label))
prob(fs|label) = ---------------------------------------------------
                 sum(dotprod(weights, encode(fs,l)) for l in labels)</pre>
</div>
<p>Where C{dotprod} is the dot product:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">dotprod</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">x</span><span class="o">*</span><span class="n">y</span> <span class="k">for</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">))</span>
</pre></div>
</div>
<dl class="method">
<dt id="nltk.classify.MaxentClassifier.explain">
<tt class="descname">explain</tt><big>(</big><em>featureset</em>, <em>columns=4</em><big>)</big><a class="headerlink" href="#nltk.classify.MaxentClassifier.explain" title="Permalink to this definition">¶</a></dt>
<dd><p>Print a table showing the effect of each of the features in
the given feature set, and how they combine to determine the
probabilities of each label for that featureset.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.classify.MaxentClassifier.set_weights">
<tt class="descname">set_weights</tt><big>(</big><em>new_weights</em><big>)</big><a class="headerlink" href="#nltk.classify.MaxentClassifier.set_weights" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the feature weight vector for this classifier.  
&#64;param new_weights: The new feature weight vector.
&#64;type new_weights: C{list} of C{float}</p>
</dd></dl>

<dl class="method">
<dt id="nltk.classify.MaxentClassifier.show_most_informative_features">
<tt class="descname">show_most_informative_features</tt><big>(</big><em>n=10</em>, <em>show='all'</em><big>)</big><a class="headerlink" href="#nltk.classify.MaxentClassifier.show_most_informative_features" title="Permalink to this definition">¶</a></dt>
<dd><p>&#64;param show: all, neg, or pos (for negative-only or positive-only)</p>
</dd></dl>

<dl class="classmethod">
<dt id="nltk.classify.MaxentClassifier.train">
<em class="property">classmethod </em><tt class="descname">train</tt><big>(</big><em>train_toks</em>, <em>algorithm=None</em>, <em>trace=3</em>, <em>encoding=None</em>, <em>labels=None</em>, <em>sparse=True</em>, <em>gaussian_prior_sigma=0</em>, <em>**cutoffs</em><big>)</big><a class="headerlink" href="#nltk.classify.MaxentClassifier.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Train a new maxent classifier based on the given corpus of
training samples.  This classifier will have its weights
chosen to maximize entropy while remaining empirically
consistent with the training corpus.</p>
<p>&#64;rtype: L{MaxentClassifier}
&#64;return: The new maxent classifier</p>
<p>&#64;type train_toks: C{list}
&#64;param train_toks: Training data, represented as a list of</p>
<blockquote>
<div>pairs, the first member of which is a featureset,
and the second of which is a classification label.</div></blockquote>
<p>&#64;type algorithm: C{str}
&#64;param algorithm: A case-insensitive string, specifying which</p>
<blockquote>
<div><p>algorithm should be used to train the classifier.  The
following algorithms are currently available.</p>
<blockquote>
<div><ul>
<li><p class="first">Iterative Scaling Methods
- C{&#8216;GIS&#8217;}: Generalized Iterative Scaling
- C{&#8216;IIS&#8217;}: Improved Iterative Scaling</p>
</li>
<li><p class="first">Optimization Methods (require C{scipy})
- C{&#8216;CG&#8217;}: Conjugate gradient
- C{&#8216;BFGS&#8217;}: Broyden-Fletcher-Goldfarb-Shanno algorithm
- C{&#8216;Powell&#8217;}: Powell agorithm
- C{&#8216;LBFGSB&#8217;}: A limited-memory variant of the BFGS algorithm
- C{&#8216;Nelder-Mead&#8217;}: The Nelder-Mead algorithm</p>
</li>
<li><p class="first">External Libraries
- C{&#8216;megam&#8217;}: LM-BFGS algorithm, with training performed</p>
<blockquote>
<div><p>by an U{megam &lt;<a class="reference external" href="http://www.cs.utah.edu/~hal/megam/">http://www.cs.utah.edu/~hal/megam/</a>&gt;}.
(requires that C{megam} be installed.)</p>
</div></blockquote>
</li>
</ul>
</div></blockquote>
<p>The default algorithm is C{&#8216;CG&#8217;} if C{&#8216;scipy&#8217;} is
installed; and C{&#8216;iis&#8217;} otherwise.</p>
</div></blockquote>
<p>&#64;type trace: C{int}
&#64;param trace: The level of diagnostic tracing output to produce.</p>
<blockquote>
<div>Higher values produce more verbose output.</div></blockquote>
<p>&#64;type encoding: L{MaxentFeatureEncodingI}
&#64;param encoding: A feature encoding, used to convert featuresets</p>
<blockquote>
<div>into feature vectors.  If none is specified, then a
L{BinaryMaxentFeatureEncoding} will be built based on the
features that are attested in the training corpus.</div></blockquote>
<p>&#64;type labels: C{list} of C{str}
&#64;param labels: The set of possible labels.  If none is given, then</p>
<blockquote>
<div>the set of all labels attested in the training data will be
used instead.</div></blockquote>
<dl class="docutils">
<dt>&#64;param sparse: If true, then use sparse matrices instead of</dt>
<dd>dense matrices.  Currently, this is only supported by
the scipy (optimization method) algorithms.  For other
algorithms, its value is ignored.</dd>
<dt>&#64;param gaussian_prior_sigma: The sigma value for a gaussian</dt>
<dd>prior on model weights.  Currently, this is supported by
the scipy (optimization method) algorithms and C{megam}.
For other algorithms, its value is ignored.</dd>
<dt>&#64;param cutoffs: Arguments specifying various conditions under</dt>
<dd><p class="first">which the training should be halted.  (Some of the cutoff
conditions are not supported by some algorithms.)</p>
<blockquote class="last">
<div><ul class="simple">
<li>C{max_iter=v}: Terminate after C{v} iterations.</li>
<li>C{min_ll=v}: Terminate after the negative average
log-likelihood drops under C{v}.</li>
<li>C{min_lldelta=v}: Terminate if a single iteration improves
log likelihood by less than C{v}.</li>
<li>C{tolerance=v}: Terminate a scipy optimization method when
improvement drops below a tolerance level C{v}.  The
exact meaning of this tolerance depends on the scipy
algorithm used.  See C{scipy} documentation for more
info.  Default values: 1e-3 for CG, 1e-5 for LBFGSB,
and 1e-4 for other algorithms.  I{(C{scipy} only)}</li>
</ul>
</div></blockquote>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nltk.classify.MaxentClassifier.weights">
<tt class="descname">weights</tt><big>(</big><big>)</big><a class="headerlink" href="#nltk.classify.MaxentClassifier.weights" title="Permalink to this definition">¶</a></dt>
<dd><p>&#64;return: The feature weight vector for this classifier.
&#64;rtype: C{list} of C{float}</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.classify.BinaryMaxentFeatureEncoding">
<em class="property">class </em><tt class="descclassname">nltk.classify.</tt><tt class="descname">BinaryMaxentFeatureEncoding</tt><big>(</big><em>labels</em>, <em>mapping</em>, <em>unseen_features=False</em>, <em>alwayson_features=False</em><big>)</big><a class="headerlink" href="#nltk.classify.BinaryMaxentFeatureEncoding" title="Permalink to this definition">¶</a></dt>
<dd><p>A feature encoding that generates vectors containing a binary
joint-features of the form:</p>
<div class="highlight-python"><pre>joint_feat(fs, l) = { 1 if (fs[fname] == fval) and (l == label)
                    {
                    { 0 otherwise</pre>
</div>
<p>Where C{fname} is the name of an input-feature, C{fval} is a value
for that input-feature, and C{label} is a label.</p>
<p>Typically, these features are constructed based on a training
corpus, using the L{train()} method.  This method will create one
feature for each combination of C{fname}, C{fval}, and C{label}
that occurs at least once in the training corpus.</p>
<p>The C{unseen_features} parameter can be used to add X{unseen-value
features}, which are used whenever an input feature has a value
that was not encountered in the training corpus.  These features
have the form:</p>
<div class="highlight-python"><pre>joint_feat(fs, l) = { 1 if is_unseen(fname, fs[fname])
                    {      and l == label
                    {
                    { 0 otherwise</pre>
</div>
<p>Where C{is_unseen(fname, fval)} is true if the encoding does not
contain any joint features that are true when C{fs[fname]==fval}.</p>
<p>The C{alwayson_features} parameter can be used to add X{always-on
features}, which have the form:</p>
<div class="highlight-python"><pre>joint_feat(fs, l) = { 1 if (l == label)
                    {
                    { 0 otherwise</pre>
</div>
<p>These always-on features allow the maxent model to directly model
the prior probabilities of each label.</p>
<dl class="classmethod">
<dt id="nltk.classify.BinaryMaxentFeatureEncoding.train">
<em class="property">classmethod </em><tt class="descname">train</tt><big>(</big><em>train_toks</em>, <em>count_cutoff=0</em>, <em>labels=None</em>, <em>**options</em><big>)</big><a class="headerlink" href="#nltk.classify.BinaryMaxentFeatureEncoding.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Construct and return new feature encoding, based on a given
training corpus C{train_toks}.  See the L{class description
&lt;BinaryMaxentFeatureEncoding&gt;} for a description of the
joint-features that will be included in this encoding.</p>
<p>&#64;type train_toks: C{list} of C{tuples} of (C{dict}, C{str})
&#64;param train_toks: Training data, represented as a list of</p>
<blockquote>
<div>pairs, the first member of which is a feature dictionary,
and the second of which is a classification label.</div></blockquote>
<p>&#64;type count_cutoff: C{int}
&#64;param count_cutoff: A cutoff value that is used to discard</p>
<blockquote>
<div>rare joint-features.  If a joint-feature&#8217;s value is 1
fewer than C{count_cutoff} times in the training corpus,
then that joint-feature is not included in the generated
encoding.</div></blockquote>
<p>&#64;type labels: C{list}
&#64;param labels: A list of labels that should be used by the</p>
<blockquote>
<div>classifier.  If not specified, then the set of labels
attested in C{train_toks} will be used.</div></blockquote>
<dl class="docutils">
<dt>&#64;param options: Extra parameters for the constructor, such as</dt>
<dd>C{unseen_features} and C{alwayson_features}.</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="attribute">
<dt id="nltk.classify.ConditionalExponentialClassifier">
<tt class="descclassname">nltk.classify.</tt><tt class="descname">ConditionalExponentialClassifier</tt><a class="headerlink" href="#nltk.classify.ConditionalExponentialClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <a class="reference internal" href="#nltk.classify.MaxentClassifier" title="nltk.classify.MaxentClassifier"><tt class="xref py py-class docutils literal"><span class="pre">MaxentClassifier</span></tt></a></p>
</dd></dl>

</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">nltk.classify</a><ul>
<li><a class="reference internal" href="#features">Features</a></li>
<li><a class="reference internal" href="#featuresets">Featuresets</a></li>
<li><a class="reference internal" href="#training-classifiers">Training Classifiers</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="ccg.html"
                        title="previous chapter">nltk.ccg</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="corpus.html"
                        title="next chapter">nltk.corpus</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="_sources/classify.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="corpus.html" title="nltk.corpus"
             >next</a> |</li>
        <li class="right" >
          <a href="ccg.html" title="nltk.ccg"
             >previous</a> |</li>
        <li><a href="index.html">NLTK vr2 documentation</a> &raquo;</li>
          <li><a href="api.html" >API Documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2011, Steven Bird, Ewan Klein, Edward Loper.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.0.8.
    </div>
  </body>
</html>