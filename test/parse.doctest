--------------------------------------------------------------------------------
Unit tests for the utility parse functions.
--------------------------------------------------------------------------------

    >>> from nltk_lite import tokenize

--------------------------------------------------------------------------------
Unit tests for the CFG (Context Free Grammar) class
--------------------------------------------------------------------------------

    >>> from nltk_lite.parse import cfg

    >>> nt1 = cfg.Nonterminal('NP')
    >>> nt2 = cfg.Nonterminal('VP')

    >>> nt1.symbol()
    'NP'

    >>> nt1 == cfg.Nonterminal('NP')
    True

    >>> nt1 == nt2
    False

    >>> S, NP, VP, PP = cfg.nonterminals('S, NP, VP, PP')
    >>> N, V, P, DT = cfg.nonterminals('N, V, P, DT') 

    >>> prod1 = cfg.Production(S, [NP, VP])
    >>> prod2 = cfg.Production(NP, [DT, NP])

    >>> prod1.lhs()
    <S>

    >>> prod1.rhs()
    (<NP>, <VP>)

    >>> prod1 == cfg.Production(S, [NP, VP])
    True

    >>> prod1 == prod2
    False

    >>> grammar = cfg.parse_cfg("""
    ... S -> NP VP
    ... PP -> P NP
    ... NP -> DT N | N PP | DT N PP
    ... VP -> V NP | V PP | V NP PP
    ... DT -> 'a'
    ... DT -> 'the'
    ... N -> 'cat'
    ... N -> 'dog'
    ... N -> 'rug'
    ... V -> 'chased'
    ... V -> 'sat'
    ... P -> 'in'
    ... P -> 'on'
    ... """)

--------------------------------------------------------------------------------
Unit tests for the rd (Recursive Descent Parser) class
--------------------------------------------------------------------------------


Create and run a recursive descent parser over both a syntactically ambiguous
and unambiguous sentence.

    >>> from nltk_lite.parse import RecursiveDescent
    >>> rd = RecursiveDescent(grammar)

    >>> sentence1 = list(tokenize.whitespace('the cat chased the dog'))
    >>> sentence2 = list(tokenize.whitespace('the cat chased the dog on the rug'))

    >>> rd.get_parse_list(sentence1)
    [('S': ('NP': ('DT': 'the') ('N': 'cat')) ('VP': ('V': 'chased') ('NP': ('DT': 'the') ('N': 'dog'))))]

    >>> rd.get_parse_list(sentence2)
    [('S': ('NP': ('DT': 'the') ('N': 'cat')) ('VP': ('V': 'chased') ('NP': ('DT': 'the') ('N': 'dog') ('PP': ('P': 'on') ('NP': ('DT': 'the') ('N': 'rug')))))), ('S': ('NP': ('DT': 'the') ('N': 'cat')) ('VP': ('V': 'chased') ('NP': ('DT': 'the') ('N': 'dog')) ('PP': ('P': 'on') ('NP': ('DT': 'the') ('N': 'rug')))))]

--------------------------------------------------------------------------------
Unit tests for the sr (Shift Reduce Parser) class
--------------------------------------------------------------------------------

Create and run a shift reduce parser over both a syntactically ambiguous
and unambiguous sentence. Note that unlike the recursive descent parser, one
and only one parse is ever returned.

    >>> from nltk_lite.parse import ShiftReduce
    >>> sr = ShiftReduce(grammar)

    >>> sentence1 = list(tokenize.whitespace('the cat chased the dog'))
    >>> sentence2 = list(tokenize.whitespace('the cat chased the dog on the rug'))

    >>> sr.get_parse_list(sentence1)
    [('S': ('NP': ('DT': 'the') ('N': 'cat')) ('VP': ('V': 'chased') ('NP': ('DT': 'the') ('N': 'dog'))))]

The shift reduce parser uses heuristics to decide what to do when there are
multiple possible shift or reduce operations available - for the supplied
grammar clearly the wrong operation is selected.

    >>> sr.get_parse_list(sentence2)
    []

--------------------------------------------------------------------------------
Unit tests for the Chart parser class
--------------------------------------------------------------------------------

    >>> from nltk_lite.parse import ChartParse, EarleyChartParse, BU_STRATEGY, TD_STRATEGY

Define a grammar.

    >>> grammar = cfg.parse_cfg("""
    ... S -> NP VP
    ... PP -> P NP
    ... NP -> DT N | N | NP PP
    ... VP -> V NP |  VP PP
    ... DT -> 'a' | 'the'
    ... N -> 'Marc' | 'man' | 'park' | 'telescope'
    ... V -> 'has' | 'saw'
    ... P -> 'in'
    ... """)

Some example sentences, one ambiguous and one unambiguous.

    >>> sentence1 = list(tokenize.whitespace("Marc has a telescope"))
    >>> sentence2 = list(tokenize.whitespace("Marc saw a man in the park"))

Create a chart parser. First give it a bottom-up strategy.

    >>> parser = ChartParse(grammar, BU_STRATEGY)

    >>> parser.get_parse_list(sentence1)
    [('S': ('NP': ('N': 'Marc')) ('VP': ('V': 'has') ('NP': ('DT': 'a') ('N': 'telescope'))))]

    >>> parser.get_parse_list(sentence2)
    [('S': ('NP': ('N': 'Marc')) ('VP': ('V': 'saw') ('NP': ('NP': ('DT': 'a') ('N': 'man')) ('PP': ('P': 'in') ('NP': ('DT': 'the') ('N': 'park')))))), ('S': ('NP': ('N': 'Marc')) ('VP': ('VP': ('V': 'saw') ('NP': ('DT': 'a') ('N': 'man'))) ('PP': ('P': 'in') ('NP': ('DT': 'the') ('N': 'park')))))]

Redefine the chart parser to use a top-down strategy.

    >>> parser = ChartParse(grammar, TD_STRATEGY)

    >>> parser.get_parse_list(sentence1)
    [('S': ('NP': ('N': 'Marc')) ('VP': ('V': 'has') ('NP': ('DT': 'a') ('N': 'telescope'))))]

    >>> parser.get_parse_list(sentence2)
    [('S': ('NP': ('N': 'Marc')) ('VP': ('V': 'saw') ('NP': ('NP': ('DT': 'a') ('N': 'man')) ('PP': ('P': 'in') ('NP': ('DT': 'the') ('N': 'park')))))), ('S': ('NP': ('N': 'Marc')) ('VP': ('VP': ('V': 'saw') ('NP': ('DT': 'a') ('N': 'man'))) ('PP': ('P': 'in') ('NP': ('DT': 'the') ('N': 'park')))))]

Create and test the Earley variant chart parser. This requires a lexicon.

    >>> syntactic_productions = [
    ... cfg.Production(S, [NP, VP]),
    ... cfg.Production(PP, [P, NP]),
    ... cfg.Production(NP, [DT, N]),
    ... cfg.Production(NP, [N]),
    ... cfg.Production(NP, [NP, PP]),
    ... cfg.Production(VP, [V, NP]),
    ... cfg.Production(VP, [VP, PP])
    ... ]

    >>> grammar = cfg.Grammar(S, syntactic_productions)

    >>> lexical_productions = [
    ... cfg.Production(DT, ['a']),
    ... cfg.Production(DT, ['the']),
    ... cfg.Production(N, ['man']),
    ... cfg.Production(N, ['Marc']),
    ... cfg.Production(N, ['park']),
    ... cfg.Production(N, ['telescope']),
    ... cfg.Production(V, ['has']),
    ... cfg.Production(V, ['saw']),
    ... cfg.Production(P, ['in'])
    ... ]

    >>> lexicon = {}
    >>> for production in lexical_productions:
    ...     lexicon.setdefault(production.rhs()[0], []).append(production.lhs())

    >>> parser = EarleyChartParse(grammar, lexicon)

    >>> parser.get_parse_list(sentence1)
    [('S': ('NP': ('N': 'Marc')) ('VP': ('V': 'has') ('NP': ('DT': 'a') ('N': 'telescope'))))]

    >>> parser.get_parse_list(sentence2)
    [('S': ('NP': ('N': 'Marc')) ('VP': ('V': 'saw') ('NP': ('NP': ('DT': 'a') ('N': 'man')) ('PP': ('P': 'in') ('NP': ('DT': 'the') ('N': 'park')))))), ('S': ('NP': ('N': 'Marc')) ('VP': ('VP': ('V': 'saw') ('NP': ('DT': 'a') ('N': 'man'))) ('PP': ('P': 'in') ('NP': ('DT': 'the') ('N': 'park')))))]

------------------
GrammarFile class
------------------

    >>> from nltk_lite.parse import GrammarFile
    >>> g = GrammarFile.read_file('../parse/test.cfg')
    >>> print g.earley_grammar()
    Grammar with 3 productions (start state = S[])
        S[ sem = ApplicationExpression('?vp', '?subj') ] -> NP[ sem = ?subj ] VP[ sem = ?vp ]
        VP[ sem = ApplicationExpression('?v', '?obj') ] -> V[ sem = ?v ] NP[ sem = ?obj ]
        VP[ sem = ?v ] -> V[ sem = ?v ]
    >>> print g.earley_lexicon()
    {'I': [NP[sem='i']], 'sleeps': [V[sem='\\x.(sleeps x)', tns=pres]], 'like': [V[sem='\\x y.(like x y)', tns=pres]], 'Kim': [NP[sem='kim']]}
    >>> p = g.earley_parser()

--------------------------------------------------------------------------------
Unit tests for the Probabilistic CFG class
--------------------------------------------------------------------------------

    >>> from nltk_lite.corpora import treebank
    >>> from itertools import islice
    >>> from nltk_lite.parse import pcfg

Create a set of probabilistic CFG productions.

    >>> grammar = pcfg.parse_pcfg("""
    ... A -> B B [.3] | C B C [.7]
    ... B -> B D [.5] | C [.5]
    ... C -> 'a' [.1] | 'b' [0.9]
    ... D -> 'b' [1.0]
    ... """)
    >>> prod = grammar.productions()[0]
    >>> prod
    A -> B B [0.3]

    >>> prod.lhs()
    <A>

    >>> prod.rhs()
    (<B>, <B>)

    >>> prod.prob()
    0.29999999999999999

    >>> grammar.start()
    <A>

    >>> grammar.productions()
    [A -> B B [0.3], A -> C B C [0.7], B -> B D [0.5], B -> C [0.5], C -> 'a' [0.1], C -> 'b' [0.9], D -> 'b' [1.0]]

Induce some productions using parsed Treebank data.

    >>> productions = []
    >>> for tree in islice(treebank.parsed(), 3):
    ...     productions += tree.productions()

    >>> grammar = pcfg.induce(S, productions)
    >>> grammar
    <Grammar with 71 productions>

    >>> grammar.productions()[:5]
    [PP -> IN NP [1.0], NNP -> 'Nov.' [0.0714285714286], NNP -> 'Agnew' [0.0714285714286], JJ -> 'industrial' [0.142857142857], NP -> CD NNS [0.133333333333]]

--------------------------------------------------------------------------------
Unit tests for the Probabilistic Chart Parse classes
--------------------------------------------------------------------------------

    >>> tokens = list(tokenize.whitespace("Jack saw Bob with my cookie"))
    >>> grammar = pcfg.toy2
    >>> print grammar
    Grammar with 23 productions (start state = S)
        S -> NP VP [1.0]
        VP -> V NP [0.59]
        VP -> V [0.4]
        VP -> VP PP [0.01]
        NP -> Det N [0.41]
        NP -> Name [0.28]
        NP -> NP PP [0.31]
        PP -> P NP [1.0]
        V -> 'saw' [0.21]
        V -> 'ate' [0.51]
        V -> 'ran' [0.28]
        N -> 'boy' [0.11]
        N -> 'cookie' [0.12]
        N -> 'table' [0.13]
        N -> 'telescope' [0.14]
        N -> 'hill' [0.5]
        Name -> 'Jack' [0.52]
        Name -> 'Bob' [0.48]
        P -> 'with' [0.61]
        P -> 'under' [0.39]
        Det -> 'the' [0.41]
        Det -> 'a' [0.31]
        Det -> 'my' [0.28]

Create several parsers using different queuing strategies and show the
resulting parses.

    >>> from nltk_lite.parse import pchart
    
    >>> parser = pchart.InsideParse(grammar)
    >>> parser.get_parse_list(tokens)
    [('S': ('NP': ('Name': 'Jack') (p=0.52)) (p=0.1456) ('VP': ('V': 'saw') (p=0.21) ('NP': ('NP': ('Name': 'Bob') (p=0.48)) (p=0.1344) ('PP': ('P': 'with') (p=0.61) ('NP': ('Det': 'my') (p=0.28) ('N': 'cookie') (p=0.12)) (p=0.013776)) (p=0.00840336)) (p=0.00035011759104)) (p=4.33795695299e-05)) (p=6.31606532355e-06), ('S': ('NP': ('Name': 'Jack') (p=0.52)) (p=0.1456) ('VP': ('VP': ('V': 'saw') (p=0.21) ('NP': ('Name': 'Bob') (p=0.48)) (p=0.1344)) (p=0.01665216) ('PP': ('P': 'with') (p=0.61) ('NP': ('Det': 'my') (p=0.28) ('N': 'cookie') (p=0.12)) (p=0.013776)) (p=0.00840336)) (p=1.39934095258e-06)) (p=2.03744042695e-07)]

    >>> parser = pchart.RandomParse(grammar)
    >>> parser.get_parse_list(tokens)
    [('S': ('NP': ('Name': 'Jack') (p=0.52)) (p=0.1456) ('VP': ('V': 'saw') (p=0.21) ('NP': ('NP': ('Name': 'Bob') (p=0.48)) (p=0.1344) ('PP': ('P': 'with') (p=0.61) ('NP': ('Det': 'my') (p=0.28) ('N': 'cookie') (p=0.12)) (p=0.013776)) (p=0.00840336)) (p=0.00035011759104)) (p=4.33795695299e-05)) (p=6.31606532355e-06), ('S': ('NP': ('Name': 'Jack') (p=0.52)) (p=0.1456) ('VP': ('VP': ('V': 'saw') (p=0.21) ('NP': ('Name': 'Bob') (p=0.48)) (p=0.1344)) (p=0.01665216) ('PP': ('P': 'with') (p=0.61) ('NP': ('Det': 'my') (p=0.28) ('N': 'cookie') (p=0.12)) (p=0.013776)) (p=0.00840336)) (p=1.39934095258e-06)) (p=2.03744042695e-07)]

    >>> parser = pchart.UnsortedParse(grammar)
    >>> parser.get_parse_list(tokens)
    [('S': ('NP': ('Name': 'Jack') (p=0.52)) (p=0.1456) ('VP': ('V': 'saw') (p=0.21) ('NP': ('NP': ('Name': 'Bob') (p=0.48)) (p=0.1344) ('PP': ('P': 'with') (p=0.61) ('NP': ('Det': 'my') (p=0.28) ('N': 'cookie') (p=0.12)) (p=0.013776)) (p=0.00840336)) (p=0.00035011759104)) (p=4.33795695299e-05)) (p=6.31606532355e-06), ('S': ('NP': ('Name': 'Jack') (p=0.52)) (p=0.1456) ('VP': ('VP': ('V': 'saw') (p=0.21) ('NP': ('Name': 'Bob') (p=0.48)) (p=0.1344)) (p=0.01665216) ('PP': ('P': 'with') (p=0.61) ('NP': ('Det': 'my') (p=0.28) ('N': 'cookie') (p=0.12)) (p=0.013776)) (p=0.00840336)) (p=1.39934095258e-06)) (p=2.03744042695e-07)]

    >>> parser = pchart.LongestParse(grammar)
    >>> parser.get_parse_list(tokens)
    [('S': ('NP': ('Name': 'Jack') (p=0.52)) (p=0.1456) ('VP': ('V': 'saw') (p=0.21) ('NP': ('NP': ('Name': 'Bob') (p=0.48)) (p=0.1344) ('PP': ('P': 'with') (p=0.61) ('NP': ('Det': 'my') (p=0.28) ('N': 'cookie') (p=0.12)) (p=0.013776)) (p=0.00840336)) (p=0.00035011759104)) (p=4.33795695299e-05)) (p=6.31606532355e-06), ('S': ('NP': ('Name': 'Jack') (p=0.52)) (p=0.1456) ('VP': ('VP': ('V': 'saw') (p=0.21) ('NP': ('Name': 'Bob') (p=0.48)) (p=0.1344)) (p=0.01665216) ('PP': ('P': 'with') (p=0.61) ('NP': ('Det': 'my') (p=0.28) ('N': 'cookie') (p=0.12)) (p=0.013776)) (p=0.00840336)) (p=1.39934095258e-06)) (p=2.03744042695e-07)]

    >>> parser = pchart.InsideParse(grammar, beam_size = len(tokens)+1)
    >>> parser.get_parse_list(tokens)
    []

----------------------------------------
Unit tests for the Viterbi Parse classes
----------------------------------------

    >>> from nltk_lite.parse import ViterbiParse
    >>> tokens = list(tokenize.whitespace("Jack saw Bob with my cookie"))
    >>> grammar = pcfg.toy2

Parse the tokenized sentence.

    >>> parser = ViterbiParse(grammar)
    >>> parser.get_parse_list(tokens)
    [('S': ('NP': ('Name': 'Jack') (p=0.52)) (p=0.1456) ('VP': ('V': 'saw') (p=0.21) ('NP': ('NP': ('Name': 'Bob') (p=0.48)) (p=0.1344) ('PP': ('P': 'with') (p=0.61) ('NP': ('Det': 'my') (p=0.28) ('N': 'cookie') (p=0.12)) (p=0.013776)) (p=0.00840336)) (p=0.00035011759104)) (p=4.33795695299e-05)) (p=6.31606532355e-06)]

-----------------------------
Unit tests for the Tree class
-----------------------------

C{Trees} are used to encode hierarchical structures.  Each C{Tree}
object encodes a single grouping in the hierarchy.  A C{Tree} is
actually a specialized subclass of C{list} that:

  - Adds a new attribute, the C{node} value.

  - Conceptually divides its children into two groups: C{Tree}
    children (called \"subtrees\") and non-C{Tree} children (called
    \"leaves\").

  - Adds several tree-specific operations.

  - Adds a new tree-specific indexing scheme.

Trees are created from a node and a list (or any iterable) of
children:

    >>> from nltk_lite.parse import Tree
    >>> Tree(1, [2, 3, 4])
    (1: 2 3 4)
    >>> Tree('S', [Tree('NP', ['I']), 
    ...            Tree('VP', [Tree('V', ['saw']),
    ...                        Tree('NP', ['him'])])])
    ('S': ('NP': 'I') ('VP': ('V': 'saw') ('NP': 'him')))

One exception to \"any iterable\": in order to avoid confusion,
strings are I{not} accepted as children lists:

    >>> Tree('NP', 'Bob')
    Traceback (most recent call last):
      ...
    TypeError: children should be a list, not a string

A single level can contain both leaves and subtrees:

    >>> Tree(1, [2, Tree(3, [4]), 5])
    (1: 2 (3: 4) 5)

Some trees to run tests on:

    >>> dp1 = Tree('dp', [Tree('d', ['the']), Tree('np', ['dog'])])
    >>> dp2 = Tree('dp', [Tree('d', ['the']), Tree('np', ['cat'])])
    >>> vp = Tree('vp', [Tree('v', ['chased']), dp2])
    >>> tree = Tree('s', [dp1, vp])
    >>> print tree
    (s:
      (dp: (d: 'the') (np: 'dog'))
      (vp: (v: 'chased') (dp: (d: 'the') (np: 'cat'))))

The node value is stored using the C{node} attribute:

    >>> dp1.node, dp2.node, vp.node, tree.node
    ('dp', 'dp', 'vp', 's')

This attribute can be modified directly:

    >>> dp1.node = 'np'
    >>> dp2.node = 'np'
    >>> print tree
    (s:
      (np: (d: 'the') (np: 'dog'))
      (vp: (v: 'chased') (np: (d: 'the') (np: 'cat'))))

Children can be accessed with indexing, just as with normal lists:

    >>> tree[0]
    ('np': ('d': 'the') ('np': 'dog'))
    >>> tree[1][1]
    ('np': ('d': 'the') ('np': 'cat'))

Children can be modified directly, as well:

    >>> tree[0], tree[1][1] = tree[1][1], tree[0]
    >>> print tree
    (s:
      (np: (d: 'the') (np: 'cat'))
      (vp: (v: 'chased') (np: (d: 'the') (np: 'dog'))))

The C{Tree} class adds a new method of indexing, using tuples rather
than ints.  C{t[a,b,c]} is equivalant to C{t[a][b][c]}.  The sequence
C{(a,b,c)} is called a \"tree path\".

    >>> print tree[1,1][0]
    (d: 'the')

    >>> # Switch the cat & dog back the way they were.
    >>> tree[1,1], tree[0] = tree[0], tree[1,1]
    >>> print tree
    (s:
      (np: (d: 'the') (np: 'dog'))
      (vp: (v: 'chased') (np: (d: 'the') (np: 'cat'))))

    >>> path = (1,1,1,0)
    >>> print tree[path]
    cat

The length of a tree is the number of children it has.

    >>> len(tree), len(dp1), len(dp2), len(dp1[0])
    (2, 2, 2, 1)
    >>> len(Tree('x', []))
    0

The current repr for trees looks like this:

    >>> print repr(tree)
    ('s': ('np': ('d': 'the') ('np': 'dog')) ('vp': ('v': 'chased') ('np': ('d': 'the') ('np': 'cat'))))

But this might change in the future.  Similarly, the current str looks
like:

    >>> print str(tree)
    (s:
      (np: (d: 'the') (np: 'dog'))
      (vp: (v: 'chased') (np: (d: 'the') (np: 'cat'))))
    
(Note line-wrapping).  But the details of both reprs might change.

The C{leaves} method returns a list of a trees leaves:

    >>> print tree.leaves()
    ['the', 'dog', 'chased', 'the', 'cat']

The C{height} method returns the height of the tree.  A tree with no
children is considered to have a height of 1; a tree with only
children is considered to have a height of 2; and any other tree's
height is one plus the maximum of its children's heights:

    >>> print tree.height()
    5
    >>> print tree[1,1,1].height()
    2
    >>> print tree[0].height()
    3

The C{treepositions} method returns a list of the tree positions of
subtrees and leaves in a tree.  By default, it gives the position of
every tree, subtree, and leaf, in prefix order:

    >>> print tree.treepositions()
    [(), (0,), (0, 0), (0, 0, 0), (0, 1), (0, 1, 0), (1,), (1, 0), (1, 0, 0), (1, 1), (1, 1, 0), (1, 1, 0, 0), (1, 1, 1), (1, 1, 1, 0)]

The order can also be specified explicitly.  Four orders are currently
supported:

    # Prefix order
    >>> print tree.treepositions('preorder')
    [(), (0,), (0, 0), (0, 0, 0), (0, 1), (0, 1, 0), (1,), (1, 0), (1, 0, 0), (1, 1), (1, 1, 0), (1, 1, 0, 0), (1, 1, 1), (1, 1, 1, 0)]

    # Postfix order
    >>> print tree.treepositions('postorder')
    [(0, 0, 0), (0, 0), (0, 1, 0), (0, 1), (0,), (1, 0, 0), (1, 0), (1, 1, 0, 0), (1, 1, 0), (1, 1, 1, 0), (1, 1, 1), (1, 1), (1,), ()]
    
    # Both prefix & postfix order (subtrees listed twice, leaves once)
    >>> print tree.treepositions('bothorder')
    [(), (0,), (0, 0), (0, 0, 0), (0, 0), (0, 1), (0, 1, 0), (0, 1), (0,), (1,), (1, 0), (1, 0, 0), (1, 0), (1, 1), (1, 1, 0), (1, 1, 0, 0), (1, 1, 0), (1, 1, 1), (1, 1, 1, 0), (1, 1, 1), (1, 1), (1,), ()]
    
    # Leaves only (in order)
    >>> print tree.treepositions('leaves')
    [(0, 0, 0), (0, 1, 0), (1, 0, 0), (1, 1, 0, 0), (1, 1, 1, 0)]

C{treepositions} can be useful for modifying a tree.  For example, we
could upper-case all leaves with:

    >>> for pos in tree.treepositions('leaves'):
    ...     tree[pos] = tree[pos].upper()
    >>> print tree
    (s:
      (np: (d: 'THE') (np: 'DOG'))
      (vp: (v: 'CHASED') (np: (d: 'THE') (np: 'CAT'))))

In addition to C{str} and C{repr}, several methods exist to convert a
tree object to one of several standard tree encodings:

    >>> print tree.pp_treebank()
    (s (np (d THE) (np DOG)) (vp (v CHASED) (np (d THE) (np CAT))))
    >>> print tree.pp_latex_qtree()
    \Tree [.s
            [.np [.d 'THE' ] [.np 'DOG' ] ]
            [.vp [.v 'CHASED' ] [.np [.d 'THE' ] [.np 'CAT' ] ] ] ]

Trees can be parsed from treebank strings with the C{bracket_parse} method:

    >>> from nltk_lite.parse import bracket_parse
    >>> tree2 = bracket_parse('(S (NP I) (VP (V enjoyed) (NP my cookie)))')
    >>> print tree2
    (S: (NP: 'I') (VP: (V: 'enjoyed') (NP: 'my' 'cookie')))

Trees can be compared for equality:

    >>> tree == bracket_parse(tree.pp_treebank())
    True
    >>> tree2 == bracket_parse(tree2.pp_treebank())
    True
    >>> tree == tree2
    False
    >>> tree == bracket_parse(tree2.pp_treebank())
    False
    >>> tree2 == bracket_parse(tree.pp_treebank())
    False

    >>> tree != bracket_parse(tree.pp_treebank())
    False
    >>> tree2 != bracket_parse(tree2.pp_treebank())
    False
    >>> tree != tree2
    True
    >>> tree != bracket_parse(tree2.pp_treebank())
    True
    >>> tree2 != bracket_parse(tree.pp_treebank())
    True
    
    >>> tree < tree2 or tree > tree2
    True

--------------------------------------------------------------------------------
Unit tests for the TreeTransformation class
--------------------------------------------------------------------------------

    >>> from copy import deepcopy
    >>> from nltk_lite.parse.treetransforms import *

    >>> sentence = "(TOP (S (S (VP (VBN Turned) (ADVP (RB loose)) (PP (IN in) (NP (NP (NNP Shane) (NNP Longman) (POS 's)) (NN trading) (NN room))))) (, ,) (NP (DT the) (NN yuppie) (NNS dealers)) (VP (AUX do) (NP (NP (RB little)) (ADJP (RB right)))) (. .)))"

    >>> tree = bracket_parse(sentence)
    >>> tree
    ('TOP': ('S': ('S': ('VP': ('VBN': 'Turned') ('ADVP': ('RB': 'loose')) ('PP': ('IN': 'in') ('NP': ('NP': ('NNP': 'Shane') ('NNP': 'Longman') ('POS': "'s")) ('NN': 'trading') ('NN': 'room'))))) (',': ',') ('NP': ('DT': 'the') ('NN': 'yuppie') ('NNS': 'dealers')) ('VP': ('AUX': 'do') ('NP': ('NP': ('RB': 'little')) ('ADJP': ('RB': 'right')))) ('.': '.')))

Make a copy of the original tree and collapse the subtrees with only one child

    >>> collapsedTree = deepcopy(tree)
    >>> collapseUnary(collapsedTree)
    >>> collapsedTree
    ('TOP': ('S': ('S+VP': ('VBN': 'Turned') ('ADVP': ('RB': 'loose')) ('PP': ('IN': 'in') ('NP': ('NP': ('NNP': 'Shane') ('NNP': 'Longman') ('POS': "'s")) ('NN': 'trading') ('NN': 'room')))) (',': ',') ('NP': ('DT': 'the') ('NN': 'yuppie') ('NNS': 'dealers')) ('VP': ('AUX': 'do') ('NP': ('NP': ('RB': 'little')) ('ADJP': ('RB': 'right')))) ('.': '.')))

    >>> collapsedTree2 = deepcopy(tree)
    >>> collapseUnary(collapsedTree2, collapsePOS=True, collapseRoot=True)
    >>> collapsedTree2
    ('TOP+S': ('S+VP': ('VBN': 'Turned') ('ADVP+RB': 'loose') ('PP': ('IN': 'in') ('NP': ('NP': ('NNP': 'Shane') ('NNP': 'Longman') ('POS': "'s")) ('NN': 'trading') ('NN': 'room')))) (',': ',') ('NP': ('DT': 'the') ('NN': 'yuppie') ('NNS': 'dealers')) ('VP': ('AUX': 'do') ('NP': ('NP+RB': 'little') ('ADJP+RB': 'right'))) ('.': '.'))

Convert the tree to Chomsky Normal Form i.e. each subtree has either two
subtree children or a single leaf value. This conversion can be performed
using either left- or right-factoring.

    >>> cnfTree = deepcopy(collapsedTree)
    >>> chomskyNormalForm(cnfTree, factor='left')
    >>> cnfTree
    ('TOP': ('S': ('S|<S+VP-,-NP-VP>': ('S|<S+VP-,-NP>': ('S|<S+VP-,>': ('S+VP': ('S+VP|<VBN-ADVP>': ('VBN': 'Turned') ('ADVP': ('RB': 'loose'))) ('PP': ('IN': 'in') ('NP': ('NP|<NP-NN>': ('NP': ('NP|<NNP-NNP>': ('NNP': 'Shane') ('NNP': 'Longman')) ('POS': "'s")) ('NN': 'trading')) ('NN': 'room')))) (',': ',')) ('NP': ('NP|<DT-NN>': ('DT': 'the') ('NN': 'yuppie')) ('NNS': 'dealers'))) ('VP': ('AUX': 'do') ('NP': ('NP': ('RB': 'little')) ('ADJP': ('RB': 'right'))))) ('.': '.')))

    >>> cnfTree = deepcopy(collapsedTree)
    >>> chomskyNormalForm(cnfTree, factor='right')
    >>> cnfTree
    ('TOP': ('S': ('S+VP': ('VBN': 'Turned') ('S+VP|<ADVP-PP>': ('ADVP': ('RB': 'loose')) ('PP': ('IN': 'in') ('NP': ('NP': ('NNP': 'Shane') ('NP|<NNP-POS>': ('NNP': 'Longman') ('POS': "'s"))) ('NP|<NN-NN>': ('NN': 'trading') ('NN': 'room')))))) ('S|<,-NP-VP-.>': (',': ',') ('S|<NP-VP-.>': ('NP': ('DT': 'the') ('NP|<NN-NNS>': ('NN': 'yuppie') ('NNS': 'dealers'))) ('S|<VP-.>': ('VP': ('AUX': 'do') ('NP': ('NP': ('RB': 'little')) ('ADJP': ('RB': 'right')))) ('.': '.'))))))

Employ some Markov smoothing to make the artificial node labels a bit more
readable. See the treetransforms.py documentation for more details.

    >>> markovTree = deepcopy(collapsedTree)
    >>> chomskyNormalForm(markovTree, horzMarkov=2, vertMarkov=1)
    >>> markovTree
    ('TOP': ('S^<TOP>': ('S+VP^<S>': ('VBN': 'Turned') ('S+VP|<ADVP-PP>^<S>': ('ADVP^<S+VP>': ('RB': 'loose')) ('PP^<S+VP>': ('IN': 'in') ('NP^<PP>': ('NP^<NP>': ('NNP': 'Shane') ('NP|<NNP-POS>^<NP>': ('NNP': 'Longman') ('POS': "'s"))) ('NP|<NN-NN>^<PP>': ('NN': 'trading') ('NN': 'room')))))) ('S|<,-NP>^<TOP>': (',': ',') ('S|<NP-VP>^<TOP>': ('NP^<S>': ('DT': 'the') ('NP|<NN-NNS>^<S>': ('NN': 'yuppie') ('NNS': 'dealers'))) ('S|<VP-.>^<TOP>': ('VP^<S>': ('AUX': 'do') ('NP^<VP>': ('NP^<NP>': ('RB': 'little')) ('ADJP^<NP>': ('RB': 'right')))) ('.': '.'))))))

Convert the transformed tree back to its original form

    >>> unChomskyNormalForm(markovTree)
    >>> tree == markovTree
    True

--------------------------------------------------------------------------------
Unit tests for the Category class
--------------------------------------------------------------------------------

    >>> from nltk_lite.parse import Category, GrammarCategory
    >>> Category(pos='n', agr=Category(number='pl', gender='f'))
    [agr=[gender='f', number='pl'], pos='n']

    >>> GrammarCategory.parse('VP[+fin]/NP[+pl]')
    VP[+fin]/NP[+pl]

--------------------------------------------------------------------------------
Unit tests for FeatureStructure classes
--------------------------------------------------------------------------------

    >>> from nltk_lite.featurestructure import *

Copying from self to other.

    >>> fs1 = FeatureStructure(number='singular')
    >>> fs2 = fs1.unify(FeatureStructure())
    >>> repr(fs2)
    "[number='singular']"

Copying from other to self

    >>> fs1 = FeatureStructure()
    >>> fs2 = fs1.unify(FeatureStructure(number='singular'))
    >>> repr(fs2)
    "[number='singular']"

Cross copying

    >>> fs1 = FeatureStructure(number='singular')
    >>> fs2 = fs1.unify(FeatureStructure(person=3))
    >>> repr(fs2)
    "[number='singular', person=3]"

Merging a nested structure

    >>> fs1 = FeatureStructure.parse('[A=[B=b]]')
    >>> fs2 = FeatureStructure.parse('[A=[C=c]]')
    >>> fs3 = fs1.unify(fs2)
    >>> repr(fs3)
    "[A=[B='b', C='c']]"

A basic case of reentrant unification

    >>> fs1 = FeatureStructure.parse('[A=(1)[B=b], E=[F->(1)]]')
    >>> fs2 = FeatureStructure.parse("[A=[C='c'], E=[F=[D='d']]]")
    >>> fs3 = fs1.unify(fs2)
    >>> repr(fs3)
    "[A=(1)[B='b', C='c', D='d'], E=[F->(1)]]"

Try unifying both ways

    >>> fs3 = fs2.unify(fs1)
    >>> repr(fs3)
    "[A=(1)[B='b', C='c', D='d'], E=[F->(1)]]"

More than 2 paths to a value

    >>> fs1 = FeatureStructure.parse("[a=[],b=[],c=[],d=[]]")
    >>> fs2 = FeatureStructure.parse('[a=(1)[], b->(1), c->(1), d->(1)]')
    >>> fs3 = fs1.unify(fs2)
    >>> repr(fs3)
    '[a=(1)[], b->(1), c->(1), d->(1)]'

fs1[a] gets unified with itself

    >>> fs1 = FeatureStructure.parse('[x=(1)[], y->(1)]')
    >>> fs2 = FeatureStructure.parse('[x=(1)[], y->(1)]')
    >>> fs3 = fs1.unify(fs2)

Bound variables should get forwarded appropriately

    >>> fs1 = FeatureStructure.parse('[A=(1)[X=x], B->(1), C=?cvar, D=?dvar]')
    >>> fs2y = FeatureStructure(Y='y')
    >>> fs2z = FeatureStructure(Z='z')
    >>> fs2 = FeatureStructure.parse('[A=(1)[Y=y], B=(2)[Z=z], C->(1), D->(2)]')
    >>> fs3 = fs1.unify(fs2)
    >>> repr(fs3)
    "[A=(1)[X='x', Y='y', Z='z'], B->(1), C->(1), D->(1)]"

Create a cyclic structure via unification.

    >>> fs1 = FeatureStructure.parse('[F=(1)[], G->(1)]')
    >>> fs2 = FeatureStructure.parse('[F=[H=(2)[]], G->(2)]')
    >>> fs3 = fs1.unify(fs2)

Check that we got the value right.

    >>> repr(fs3)
    '[F=(1)[H->(1)], G->(1)]'

Check that we got the cyclicity right.

    >>> fs3['F'] is fs3['G']
    True

    >>> fs3['F'] is fs3['G', 'H']
    True

    >>> fs3['F'] is fs3['G', 'H', 'H']
    True

    >>> fs3['F'] is fs3[('G',)+(('H',)*10)]
    True

Create a cyclic structure with variables.

    >>> x = FeatureVariable('x')
    >>> fs1 = FeatureStructure(F=FeatureStructure(H=x))
    >>> fs2 = FeatureStructure(F=x)
    >>> fs3 = fs1.unify(fs2)

Check that we got the value right.

    >>> repr(fs3)
    '[F=(1)[H->(1)]]'

Check that we got the cyclicity right.

    >>> fs3['F'] is fs3['F','H']
    True

    >>> fs3['F'] is fs3['F','H','H']
    True

    >>> fs3['F'] is fs3[('F',)+(('H',)*10)]
    True

Cyclic structure as LHS

    >>> fs4 = FeatureStructure.parse('[F=[H=[H=[H=(1)[]]]], K->(1)]')
    >>> fs5 = fs3.unify(fs4)
    >>> repr(fs5)
    '[F=(1)[H->(1)], K->(1)]'

Cyclic structure as RHS

    >>> fs6 = fs4.unify(fs3)
    >>> repr(fs6)
    '[F=(1)[H->(1)], K->(1)]'

Variable bindings should preserve reentrance.

    >>> bindings = FeatureBindings()
    >>> fs1 = FeatureStructure.parse("[a=?x]")
    >>> fs2 = fs1.unify(FeatureStructure.parse("[a=[]]"), bindings)
    >>> fs3 = fs2.unify(FeatureStructure.parse("[b=?x]"), bindings)
    >>> repr(fs3)
    '[a=(1)[], b->(1)]'

Aliased variable tests

    >>> fs1 = FeatureStructure.parse("[a=?x, b=?x]")
    >>> fs2 = fs1.unify(FeatureStructure.parse("[b=?y, c=?y]"))
    >>> repr(fs2)
    '[a=?x, b=?<x=y>, c=?y]'

    >>> fs3 = fs2.unify(FeatureStructure.parse("[a=1]"))
    >>> repr(fs3)
    '[a=1, b=1, c=1]'

    >>> fs1 = FeatureStructure.parse("[a=1]")
    >>> fs2 = FeatureStructure.parse("[a=?x, b=?x]")
    >>> fs3 = fs2.unify(fs1)
    >>> repr(fs3)
    '[a=1, b=1]'

--------------------------------------------------------------------------------
Unit tests for the Feature Chart Parser classes
--------------------------------------------------------------------------------

    >>> S = GrammarCategory.parse('S')
    >>> VP = GrammarCategory.parse('VP')
    >>> NP = GrammarCategory.parse('NP')
    >>> PP = GrammarCategory.parse('PP')
    >>> V = GrammarCategory.parse('V')
    >>> N = GrammarCategory.parse('N')
    >>> P = GrammarCategory.parse('P')
    >>> Name = GrammarCategory.parse('Name')
    >>> Det = GrammarCategory.parse('Det')
    >>> DetSg = GrammarCategory.parse('Det[-pl]')
    >>> DetPl = GrammarCategory.parse('Det[+pl]')
    >>> NSg = GrammarCategory.parse('N[-pl]')
    >>> NPl = GrammarCategory.parse('N[+pl]')

Define some grammatical productions.

    >>> grammatical_productions = [
    ...     cfg.Production(S, (NP, VP)),  cfg.Production(PP, (P, NP)),
    ...     cfg.Production(NP, (NP, PP)),
    ...     cfg.Production(VP, (VP, PP)), cfg.Production(VP, (V, NP)),
    ...     cfg.Production(VP, (V,)), cfg.Production(NP, (DetPl, NPl)),
    ...     cfg.Production(NP, (DetSg, NSg))]

Define some lexical productions.

    >>> lexical_productions = [
    ...     cfg.Production(NP, ('John',)), cfg.Production(NP, ('I',)),
    ...     cfg.Production(Det, ('the',)), cfg.Production(Det, ('my',)),
    ...     cfg.Production(Det, ('a',)),
    ...     cfg.Production(NSg, ('dog',)),   cfg.Production(NSg, ('cookie',)),
    ...     cfg.Production(V, ('ate',)),  cfg.Production(V, ('saw',)),
    ...     cfg.Production(P, ('with',)), cfg.Production(P, ('under',))]
    
    >>> earley_grammar = cfg.Grammar(S, grammatical_productions)
    >>> earley_lexicon = {}

    >>> for prod in lexical_productions:
    ...     earley_lexicon.setdefault(prod.rhs()[0].upper(), []).append(prod.lhs())

    >>> tokens = list(tokenize.whitespace('I saw John with a dog with my cookie'))
    >>> cp = FeatureEarleyChartParse(earley_grammar, earley_lexicon)
    >>> trees = cp.get_parse_list(tokens)
    >>> for tree in trees:
    ...     print tree
    ([INIT][]:
      (S[]:
        (NP[]: 'I')
        (VP[]:
          (VP[]:
            (VP[]: (V[]: 'saw') (NP[]: 'John'))
            (PP[]:
              (P[]: 'with')
              (NP[]: (Det[]: 'a') (N[ pl  = False ]: 'dog'))))
          (PP[]:
            (P[]: 'with')
            (NP[]: (Det[]: 'my') (N[ pl  = False ]: 'cookie'))))))
    ([INIT][]:
      (S[]:
        (NP[]: 'I')
        (VP[]:
          (VP[]: (V[]: 'saw') (NP[]: 'John'))
          (PP[]:
            (P[]: 'with')
            (NP[]:
              (NP[]: (Det[]: 'a') (N[ pl  = False ]: 'dog'))
              (PP[]:
                (P[]: 'with')
                (NP[]: (Det[]: 'my') (N[ pl  = False ]: 'cookie'))))))))
    ([INIT][]:
      (S[]:
        (NP[]: 'I')
        (VP[]:
          (VP[]:
            (V[]: 'saw')
            (NP[]:
              (NP[]: 'John')
              (PP[]:
                (P[]: 'with')
                (NP[]: (Det[]: 'a') (N[ pl  = False ]: 'dog')))))
          (PP[]:
            (P[]: 'with')
            (NP[]: (Det[]: 'my') (N[ pl  = False ]: 'cookie'))))))
    ([INIT][]:
      (S[]:
        (NP[]: 'I')
        (VP[]:
          (V[]: 'saw')
          (NP[]:
            (NP[]:
              (NP[]: 'John')
              (PP[]:
                (P[]: 'with')
                (NP[]: (Det[]: 'a') (N[ pl  = False ]: 'dog'))))
            (PP[]:
              (P[]: 'with')
              (NP[]: (Det[]: 'my') (N[ pl  = False ]: 'cookie')))))))
    ([INIT][]:
      (S[]:
        (NP[]: 'I')
        (VP[]:
          (V[]: 'saw')
          (NP[]:
            (NP[]: 'John')
            (PP[]:
              (P[]: 'with')
              (NP[]:
                (NP[]: (Det[]: 'a') (N[ pl  = False ]: 'dog'))
                (PP[]:
                  (P[]: 'with')
                  (NP[]: (Det[]: 'my') (N[ pl  = False ]: 'cookie')))))))))

