In general, when we are trying to develop even a very small grammar,
it is convenient to put the rules in a file where they can be edited,
tested and revised. Assuming we have saved feat0cfg_ as a file named
``'feat0.cfg'``, the function ``GrammarFile.read_file()`` allows us to
read the grammar into NLTK, ready for use in parsing.

    >>> from parse import ncategory
    >>> g = ncategory.GrammarFile.read_file('feat0.cfg')
    >>> import tokenize
    >>> sent = 'Kim likes children'
    >>> tokens = list(tokenize.whitespace(sent))
    >>> tokens 
    ['Kim', 'likes', 'children']
    >>> cp = g.earley_parser(trace=2)
    >>> trees = cp.get_parse_list(tokens)
              |.K.l.c.|
    Processing queue 0
    Predictor |> . . .| S -> * NP[NUM=?n] VP[NUM=?n] {} 
    Predictor |> . . .| NP[NUM=?n] -> * N[NUM=?n] {} 
    Predictor |> . . .| NP[NUM=?n] -> * PropN[NUM=?n] {} 
    Predictor |> . . .| NP[NUM=?n] -> * Det[NUM=?n] N[NUM=?n] {} 
    Predictor |> . . .| NP[NUM='pl'] -> * N[NUM='pl'] {} 
    Processing queue 1
    Scanner   |[-] . .| PropN[NUM='sg'] -> Kim * {} 
    Completer |[-] . .| NP[NUM='sg'] -> PropN[NUM='sg'] * {'n': 'sg'} 
    Completer |[-> . .| S -> NP[NUM='sg'] * VP[NUM='sg'] {'n': 'sg'} 
    Predictor |. > . .| VP[NUM=?n, TENSE=?t] -> * IV[NUM=?n, TENSE=?t] {} 
    Predictor |. > . .| VP[NUM=?n, TENSE=?t] -> * TV[NUM=?n, TENSE=?t] NP {} 
    Processing queue 2
    Scanner   |. [-] .| TV[NUM='sg', TENSE='pres'] -> likes * {} 
    Completer |. [-> .| VP[NUM='sg', TENSE='pres'] -> TV[NUM='sg', TENSE='pres'] * NP {'t': 'pres', 'n': 'sg'} 
    Predictor |. . > .| NP[NUM=?n] -> * N[NUM=?n] {} 
    Predictor |. . > .| NP[NUM=?n] -> * PropN[NUM=?n] {} 
    Predictor |. . > .| NP[NUM=?n] -> * Det[NUM=?n] N[NUM=?n] {} 
    Predictor |. . > .| NP[NUM='pl'] -> * N[NUM='pl'] {} 
    Processing queue 3
    Scanner   |. . [-]| N[NUM='pl'] -> children * {} 
    Completer |. . [-]| NP[NUM='pl'] -> N[NUM='pl'] * {'n': 'pl'} 
    Completer |. [---]| VP[NUM='sg', TENSE='pres'] -> TV[NUM='sg', TENSE='pres'] NP * {'t': 'pres', 'n': 'sg'} 
    Completer |[=====]| S -> NP[NUM='sg'] VP[NUM='sg'] * {'n': 'sg'} 
    Completer |[=====]| [INIT] -> S * {} 
    >>> for tree in trees: print tree
    ([INIT]:
      (S:
        (NP[NUM='sg']: (PropN[NUM='sg']: 'Kim'))
        (VP[NUM='sg', TENSE='pres']:
          (TV[NUM='sg', TENSE='pres']: 'likes')
          (NP[NUM='pl']: (N[NUM='pl']: 'children')))))	

Feature structures in NLTK are ... Atomic feature values can be strings or
integers.

    >>> fs1 = dict(TENSE='past', NUM='sg') 
    >>> print fs1
    {'NUM': 'sg', 'TENSE': 'past'}

We can think of a feature structure as being like a Python dictionary,
and access its values by indexing in the usual way.

    >>> fs1 = dict(PER=3, NUM='pl', GND='fem')
    >>> print fs1['GND']
    fem

We can also define feature structures which have complex values, as
discussed earlier.

    >>> fs2 = dict(POS='N', AGR=fs1)
    >>> print fs2
    {'AGR': {'NUM': 'pl', 'GND': 'fem', 'PER': 3}, 'POS': 'N'}
    >>> print fs2['AGR']
    {'NUM': 'pl', 'GND': 'fem', 'PER': 3}
    >>> print fs2['AGR']['PER']
    3

An alternative method of specifying feature structures in NLTK is to
use the ``load`` method of ``yaml``. This gives us the
facility to use square bracket notation for embedding one feature
structure within another.

Representing dictionaries in YAML form is useful for making feature
structures readable:
    
    >>> from parse.featurelite import *
    >>> f1 = yaml.load("NUMBER: SINGULAR")
    >>> f2 = yaml.load("PERSON: 3")
    >>> print show(unify(f1, f2))
    NUMBER: SINGULAR
    PERSON: 3

    >>> f1 = yaml.load('''
    ... A:
    ...   B: b
    ...   D: d
    ... ''')
    >>> f2 = yaml.load('''
    ... A:
    ...   C: c
    ...   D: d
    ... ''')
    >>> print show(unify(f1, f2))
    A:
      B: b
      C: c
      D: d


Feature Structures as Graphs
----------------------------

Feature structures are not inherently tied to linguistic objects; they are
general purpose structures for representing knowledge. For example, we
could encode information about a person in a feature structure:

    >>> person01 = yaml.load('''
    ... NAME: 'Lee'
    ... TELNO: '01 27 86 42 96'
    ... AGE: 33
    ... ''')
    >>> print show(person01)
    AGE: 33
    NAME: Lee
    TELNO: 01 27 86 42 96

There are a number of notations for representing reentrancy in
matrix-style representations of feature structures. In NLTK, we adopt
the following convention: the first occurrence of a shared feature structure 
is prefixed with an integer in parentheses, such as ``(1)``, and any
subsequent reference to that structure uses the notation
``->(1)``, as shown below.

    >>> fs=yaml.load("""
    ... NAME: 'Lee'
    ... ADDRESS: &1
    ...   NUMBER: 74
    ...   STREET: 'rue Pascal'
    ... SPOUSE:
    ...   NAME: 'Kim'
    ...   ADDRESS: *1
    ... """)
    >>> print show(fs)
    ADDRESS: &id001
      NUMBER: 74
      STREET: rue Pascal
    NAME: Lee
    SPOUSE:
      ADDRESS: *id001
      NAME: Kim

There can be any number of tags within a single feature structure.

    >>> fs3 = yaml.load("""
    ... A: 'a'
    ... B: &1
    ...   C: 'c'
    ... D: *1
    ... E: *1
    ... """)
    >>> print show(fs3)
    A: a
    B: &id001
      C: c
    D: *id001
    E: *id001


    >>> fs1 = yaml.load("""
    ... NUMBER: 74
    ... STREET: 'rue Pascal'
    ... """)
    >>> fs2 = yaml.load("CITY: Paris")
    >>> print show(unify(fs1, fs2))
    CITY: Paris
    NUMBER: 74
    STREET: rue Pascal

Unification is symmetric:

    >>> unify(fs1, fs2) == unify(fs2, fs1)
    True

Unification is commutative:

    >>> fs3 = yaml.load("TELNO: 01 27 86 42 96")
    >>> unify(unify(fs1, fs2), fs3) == unify(fs1, unify(fs2, fs3))
    True

Unification between `FS`:math:\ :subscript:`0` and `FS`:math:\
:subscript:`1` will fail if the two feature structures share a path |pi|,
but the value of |pi| in `FS`:math:\ :subscript:`0` is a distinct
atom from the value of |pi| in `FS`:math:\ :subscript:`1`. In NLTK,
this is implemented by setting the result of unification to be
``None``.

    >>> fs0 = dict(A='a')
    >>> fs1 = dict(A='b')
    >>> unify(fs0, fs1)
    Traceback (most recent call last):
    ...
    UnificationFailure

Now, if we look at how unification interacts with structure-sharing,
things become really interesting.

    >>> fs0 = yaml.load("""
    ... NAME: Lee
    ... ADDRESS: 
    ...   NUMBER: 74 
    ...   STREET: 'rue Pascal'
    ... SPOUSE:  
    ...   NAME: Kim
    ...   ADDRESS: 
    ...     NUMBER: 74
    ...     STREET: 'rue Pascal'
    ... """)
    >>> print show(fs0)
    ADDRESS:
      NUMBER: 74
      STREET: rue Pascal
    NAME: Lee
    SPOUSE:
      ADDRESS:
        NUMBER: 74
        STREET: rue Pascal
      NAME: Kim

    >>> fs1 = yaml.load("""
    ... SPOUSE: 
    ...   ADDRESS: 
    ...     CITY: Paris
    ... """)
    >>> print show(unify(fs0, fs1))
    ADDRESS:
      NUMBER: 74
      STREET: rue Pascal
    NAME: Lee
    SPOUSE:
      ADDRESS:
        CITY: Paris
        NUMBER: 74
        STREET: rue Pascal
      NAME: Kim

    >>> fs0 = yaml.load("""
    ... NAME: Lee
    ... ADDRESS: &1
    ...   NUMBER: 74 
    ...   STREET: 'rue Pascal'
    ... SPOUSE:  
    ...   NAME: Kim
    ...   ADDRESS: *1
    ... """)
    >>> print show(fs0)
    ADDRESS: &id001
      NUMBER: 74
      STREET: rue Pascal
    NAME: Lee
    SPOUSE:
      ADDRESS: *id001
      NAME: Kim

    >>> print show(unify(fs0, fs1))
    ADDRESS: &id001
      CITY: Paris
      NUMBER: 74
      STREET: rue Pascal
    NAME: Lee
    SPOUSE:
      ADDRESS: *id001
      NAME: Kim

    >>> fs1 = yaml.load("""
    ... ADDRESS1: 
    ...   NUMBER: 74
    ...   STREET: 'rue Pascal'
    ... """)

    >>> fs2 = yaml.load("""
    ... ADDRESS1: ?x
    ... ADDRESS2: ?x
    ... """)

    >>> print show(unify(fs1, fs2))
    ADDRESS1: &id001
      NUMBER: 74
      STREET: rue Pascal
    ADDRESS2: *id001

    >>> from parse import ncategory
    >>> g = ncategory.GrammarFile.read_file('feat1.cfg')
    >>> import tokenize
    >>> sent = 'who do you claim that you like'
    >>> tokens = list(tokenize.whitespace(sent))
    >>> cp = g.earley_parser(trace=0)
    >>> trees = cp.get_parse_list(tokens)
    >>> for tree in trees: print tree
    ([INIT]:
      (S[-INV]:
        (NP[+WH]: 'who')
        (S[+INV]/NP:
          (V[+AUX, SUBCAT=3]: 'do')
          (NP[-WH]: 'you')
          (VP/NP:
            (V[-AUX, SUBCAT=2]: 'claim')
            (S-BAR/NP:
              (Comp: 'that')
              (S[-INV]/NP:
                (NP[-WH]: 'you')
                (VP/NP: (V[-AUX, SUBCAT=1]: 'like') (NP/NP: ))))))))

Try a more succinct interface to the parser:

    >>> from parse.featurechart import *
    >>> cp = load_earley('feat1.cfg', trace=0)
    >>> trees = cp.parse(tokens)
    >>> for tree in trees: print tree
    (S[-INV]:
      (NP[+WH]: 'who')
      (S[+INV]/NP:
        (V[+AUX, SUBCAT=3]: 'do')
        (NP[-WH]: 'you')
        (VP/NP:
          (V[-AUX, SUBCAT=2]: 'claim')
          (S-BAR/NP:
            (Comp: 'that')
            (S[-INV]/NP:
              (NP[-WH]: 'you')
              (VP/NP: (V[-AUX, SUBCAT=1]: 'like') (NP/NP: )))))))

Let's load a German grammar:

    >>> g = ncategory.GrammarFile.read_file('german1.cfg')
    >>> sent = 'die katze sieht dem hund'
    >>> tokens = list(tokenize.whitespace(sent))
    >>> cp = g.earley_parser(trace=0)
    >>> trees = cp.get_parse_list(tokens)
    >>> for tree in trees: print tree
