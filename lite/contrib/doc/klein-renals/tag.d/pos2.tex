\usepackage{amsmath}
\usepackage{color}
\title{Part-of-speech tagging (2)}
\author{Steve Renals \newline \mbox{ }s.renals@ed.ac.uk\mbox{ }}
\date{ICL --- 20 October 2005}

\mode<presentation>{\newcommand{\myquote}[1]{\textcolor{blue}{#1}}}
\mode<article>{\newcommand{\myquote}[1]{\textsf{#1}}}

\mode<presentation>{\newcommand{\myemphquote}[1]{\textcolor{red}{#1}}}
\mode<article>{\newcommand{\myemphquote}[1]{\textsf{\slshape#1}}}


\begin{document}

\frame{\titlepage}



\mode<article>{\section[Outline]{ICL/PoS Tagging 2/2005--10--20}}
\mode<presentation>{
  \section[Outline]{}
}

\frame{\tableofcontents}

\section{Approaches to POS Tagging}

\begin{frame}
  \mode<presentation>{\frametitle{Approaches to POS Tagging}}
  \begin{itemize}
  \item Rule-based (briefly discussed in Jurafsky and Martin,, sec
    8.4)
  \item N-gram / HMM based tagging (eg TnT)
  \item Transformation-based learning (the Brill tagger)
  \end{itemize}
\end{frame}

\section{n-gram based tagging}

\begin{frame}
  \mode<presentation>{\frametitle{n-gram based tagging}}
  \begin{itemize}
  \item<1-> Also referred to as hidden Markov model (HMM) tagging
  \item<1-> Basic idea:  Choose the tag  that maximises:
    \[ P(\mbox{word} | \mbox{tag}) \cdot 
       P(\mbox{tag} | \mbox{previous n tags}) \]
       
  \item<2-> For a bigram model the best tag at position $i$ is:
    \[ t_i = \arg\max_{t_j} P(w_i|t_j) P(t_j | t_{i-1}) \]
    Assuming that you know the previous tag, $t_{i-1}$.
  \item<3-> Interpretation: choose the tag $t_i$ that is most likely to
    \alert{generate} word $w_i$ given that the previous tag was
    $t_{i-1}$ 
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Example (J+M, p304)}
  {\small
  \myquote{Secretariat/NNP is/VBZ expected/VBZ to/TO} \myemphquote{race/VB}
  \myquote{tomorrow/NN} 

  \medskip
  \myquote{People/NNS continue/VBP to/TO inquire/VB the/DT
    reason/NN for/IN the/DT}
  \myemphquote{race/NN} \myquote{for/IN outer/JJ space/NN}}

  \begin{itemize}
  \item<2-> ``race'' is a verb in the first, a noun in the second.
  \item<3-> Assume that race is the only untagged word, so we can assume
    the tags of the others.
  \item<4-> Probabilities of ``race'' being a verb, or race being a noun
    in the first example:
    \begin{align*}
      P(\mbox{race is }VB) &= P(VB|TO) P(\mbox{race}|VB) \\
      P(\mbox{race is }NN) &= P(NN|TO) P(\mbox{race}|NN) 
    \end{align*}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Example (continued)}
  \begin{align*}
    P(NN|TO) &= 0.021 \\
    P(VB|TO) &= 0.34 \\
    \\
    P(\mbox{race}|NN) &= 0.00041\\
    P(\mbox{race}|VB) &= 0.00003\\
    \\
    P(\mbox{race is }VB) &= P(VB|TO) P(\mbox{race}|VB) \\
    &= 0.34 \times 0.00003 = 0.00001 \\
    P(\mbox{race is }NN) &= P(NN|TO) P(\mbox{race}|NN) \\
    &= 0.021 \times 0.00041 = 0.000007
   \end{align*}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Simple bigram tagging in NLTK}
{\small
\begin{verbatim}
>>> default_pattern = (r'.*', 'NN')
>>> cd_pattern = (r' ^[0-9]+(.[0-9]+)?$', 'CD')
>>> patterns = [cd_pattern, default_pattern]
>>> NN_CD_tagger = tag.Regexp(patterns)
>>> unigram_tagger = tag.Unigram(cutoff=0, backoff=NN_CD_tagger)
>>> unigram_tagger.train(train_sents)
>>> bigram_tagger = tag.Bigram(backoff=unigram_tagger)
>>> bigram_tagger.train(train_sents)
\end{verbatim}%$

\begin{verbatim}
# uses print_accuracy function defined in lecture PoS1
>>> print_accuracy(bigram_tagger, train_sents)
95.6%
>>> print_accuracy(bigram_tagger, test_sents)
84.2%
\end{verbatim}
}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Limitation of NLTK n-gram taggers}

  \begin{itemize}
  \item<1-> Does not find the most likely sequence of tags, simply works
    left to right always assigning the most probable single tag (given
    the previous tag assignments)
  \item<1-> Does not cope with zero probability problem well (no smoothing
    or discounting)
  \item<2-> (see module \verb+nltk_lite.tag.hmm+)
  \end{itemize}
\end{frame}

\section{Tagging a sequence of words}

\begin{frame}
  \mode<presentation>{\frametitle{Tagging a sequence}}
  \begin{itemize}
  \item<1-> Rather than choosing a tag for a single word only, we want to
    choose a tag sequence for a word sequence
  \item<1-> Solution: choose the most probable tag sequence given the word
    sequence
  \item<2-> Let $W_1^n$ be a word sequence and $T_1^n$ be a tag sequence:
    \begin{displaymath}
      P(T_1^n|W_1^n) = \frac{P(W_1^n | T_1^n)P(T_1^n)}{P(W_1^n)} \propto P(W_1^n | T_1^n)P(T_1^n)
    \end{displaymath}
  \item<3-> And the most likely tag sequence $T^\ast$ is:
    \[ T^\ast = \arg\max_{T_1^n} P(W_1^n | T_1^n)P(T_1^n) \]
  \item<3-> So we need to estimate $P(W_1^n | T_1^n)$ and $P(T_1^n)$. 
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Approximation (1)}
  \begin{align*}
    P(W_1^n | T_1^n) &= P(w_1,\ldots,w_n | T_1,\ldots,T_n) \\
    &= P(w_1 | T_1,\ldots,T_n) P(w_2 | w_1, T_1,\ldots,T_n) \ldots \\
    & \qquad \qquad \ldots 
    P(w_n | w_{n-1},\ldots,w_1, T_{n-1}, \ldots, t_1)
  \end{align*}
  
  \uncover<2->{Two assumptions:}
  \begin{enumerate}
  \item<3-> $w_i$ is depends on $t_i$ and no previous tags
  \item<4-> $w_i$ is independent of the previous words
  \end{enumerate}
  \uncover<4->{
    \[P(W_1^n | T_1^n) \sim \prod_i P(w_i | t_i) \]  }
\end{frame}

\begin{frame}
  \frametitle{Approximation (2)}
  Bigram estimate of $P(T_1^n)$
  \begin{align*}
    P(T_1^n) &= P(t_1) P(t_2 | t_1) \ldots P(t_n | t_{n-1}, \ldots,
    t_1)\\
    &= P(t_1) P(t_2 | t_1) \ldots P(t_n | t_{n-1})
  \end{align*}

  \uncover<2->{
  Combining,
  \[ P(W_1^n | T_1^n)P(T_1^n) \sim  \prod_{i=1}^n P(w_i | t_i) P(t_i|t_{i-1})\]}

  \begin{itemize}
  \item<3-> Problem 1: Estimate the probability tables $P(w_i | t_i)$ and
    $P(t_i|t_{i-1})$  (\textbf{Training})
  \item<4-> Problem 2: Given the probability tables and a sequence of
    words, what is the most likely sequence of tags (\textbf{Decoding})
  \end{itemize} 
\end{frame}

\begin{frame}
  \frametitle{Training}
  \begin{itemize}
  \item<1-> The training problem is the same as for word-level n-grams.
    With no smoothing or discounting,
    \begin{align*}
      P(w_i | t_i) &= \frac{c(w_i,t_i)}{c(t_i)} \\
      P(t_i | t_{i-1}) &=  \frac{c(t_{i-1}, t_i)}{c(t_{i-1})}
    \end{align*}
  \item<2-> In practice the smoothing and discounting techniques covered
    earlier are necessary (see eg the TnT tagger)
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Viterbi decoding (dynamic programming)}
  \begin{itemize}
  \item<1-> Find the most likely sequence of tags given the observed
    sequence of words
  \item<1-> Exhaustive search (ie probability evaluation of each possible
    tag sequence) is very slow (not feasible)
  \item<2-> But an n-gram model has limited history --- so need only
    consider one sequence per n-gram context
  \item<2-> Problem is that of finding the most probable path through a
    tag-time lattice
  \item<3-> The solution is \alert{Viterbi decoding} or \alert{dynamic
      programming} 
  \item<3-> Discussed (in different contexts) in chapters 5 and 7 of
    Jurafsky and Martin.  See also Manning and Sch\"utze,
    \emph{Foundations of Statisitcal NLP}, pp349--351.
  \end{itemize}
\end{frame}

\section{Trigram PoS tagging}

\begin{frame}
  \frametitle{TnT --- A trigram POS tagger}
  \begin{itemize}
  \item<1-> TnT --- trigram-based tagger by Thorsten Brants (installed on
    DICE)
    (\href{http://www.coli.uni-sb.de/~thorsten/tnt/}{http://www.coli.uni-sb.de/~thorsten/tnt/})
  \item<1-> Based on the n-gram/HMM model described above, except that the
    tag sequence is modelled by trigrams
  \item<2-> n-grams are smoothed by interpolation
  \item<3-> Unknown words handled by an n-gram model over letters
  \item<4-> Also models capitalization and has an efficient decoding
    algorithm (beam-search pruned Viterbi)
  \item<5-> Fast and accurate tagger --- 96-97\% accuracy on newspaper
    text (English or German)
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{The trigram model}
  \[ P(W_1^n | T_1^n)P(T_1^n) \sim \prod_{i=1}^{n+1} P(w_i | t_i)  P(t_i|t_{i-2},t_{i-1})\]

  \begin{itemize}
  \item<1-> The most likely tag sequence $t_1, \ldots, t_n$ is chosen to
    maximise the above expression
  \item<1-> $t_0$, $t_{-1}$ and $t_{n+1}$ are beginning- and end-of-sequence markers
  \item<2-> Probabilities estimated from relative frequency counts, eg:
    \[ \hat{P}(t_3|t_1, t_2) = \frac{c(t_1,t_2,t_3)}{c(t_1,t_2)} \]
  \item<2-> No discounting in TnT!
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Smoothing}
  \begin{itemize}
  \item<1-> Interpolation-based smoothing:
    \[ P(t_3|t_1, t_2) = \lambda_3 \hat{P}(t_3|t_1, t_2) +
    \lambda_2 \hat{P}(t_3|t_2) + \lambda_1 \hat{P}(t_3) \]
    \[ \lambda_3 + \lambda_2 + \lambda_1 = 1 \]
  \item<2-> The $\lambda$ coefficients are also estimated from the
    training data (deleted interpolation)
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Dealing with new words}
  \begin{itemize}
  \item<1-> Unknown words are calculated using a letter-based n-gram,
    using the last $m$ letters $l_i$ of an $L$-letter word:
    $P(t|l_{L-m+1}, \ldots, l_L)$.
  \item<2-> Basic idea: suffixes of unknown words give a good clue to the
    POS of the word 
  \item<2-> How big is $m$? - no bigger than 10, but it is based on the
    longest suffix found in the training set 
  \item<2-> These probabilities also smoothed by interpolation
  \end{itemize}
\end{frame}

\section{The Brill Tagger}

\begin{frame}[fragile]
  \frametitle{Transformation-based tagging}
  \begin{itemize}
  \item<1-> A rule-based system...
  \item<2-> ...but the rules are learned from a corpus
  \item<3-> Basic approach: start by applying general rules, then
    successively refine with additional rules that correct the
    mistakes
  \item<4-> Learn the rules from a corpus, using a set of rule templates,
    eg:\\
    Change tag \textbf{a} to \textbf{b} when the following word is
    tagged \textbf{z}
  \item<4-> Choose the best rule each iteration    
  \item<5-> (see module \verb+nltk_lite.tag.brill+)
  \end{itemize}
\end{frame}


\section{Summary}

\begin{frame}
  \mode<presentation>{\frametitle{Summary}}

  \begin{itemize}
  \item \textbf{Reading:} 
    \begin{itemize}
    \item Jurafsky and Martin, chapter 8 (esp. sec
    8.5); 
  \item  Manning and Sch\"utze, chapter 10; 
  \item T. Brants (2000). "TnT -- a statistical part-of-speech
      tagger". In
    Proceedings of the 6th Applied NLP Conference, ANLP-2000.\\
    \href{http://uk.arxiv.org/abs/cs.CL/0003055}{http://uk.arxiv.org/abs/cs.CL/0003055}
  \end{itemize}
  \item Rule-based and statistical tagging
  \item HMMs and n-grams for statistical tagging
  \item Operation of a simple bigram tagger
  \item TnT --- an accurate trigram-based tagger
  \end{itemize}
\end{frame}

\end{document}




















































