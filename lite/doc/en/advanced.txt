.. -*- mode: rst -*-
.. include:: ../definitions.txt
.. include:: regexp-defns.txt

==============================
Advanced Programming in Python
==============================

:Authors: Steven Bird, Ewan Klein, Edward Loper
:Version: |version|
:Revision: $Revision$
:Date: $Date$
:Copyright: |copy| |copyrightinfo|
:License: Creative Commons Attribution-NonCommercial-ShareAlike License

.. Note:: This is a draft.  Please send any feedback to the authors.

------------
Introduction
------------

This chapter introduces concepts in algorithms, data structures,
program design, and advanced Python programming.
It contains many working program fragments which you should try yourself.


-------------------
Regular Expressions
-------------------

This section explores regular expressions in detail, with examples
drawn from language processing.  It builds on the brief overview given
in the introductory programming chapter.
    
We have already noted that a text can be viewed as a string of
characters. What kinds of processing are performed at the character
level?  Perhaps word games are the most familiar example of such
processing.  In completing a crossword we may want to know which
3-letter English words end with the letter ``c`` (e.g.  ``arc``).  We
might want to know how many words can be formed from the letters:
``a``, ``c``, ``e``, ``o``, and ``n`` (e.g. ``ocean``).  We may want
to find out which unique English word contains the substring ``gnt``
(left as an exercise for the reader).  In all these examples, we are
considering which word - drawn from a large set of candidates -
matches a given pattern. To put this in a more computational
framework, we could imagine searching through a large digital corpus
in order to find all words that match a particular pattern. There are
many serious uses of this so-called *pattern matching*.

One instructive example is the task of finding all doubled words in a
text; an example would be the string ``for for example``. Notice that
we would be particularly interested in finding cases where the words
were split across a linebreak (in practice, most erroneously doubled
words occur in this context). Consequently, even with such a
relatively banal task, we need to be able to describe patterns which
refer not just to "ordinary" characters, but also to formatting
information.

There are conventions for indicating structure in strings, also known
as *formatting*. For example, there are a number of alternative ways
of formatting a "date string", such as ``23/06/2002``, ``6/23/02``, or
``2002-06-23``.  Whole texts may be formatted, such as an email
message which contains header fields followed by the message
body. Another familiar form of formatting involves visual structure,
such as tabular format and bulleted lists.

Finally, texts may contain explicit "markup", such as
``<abbrev>Phil</abbrev>``, which provides information about the
interpretation or presentation of some piece of text.  To summarize,
in language processing, strings are ubiquitous, and they often contain
important structure.

So far we have seen elementary examples of pattern matching, the
matching of individual characters.  More often we are interested in
matching *sequences* of characters.  For example, part of the
operation of a naive spell-checker could be to remove a word-final
``s`` from a suspect word token, in case the word is a plural, and see
if the putative singular form exists in the dictionary.  For this we
must locate ``s`` and remove it, but only if it precedes a word
boundary.  This requires matching a pattern consisting of two
characters.

Beyond this pattern matching on the *content* of a text, we often want
to process the *formatting* and *markup* of a text.  We may want to
check the formatting of a document (e.g. to ensure that every sentence
begins with a capital letter) or to reformat a document
(e.g. replacing sequences of space characters with a single space).
We may want to find all date strings and extract the year.  We may
want to extract all words contained inside the ``<abbrev> </abbrev>``
markup in order to construct a list of abbreviations.

Processing the content, format and markup of strings is a central task
in most kinds of NLP.  The most widespread method for string
processing uses *regular expressions*.
    
Simple Regular Expressions
--------------------------

In this section we will see the building blocks for simple regular
expressions, along with a selection of linguistic examples. We can
think of a regular expression as *a specialised notation for
describing patterns that we want to match*. In order to make explicit
when we are talking about a pattern *patt*, we will
use the notation |patt|\ . The first thing to say
about regular expressions is that most letters match themselves. For
example, the pattern |l|\ ``sing``\ |r| exactly matches the
string ``sing``. In addition, regular expressions provide us with a
set of *special characters* [#]_ which give us a way to match *sets of
strings*, and we will now look at these.

.. [#] These are often called *metacharacters*; that is,
       characters which express properties of (ordinary) characters.

The Wildcard
~~~~~~~~~~~~

The "``.``" symbol is called a *wildcard*: it matches any single
character. For example, the regular expression |s.ng| matches the
following English words: ``sang``, ``sing``, ``song``, and
``sung``. Note that |.| will match not only alphabetic characters, but
also numeric and whitespace characters. Consequently, |s.ng| will also
match non-words such as ``s3ng``.

We can also use the wildcard symbol for counting characters. For
instance |....zy| matches six-letter strings that end in ``zy``.  The
pattern |....berry| finds words like ``cranberry``. In our text from
Wall Street Journal below, the pattern |t...| will match the words
``that`` and ``term``, and will also match the word sequence ``to a``
(since the third "``.``" in the pattern can match the space
character)::

  Paragraph 12 from wsj_0034:
  It's probably worth paying a premium for funds that invest in markets
  that are partially closed to foreign investors, such as South Korea,
  some specialists say.  But some European funds recently have
  skyrocketed; Spain Fund has surged to a startling 120% premium.  It has
  been targeted by Japanese investors as a good long-term play tied to
  1992's European economic integration.  And several new funds that aren't
  even fully invested yet have jumped to trade at big premiums.

  "I'm very alarmed to see these rich valuations," says Smith Barney's
  Mr. Porter.

.. Note:: Note that the wildcard matches *exactly* one character, and must be
   repeated for as many characters as should be matched.  To match a
   variable number of characters we must use notation for *optionality*.

We can see exactly where a regular expression matches against a string
using NLTK's ``re_show`` function.  Readers are encouraged to use
``re_show`` to explore the behaviour of regular expressions.

  >>> from nltk_lite.utilities import re_show
  >>> string = """
  ... It's probably worth paying a premium for funds that invest in markets
  ... that are partially closed to foreign investors, such as South Korea, ...
  ... """
  >>> re_show('t...', string)
  I{t's }probably wor{th p}aying a premium for funds {that} inves{t in} markets
  {that} are par{tial}ly closed {to f}oreign inves{tors}, such as Sou{th K}orea, ...

Optionality and Repeatability
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The "``?``" symbol indicates that the immediately preceding regular
expression is optional.  The regular expression |colou?r| matches both
British and American spellings, ``colour`` and ``color``.  The
expression that precedes the ``?`` may be punctuation, such as an
optional hyphen.  For instance |e-?mail| matches both ``e-mail`` and
``email``.
      
The "``+``" symbol indicates that the immediately preceding expression
is repeatable, up to an arbitrary number of times.  For example, the
regular expression |coo+l| matches ``cool``, ``coool``, and so on.
This symbol is particularly effective when combined with the ``.``
symbol.  For example, |f.+f| matches all strings of length greater
than two, that begin and end with the letter ``f`` (e.g.
``foolproof``).  The expression |.+ed| finds strings that potentially
have the past-tense ``-ed`` suffix.
      
The "``*``" symbol indicates that the immediately preceding expression
is both optional and repeatable. For example |.*gnt.*| matches all
strings that contain ``gnt``.
      
Occasionally we need to match material that spans a linebreak.  For
example, we may want to strip out the HTML markup from a document.
To do this we must delete material between angle brackets.
The most obvious expression is: |<.*>|.  However, this has two
problems: it will not match an HTML tag that contains a linebreak,
and the |.*| will consume as much material as possible (including the
``>`` character).  To permit matching over a linebreak we must use
Python's ``DOTALL`` flag, and to ensure that the ``>`` matches
against the first instance of the character we must do non-greedy
matching using ``*?``:

  >>> import re
  >>> text = """one two three <font
  ...           color="red">four</font> five"""
  >>> re.sub(r'<.*?>', ' ', text, re.DOTALL)


Choices
~~~~~~~

Patterns using the wildcard symbol are very effective, but there are
many instances where we want to limit the set of characters that the
wildcard can match.  In such cases we can use the ``[]`` notation,
which enumerates the set of characters to be matched - this is called
a *character class*.  For example, we can match any English vowel, but
no consonant, using |[aeiou]|\ . Note that this pattern can be
interpreted as saying "match ``a`` or ``e`` or |cdots| or ``u``"; that
is, the pattern resembles the wildcard in only matching a string of
length one; unlike the wildcard, it restricts the characters matched
to a specific class (in this case, the vowels).  Note that the order
of vowels in the regular expression is insignificant, and we would
have had the same result with the expression |[uoiea]|\ . As a second
example, the expression |p[aeiou]t| matches the words: ``pat``,
``pet``, ``pit``, ``pot``, and ``put``.

We can combine the ``[]`` notation with our notation for
repeatability.  For example, expression |p[aeiou]+t| matches the words
listed above, along with: ``peat``, ``poet``, and ``pout``.

Often the choices we want to describe cannot be expressed at the level
of individual characters.  As discussed in the tagging tutorial,
different parts of speech are often *tagged* using labels from a
tagset. In the Brown tagset, for example, singular nouns have the tag
``NN1``, while plural nouns have the tag ``NN2``, while nouns which
are unspecified for number (e.g., ``aircraft``) are tagged ``NN0``. So
we might use |NN.*| as a pattern which will match any nominal
tag. Now, suppose we were processing the output of a tagger to extract
string of tokens corresponding to noun phrases, we might want to find
all nouns (``NN.*``), adjectives (``JJ.*``), determiners (``DT``) and
cardinals (``CD``), while excluding all other word types (e.g. verbs
``VB.*``).  It is possible, using a single regular expression, to
search for this set of candidates using the *choice operator* "``|``"
as follows: |NN.*|JJ.*|DT|CD|\ . This says: match ``NN.*`` *or*
``JJ.*`` *or* ``DT`` *or* ``CD``.
 
As another example of multi-character choices, suppose that we wanted
to create a program to simplify English prose, replacing rare words
(like ``habitation``) with a more frequent, synonymous word (like
``home``).  In this situation, we need to map from a potentially large
set of words to an individual word.  We can match the set of words
using the choice operator.  In the case of the word ``home``, we would
want to match the regular expression |dwelling|domicile|abode|habitation|\ .

.. Note:: Note that the choice operator has wide scope, so that |abc|def|
   is a choice between ``abd`` and ``def``, and not between ``abced`` and
   ``abdef``.  The latter choice must be written using parentheses: |ab(c|d)ed|\ .


More Complex Regular Expressions
--------------------------------

In this section we will cover operators which can be used to
construct more powerful and useful regular expressions.

Ranges
~~~~~~

Earlier we saw how the ``[]`` notation could be used to express a set
of choices between individual characters.  Instead of listing each
character, it is also possible to express a *range* of characters,
using the ``-`` operator.  For example, |[a-z]| matches any lowercase
letter.  This allows us to avoid the overpermissive matching we noted
above with the pattern |t...|\ . If we were to use the pattern
|t[a-z][a-z][a-z]|, then we would no longer match the two word
sequence ``to a``.

As expected, ranges can be combined with other operators. For example
|[A-Z][a-z]*| matches words that have an initial capital letter
followed by any number of lowercase letters.  The pattern
|20[0-4][0-9]| matches year expressions in the range 2000 to 2049.
  
Ranges can be combined, e.g.  |[a-zA-Z]| which matches any lowercase
or uppercase letter.  The expression |[b-df-hj-np-tv-z]+| matches
words consisting only of consonants (e.g. ``pygmy``).
  
Complementation
~~~~~~~~~~~~~~~

We just saw that the character class |l|\ ``[b-df-hj-np-tv-z]+``\ |r| allows us to
match sequences of consonants. However, this expression is quite
cumbersome. A better alternative is to say: let's match anything which
isn't a vowel. To do this, we need a way of expressing
*complementation*.  We do this using the symbol "``^``" as the first
character inside a class expression ``[]``. Let's look at an example.
The regular expression |l|\ ``[^aeiou]``\ |r| is just like our earlier character
class |l|\ ``[aeiou]``\ |r|, except now the set of vowels is preceded
by ``^``. The expression as a whole is interpreted as matching
anything which *fails* to match |l|\ ``[aeiou]``\ |r|\ . In other
words, it matches all lowercase consonants (plus all uppercase letters
and non-alphabetic characters).

As another example, suppose we want to match any string which is
enclosed by the HTML tags for boldface, namely ``<B>`` and ``</B>``.
We might try something like this: |<B>.*</B>|\ . This would
successfully match ``<B>important</B>``, but would also match
``<B>important</B> and <B>urgent</B>``, since the |.*| subpattern will
happily match all the characters from the end of ``important`` to the
end of ``urgent``. One way of ensuring that we only look at matched
pairs of tags would be to use the expression |l|\ ``<B>[^<]*</B>``\
|r|\ , where the chararacter class matches anything other than a left
angle bracket.

Finally, note that character class complementation also works with
ranges. Thus |[^a-z]| matches anything other than the lower case
alphabetic characters ``a`` through ``z``.

Common Special Symbols
~~~~~~~~~~~~~~~~~~~~~~

So far, we have only looked at patterns which match with the content
of character strings. However, it is also useful to be able to refer
to formatting properties of texts. Two important symbols in this
regard are "``^``" and "``$``" which are used to *anchor* matches to
the beginnings or ends of lines in a file.

.. Note:: "``^``" has two quite distinct uses: it is interpreted as
   complementation when it occurs as the first symbol within a character
   class, and as matching the beginning of lines when it occurs elsewhere
   in a pattern.

For example, suppose we wanted to find all the words that occur at the
beginning of lines in the WSJ text above. Our first attempt might look
like |^[A-Za-z]+|\ . This says: starting at the beginning of a line,
look for one or more alphabetic characters (upper or lower case),
followed by a space. This will match the words ``that``, ``some``,
``been``, and ``even``. However, it fails to match ``It's``, since
``'`` isn't an alphabetic character. A second attempt might be
|^[^ ]+|\ , which says to match any string starting at the
beginning of a line, followed by one or more characters which are
*not* the space character, followed by a space. This matches all the
previous words, together with ``It's``, ``skyrocketed``, ``1992s``,
``I'm`` and ``"Mr.``. As a second example, |[a-z]*s$| will
match words ending in ``s`` that occur at the end of a line. Finally,
consider the pattern |^$|\ ; this matches strings where no
character occurs between the beginning and the end of a line - in
other words, empty lines!

As we have seen, special characters like "``.``", "``*``", "``+``" and
"``$``" give us powerful means to generalise over character
strings. But suppose we wanted to match against a string which itself
contains one or more special characters? An example would be the
arithmetic statement ``$5.00 * ($3.05 + $0.85)``. In this case, we
need to resort to the so-called *escape* character "``\``"
("backslash"). For example, to match a dollar amount, we might use
|l|\ ``\$[1-9][0-9]*\.[0-9][0-9]``\ |r|\ .  The same goes for matching
other special characters.

.. Advanced Regular Expressions
   zero-width assertions
   more special symbols: \b etc


===========  =============================================================
       Special Sequences
--------------------------------------------------------------------------
``\b``       Word boundary (zero width)
-----------  -------------------------------------------------------------
``\d``       Any decimal digit (equivalent to ``[0-9]``)
-----------  -------------------------------------------------------------
``\D``       Any non-digit character (equivalent to ``[^0-9]``)
-----------  -------------------------------------------------------------
``\s``       Any whitespace character (equivalent to ``[ \t\n\r\f\v]``
-----------  -------------------------------------------------------------
``\S``       Any non-whitespace character (equivalent to ``[^ \t\n\r\f\v]``)
-----------  -------------------------------------------------------------
``\w``       Any alphanumeric character (equivalent to ``[a-zA-Z0-9_]``)
-----------  -------------------------------------------------------------
``\W``       Any non-alphanumeric character (equivalent to ``[^a-zA-Z0-9_]``)
===========  =============================================================


Python Interface
----------------

The Python ``re`` module provides a convenient interface to an
underlying regular expression engine. The module allows a regular
expression pattern to be compiled into a object whose methods can then
be called.

In the next example, we assume that we have a local copy (i.e.,
``words``) of the Unix dictionary, which may be found in the NLTK
``data/words`` directory.  This file contains over 400,000 words, one
per line.

  >>> from re import * 
  >>> from nltk_lite.corpora import words

Next we read in the list of words and count them:

  >>> wordlist = list(words.raw())
  >>> len(wordlist)
  45378

Now we can compile a regular expression for words containing a sequence of two 'a's
and find the matches:

  >>> r1 = compile('.*aa.*') 
  >>> [w for w in wordlist if r1.match(w)]
  ['Afrikaans', 'bazaar', 'bazaars', 'Canaan', 'Haag', 'Haas', 'Isaac', 'Isaacs', 'Isaacson', 'Izaak', 'Salaam', 'Transvaal', 'Waals']

Suppose now that we want to find all three-letter words ending in the
letter "``c``". Our first attempt might be as follows:

  >>> r1 = compile('..c') 
  >>> [w for w in wordlist if r1.match(w)][:10]
  ['accede', 'acceded', 'accedes', 'accelerate', 'accelerated', 'accelerates', 'accelerating', 'acceleration', 'accelerations', 'accelerator']

The problem is that we have matched words containing three-letter
sequences ending in "``c``" which occur *anywhere within a word*. For
example, the pattern will match "``c``" in words like ``aback``,
``Aerobacter`` and ``albacore``.  Instead, we must revise our pattern
so that it is anchored to the beginning and ends of the word: |l|\
``^...$``\ |r|\ :

  >>> r2 = compile('^..c$')
  >>> [w for w in wordlist if r2.match(w)]
  ['arc', 'Doc', 'Lac', 'Mac', 'Vic']

In the section on complementation, we briefly looked at the task of
matching strings which were enclosed by HTML markup. Our first attempt
is illustrated in the following code example, where we incorrectly
match the whole string, rather than just the substring
"``<B>important</B>``".

  >>> html = '<B>important</B> and <B>urgent</B>'
  >>> r2 = compile('<B>.*</B>')
  >>> print r2.findall(html)
  ['<B>important</B> and <B>urgent</B>']

As we pointed out, one solution is to use a character class which
matches with the complement of "``<``":

  >>> r4 = compile('<B>[^<]*</B>')
  >>> print r4.findall(html)
  ['<B>important</B>', '<B>urgent</B>']

However, there is another way of approaching this problem.
|l|\ ``<B>.*</B>``\ |r| gets the wrong results because the
|l|\ ``*``\ |r| operator tries to consume as much input as
possible. That is, the matching is said to be *greedy*. In the current
case, |l|\ ``*``\ |r| matches everything after the first
``<B>``, including the following ``</B>`` and ``<B>``.  If we instead
use the non-greedy star operator |l|\ ``*?``\ |r|\ , we get the
desired match, since |l|\ ``*?``\ |r| tries to consume as little
input as possible.

Exercises
---------

1. Describe the class of strings matched by the following regular expressions:

   a) ``[a-zA-Z]+``
   b) ``[A-Z][a-z]*``
   c) ``\d+(\.\d+)?``
   d) ``([bcdfghjklmnpqrstvwxyz][aeiou][bcdfghjklmnpqrstvwxyz])*``
   e) ``\w+|[^\w\s]+``

2. *Pig Latin* is a simple transliteration of English: words starting with
   a vowel have ``way`` appended (e.g. ``is`` becomes ``isway``);
   words beginning with a consonant have all consonants up to the first
   vowel moved to the end of the word, and then ``ay`` is appended
   (e.g. ``start`` becomes ``artstay``).

   a) Write a program to convert English text to Pig Latin.

   b) Extend the program to convert text, instead of individual words.

   c) Extend it further to preserve capitalisation, to keep ``qu`` together
      (i.e. so that ``quiet`` becomes ``ietquay``), and to detect when ``y``
      is used as a consonant (e.g. ``yellow``) vs a vowel (e.g. ``style``).

3. An interesting challenge for tokenisation is words that have been
   split across a linebreak.  E.g. if *long-term* is split, then we
   have the string ``long-\nterm``.

   a) Write a regular expression that identifies words that are
      hyphenated at a linebreak.  The expression will need to include the
      ``\n`` character.

   b) Use ``re.sub()`` to remove the ``\n`` character from these
      words.

4. Write a utility function that takes a URL as its argument, and returns
   the contents of the URL, with all HTML markup removed.  Use ``urllib.urlopen``
   to access the contents of the URL, e.g.
   ``raw_contents = urllib.urlopen('http://nltk.sourceforge.net/').read()``.

5. Write a program to guess the number of syllables from the orthographic
   representation of words (e.g. English text).

6. Download some text from a language that has vowel harmony (e.g. Hungarian),
   extract the vowel sequences of words, and create a vowel bigram table.

7. Obtain a pronunciation lexicon, and try generating nonsense rhymes.

-----------------
Some Applications
-----------------

Classifying Words Automatically
-------------------------------

A tagged corpus can be used to *train* a simple classifier, which can
then be used to guess the tag for untagged words.  For each word, we
can count the number of times it is tagged with each tag.  For
instance, the word ``deal`` is tagged 89 times as ``nn`` and 41 times
as ``vb``.  On this evidence, if we were asked to guess the tag for
``deal`` we would choose ``nn``, and we would be right over two-thirds
of the time.  The following program performs this tagging task, when
trained on the "g" section of the Brown Corpus (so-called *belles
lettres*, creative writing valued for its aesthetic content).

  >>> cfdist = ConditionalFreqDist()
  >>> for sentence in brown.tagged('g'):
  ...     for token in sentence:
  ...         word = token[0]
  ...         tag = token[1]
  ...         cfdist[word].inc(tag)
  >>> for word in "John saw 3 polar bears".split():
  ...     print word, cfdist[word].max()
  John np
  saw vbd
  3 cd-tl
  polar jj
  bears vbz
    
Note that ``bears`` was incorrectly tagged as the 3rd person singular
form of a verb, since this word appears more frequently as a verb than
a noun in esthetic writing.

A problem with this approach is that it creates a huge model, with an
entry for every possible combination of word and tag.  For certain
tasks it is possible to construct reasonably good models which are
tiny in comparison.  For instance, let's try to guess whether a verb
is a noun or adjective from the last letter of the word alone.  We can
do this as follows:

  >>> tokens = []
  >>> for sent in brown.tagged('g'):
  ...     for (word,tag) in sent:
  ...         if tag in ['nn', 'jj'] and len(word) > 3:
  ...             char = word[-1]
  ...             tokens.append((char,tag))
  >>> split = len(tokens)*9/10
  >>> train, test = tokens[:split], tokens[split:]
  >>> cfdist = ConditionalFreqDist()
  >>> for (char,tag) in train:
  ...     cfdist[char].inc(tag)
  >>> correct = total = 0
  >>> for (char,tag) in test:
  ...     if tag == cfdist[char].max():
  ...         correct += 1
  ...     total += 1
  >>> print correct*100/total
  71

This result of 71% is marginally better than the result of 65% that we
get if we assign the ``nn`` tag to every word.  We can inspect the
model to see which tag is assigned to a word given its final letter.
Here we learn that words which end in ``c`` or ``l`` are more likely
to be adjectives than nouns::

  >>> print [(c, cfdist[c].max()) for c in cfdist.conditions()]
  [('%', 'nn'), ("'", None), ('-', 'jj'), ('2', 'nn'), ('5', 'nn'), ('A', 'nn'), ('D', 'nn'), ('O', 'nn'), ('S', 'nn'), ('a', 'nn'), ('c', 'jj'), ('b', 'nn'), ('e', 'nn'), ('d', 'nn'), ('g', 'nn'), ('f', 'nn'), ('i', 'nn'), ('h', 'nn'), ('k', 'nn'), ('m', 'nn'), ('l', 'jj'), ('o', 'nn'), ('n', 'nn'), ('p', 'nn'), ('s', 'nn'), ('r', 'nn'), ('u', 'nn'), ('t', 'nn'), ('w', 'nn'), ('y', 'nn'), ('x', 'nn'), ('z', 'nn')]

Exploring text genres
---------------------

Now that we can load a significant quantity of tagged text, we can
process it and extract items of interest.  The following code iterates
over the fifteen genres of the Brown Corpus (accessed using
``brown.groups()``).  The material for each genre lives in a set of
files (accessed using ``brown.items``).  Each of these is tokenized
in turn.  The next step is to check if the token has the ``md`` tag.
For each of these words we increment a count.
This uses the conditional frequency distribution, where the condition
is the current genre, and the event is the modal.

  >>> cfdist = ConditionalFreqDist()
  >>> for genre in brown.items:                  # each genre
  ...     for sent in brown.tagged(genre):       # each sentence
  ...         for (word,tag) in sent:            # each tagged token
  ...             if tag == 'md':                # found a modal
  ...                  cfdist[genre].inc(word.lower())

The conditional frequency distribution is nothing more than a mapping
from each genre to the distribution of modals in that genre.  The
following code fragment identifies a small set of modals of interest,
and processes the data structure to output the required counts.

  >>> modals = ['can', 'could', 'may', 'might', 'must', 'will']
  >>> print "%-40s" % 'Genre', ' '.join([("%6s" % m) for m in modals])
  Genre                                       can  could    may  might   must   will
  >>> for genre in cfdist.conditions():    # generate rows
  ...     print "%-40s" % brown.item_name[genre],
  ...     for modal in modals:
  ...         print "%6d" % cfdist[genre].count(modal),
  ...     print
  press: reportage                             94     86     66     36     50    387
  press: reviews                               44     40     45     26     18     56
  press: editorial                            122     56     74     37     53    225
  skill and hobbies                           273     59    130     22     83    259
  religion                                     84     59     79     12     54     64
  belles-lettres                              249    216    213    113    169    222
  popular lore                                168    142    165     45     95    163
  miscellaneous: government & house organs    115     37    152     13     99    237
  fiction: general                             39    168      8     42     55     50
  learned                                     366    159    325    126    202    330
  fiction: science                             16     49      4     12      8     16
  fiction: mystery                             44    145     13     57     31     17
  fiction: adventure                           48    154      6     58     27     48
  fiction: romance                             79    195     11     51     46     43
  humor                                        17     33      8      8      9     13

There are some interesting patterns in this table.  For instance,
compare the rows for government literature and adventure literature;
the former is dominated by the use of ``can, may, must, will`` while
the latter is characterised by the use of ``could`` and ``might``.
With some further work it might be possible to guess the genre of a
new text automatically, according to its distribution of modals.

Exercises
---------

#. **Classifying words automatically:**
   The program for classifying words as nouns or adjectives scored 71%.
   Try to come up with better conditions, to get the system to score 80% or better.

   a) Revise the condition to use a longer suffix of the word, such as
      the last two characters, or the last three characters.  What happens
      to the performance?  Which suffixes are diagnostic for adjectives?

   #) Explore other conditions, such as variable length prefixes of a
      word, or the length of a word, or the number of vowels in a word.

   #) Finally, combine multiple conditions into a tuple, and explore
      which combination of conditions gives the best result.

#. **Exploring text genres:**
   Investigate the table of modal distributions and look for other patterns.
   Try to explain them in terms of your own impressionistic understanding
   of the different genres.  Can you find other closed classes of words that
   exhibit significant differences across different genres?

-------------------
Program Development
-------------------

Programming is a skill which is acquired over several years of
experience with a variety of programming languages and tasks.  Key
high-level abilities are *algorithm design* and its manifestation in
*structured programming*.  Key low-level abilities include familiarity
with the syntactic constructs of the language, and knowledge of a
variety of diagnostic methods for trouble-shooting a program which
does not exhibit the expected behaviour.

Defining Functions
------------------

It often happens that part of a program needs to be used several times over.
For example, suppose we were writing a program that needed to be able to form
the plural of a singular noun, and that this needed to be done at various
places during the program.  Rather than repeating the same code several times
over, it is more efficient (and reliable) to localize this work inside a *function*.
A function is a programming construct which takes one or more inputs, and produces
an output.  In this case, we will take the singular noun as input, and generate a
plural form as output:

  >>> def plural(word):
  ...     if word[-1] == 'y':
  ...         return word[:-1] + 'ies'
  ...     elif word[-1] in 'sx':
  ...         return word + 'es'
  ...     elif word[-2:] in ['sh', 'ch']:
  ...         return word + 'es'
  ...     elif word[-2:] == 'an':
  ...         return word[:-2] + 'en'
  ...     return word + 's'
  >>> plural('fairy')
  'fairies'
  >>> plural('woman')
  'women'

Well-structured programs often make extensive use of functions.  Often when a block
of program code grows longer than a screenful, it is a great help to readability if
it is decomposed into one or more functions.

List Comprehensions
-------------------

List comprehensions are another important Python construct.
Many language processing tasks involve applying the same operation to
every item in a list.  Here we lowercase each word:

  >>> [word.lower() for word in sent]
  ['the', 'dog', 'gave', 'john', 'the', 'newspaper']

As another example, we could remove all determiners from a list of
words:

  >>> def is_lexical(word):
  ...     return word.lower() not in ('a', 'an', 'the', 'that', 'to')
  >>> [word for word in sent if is_lexical(word)]
  ['dog', 'gave', 'John', 'newspaper']

Or equivalently:

  >>> filter(is_lexical, sent)
  ['dog', 'gave', 'John', 'newspaper']


Combining the transformations...

  >>> [word.lower() for word in sent if is_lexical(word)]
  ['dog', 'gave', 'john', 'newspaper']


.. Another example: stemming each word:
..   >>> from nltk_lite import tokenize
..   >>> stemmer = tokenize.RegexpStemmer('ing$|s$|e$')

The following code builds a list of tuples, where each tuple consists
of a word and its length.

  >>> [(x, len(x)) for x in sent]
  [('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]

Algorithm Design
----------------

An *algorithm* is a "recipe" for solving a problem.  For example,
to multiply 16 by 12 we might use any of the following methods:

1. Add 16 to itself 12 times over
#. Perform "long multiplication", starting with the least-significant
   digits of both numbers
#. Look up a multiplication table
#. Repeatedly halve the first number and double the second,
   16*12 = 8*24 = 4*48 = 2*96 = 192
#. Do 10*12 to get 120, then add 6*12

Each of these methods is a different algorithm, and requires different
amounts of computation time and different amounts of intermediate
information to store.  A similar situation holds for many other
superficially simple tasks, such as sorting a list of words.  Now, as
we saw above, Python provides a built-in function ``sort()`` that
performs this task efficiently.  However, NLTK-Lite also provides
several algorithms for sorting lists, to illustrate the variety of
possible methods.  To illustrate the difference in efficiency, we
will create a list of 1000 numbers, randomize the list, then sort it,
counting the number of list manipulations required.

  >>> from random import shuffle
  >>> a = range(1000)                     # [0,1,2,...999]
  >>> shuffle(a)                          # randomize

Now we can try a simple sort method called *bubble sort*, which
scans through the list many times, exchanging adjacent items if
they are out of order.  It sorts the list ``a`` in-place, and returns
the number of times it modified the list:

  >>> from nltk_lite.misc import sort
  >>> sort.bubble(a)
  250918

We can try the same task using various sorting algorithms.  Evidently
*merge sort* is much better than bubble sort, and *quicksort* is better still.

  >>> shuffle(a); sort.merge(a)
  6175
  >>> shuffle(a); sort.quick(a)
  2378

Readers are encouraged to look at ``nltk_lite.misc.sort`` to see how
these different methods work.  The collection of NLTK-Lite modules
exemplify a variety of algorithm design techniques, including
brute-force, divide-and-conquer, dynamic programming, and greedy search.
Readers who would like a systematic introduction to algorithm design
should consult the resources mentioned at the end of this tutorial.

Programming Style
-----------------

We have just seen how the same task can be performed in different
ways, with implications for efficiency.  Another factor influencing
program development is *programming style*.  Consider the following
program to compute the average length of words in the Brown Corpus:

  >>> from nltk_lite.corpora import brown
  >>> count = 0
  >>> total = 0
  >>> for sent in brown.raw('a'):
  ...     for token in sent:
  ...         count += 1
  ...         total += len(token)
  >>> print float(total) / count
  4.2765382469

In this program we use the variable ``count`` to keep track of the
number of tokens seen, and ``total`` to store the combined length of
all words.  This is a low-level style, not far removed from machine
code, the primitive operations performed by the computer's CPU.
The two variables are just like a CPU's registers, accumulating values
at many intermediate stages, values which are almost meaningless. 
We say that this program is written in a *procedural* style, dictating
the machine operations step by step.  Now consider the following
program which computes the same thing:

  >>> tokens = [token for sent in brown.raw('a') for token in sent]
  >>> total = sum(map(len, tokens))
  >>> print float(total)/len(tokens)
  4.2765382469

The first line uses a list comprehension to construct the sequence of
tokens.  The second line *maps* the ``len`` function to this sequence,
to create a list of length values, which are summed.  The third line
computes the average as before.  Notice here that each line of code
performs a complete, meaningful action.  Moreover, they do not dictate
how the computer will perform the computations; we state high level
relationships like "``total`` is the sum of the lengths of the tokens"
and leave the details to the Python interpreter.  Accordingly, we say
that this program is written in a *declarative* style.

Here is another example to illustrate the procedural/declarative
distinction.  Notice again that the procedural version
involves low-level steps and a variable having meaningless
intermediate values:

  >>> word_list = []
  >>> for sent in brown.raw('a'):
  ...     for token in sent:
  ...         if token not in word_list:
  ...             word_list.append(token)
  >>> word_list.sort()

The declarative version (given second) makes use of higher-level
built-in functions:

  >>> tokens = [word for sent in brown.raw('a') for word in sent]
  >>> word_list = list(set(tokens))
  >>> word_list.sort()

What do these programs compute?  Which version was easier to interpret?

Consider one further example, which sorts three-letter words by their
final letters.  The words come from the widely-used Unix word-list,
made available as an NLTK corpus called ``words``.  Two words ending
with the same letter will be sorted according to their second-last
letters.  The result of this sort method is that rhyming words will be
contiguous.  Two programs are given; Which one is more declarative,
and which is more procedural?

As an aside, for readability we define a function for reversing
strings that will be used by both programs:

  >>> def reverse(word):
  ...     return word[::-1]

Here's the first program.  We define a helper function ``reverse_cmp``
which calls the built-in ``cmp`` comparison function on reversed
strings.  The ``cmp`` function returns ``-1``, ``0``, or ``1``,
depending on whether its first argument is less than, equal to, or
greater than its second argument.  We tell the list sort function to
use ``reverse_cmp`` instead of ``cmp`` (the default).

  >>> from nltk_lite.corpora import words
  >>> def reverse_cmp(x,y):
  ...     return cmp(reverse(x), reverse(y))
  >>> word_list = [word for word in words.raw('en') if len(word) == 3]
  >>> word_list.sort(reverse_cmp)
  >>> print word_list[-12:]
  ['toy', 'spy', 'cry', 'dry', 'fry', 'pry', 'try', 'buy', 'guy', 'ivy', 'Paz', 'Liz']

Here's the second program.  In the first loop it collects up all the
three-letter words in reversed form.  Next, it sorts the list of
reversed words.  Then, in the second loop, it iterates over each
position in the list using the variable ``i``, and replaces each item
with its reverse.  We have now re-reversed the words, and can print
them out.

  >>> word_list = []
  >>> for word in words.raw('en'):
  ...     if len(word) == 3:
  ...         word_list.append(reverse(word))
  >>> word_list.sort()
  >>> for i in range(len(word_list)):
  ...     word_list[i] = reverse(word_list[i])
  >>> print word_list[-12:]
  ['toy', 'spy', 'cry', 'dry', 'fry', 'pry', 'try', 'buy', 'guy', 'ivy', 'Paz', 'Liz']

Choosing between procedural and declarative styles is just that, a
question of style.  There are no hard boundaries, and it is possible
to mix the two.  Readers new to programming are encouraged to
experiment with both styles, and to make the extra effort required to
master higher-level constructs, such as list comprehensions, and
built-in functions like ``map``.


Debugging
---------

This task is known as *debugging*, since the problems are usually
small relative to their impact, hard-to-find, and seem to take on a
life of their own as the programmer tries to hunt them down.

The first step is to isolate the problem.  If there was a run-time
error, the Python interpreter will exit, providing a *stack-trace*
which lists the most recently called functions with their line numbers.
The simplest way to deal with such errors is to add print statements
to the program just before the offending line, permitting you to inspect
the values of variables (sometimes they are different to what you expected).

Python also provides an interactive debugger called ``pdb``, which stands
for Python debugger.  If your program is saved in a file ``myscript.py``,
then you can access the debugger with ``python -m pdb myscript.py``.

Exercises
---------

1. Write a program to sort words by length.  Define a helper function
   ``cmp_len`` which uses the ``cmp`` comparison function on word
   lengths.

2. Consider the tokenized sentence
   ``['The', 'dog', 'gave', 'John', 'the', 'newspaper']``.
   Using the ``map()`` and ``len()`` functions, write a single line
   program to convert this list of tokens into a list of token
   lengths: ``[3, 3, 4, 4, 3, 9]``

2. Consider again the problem of hyphenation across linebreaks.
   Suppose that you have successfully written a tokenizer that
   returns a list of strings, where some strings may contain
   a hyphen followed by a newline character, e.g. ``long-\nterm``.
   Write a function which iterates over the tokens in a list,
   removing the newline character from each, in each of the following
   ways:

   a) Use doubly-nested for loops.  The outer loop will iterate over
      each token in the list, while the inner loop will iterate over
      each character of a string.

   b) Replace the inner loop with a call to ``re.sub()``

   c) Finally, replace the outer loop with call to the ``map()``
      function, to apply this substitution to each token.

   d) Discuss the clarity (or otherwise) of each of these approaches.

3. Develop a simple extractive summarization tool, which prints the
   sentences of a document which contain the highest total word
   frequency.  Use ``FreqDist`` to count word frequencies, and use
   ``sum`` to sum the frequencies of the words in each sentence.
   Rank the sentences according to their score.  Finally, print the *n*
   highest-scoring sentences in document order.  Carefully review the
   design of your program, especially your approach to this double
   sorting.  Make sure the program is written as clearly as possible.

---------------
Further Reading
---------------

Regular Expressions
-------------------

A.M. Kuchling.
*Regular Expression HOWTO*,
http://www.amk.ca/python/howto/regex/


Algorithmic Problem Solving
---------------------------

David Harel (2004).
*Algorithmics: The Spirit of Computing* (Third Edition),
Addison Wesley.

Anany Levitin (2004).
*The Design and Analysis of Algorithms*,
Addison Wesley.

----

NLTK_

.. _NLTK: http://nltk.sourceforge.net/

