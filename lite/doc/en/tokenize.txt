.. -*- mode: rst -*-
.. include:: ../definitions.txt

============
Tokenization
============

:Authors: Steven Bird, Ewan Klein, Edward Loper
:Version: |version|
:Revision: $Revision$
:Date: $Date$
:Copyright: |copy| |copyrightinfo|
:License: |license|

.. Note:: This is a draft.  Please send any feedback to the authors.

------------
Introduction
------------

How do we know that piece of text is a *word*, and how do we represent
words and associated information in a machine?  It might seem
needlessly picky to ask what a word is. Can't we just say that a word
is a string of characters which has white space before and after it?
However, it turns out that things are quite a bit more complex. To get
a flavour of the problems, consider the following text from the Wall
Street Journal::

  Paragraph 12 from ``wsj_0034``

  It's probably worth paying a premium for funds that invest in markets
  that are partially closed to foreign investors, such as South Korea,
  some specialists say.  But some European funds recently have
  skyrocketed; Spain Fund has surged to a startling 120% premium.  It has
  been targeted by Japanese investors as a good long-term play tied to
  1992's European economic integration.  And several new funds that aren't
  even fully invested yet have jumped to trade at big premiums.

  "I'm very alarmed to see these rich valuations," says Smith Barney's
  Mr. Porter.

Let's start with the string ``aren't``. According to our naive
definition, it counts as only one word. But consider a situation where
we wanted to check whether all the words in our text occurred in a
dictionary, and our dictionary had entries for ``are`` and ``not``,
but not for ``aren't``.  In this case, we would probably be happy to
say that ``aren't`` is a contraction of two distinct words.


.. We can make a similar point about ``1992's``. We might want to run
   a small program over our text to extract all words which express
   dates. In this case, we would get achieve more generality by first
   stripping oexcept in this case, we would not expect to find
   ``1992`` in a dictionary.

If we take our naive definition of word literally (as we should, if we
are thinking of implementing it in code), then there are some other
minor but real problems. For example, assuming our file consists of a
number of separate lines, as in the WSJ text, then all the
words which come at the beginning of a line will fail to be preceded
by whitespace (unless we treat the newline character as a
whitespace). Second, according to our criterion, punctuation symbols
will form part of words; that is, a string like ``investors,`` will
also count as a word, since there is no whitespace between
``investors`` and the following comma. Consequently, we run the risk
of failing to recognise that ``investors,`` (with appended comma) is a
token of the same type as ``investors`` (without appended comma). More
importantly, we would like punctuation to be a "first-class citizen"
for tokenization and subsequent processing. For example, we might want
to implement a rule which says that a word followed by a period is
likely to be an abbreviation if the immediately following word has a
lowercase initial. However, to formulate such a rule, we must be able
to identify a period as a token in its own right.

A slightly different challenge is raised by examples such as the
following (drawn from the MedLine corpus):

#. This is a alpha-galactosyl-1,4-beta-galactosyl-specific adhesin.

#. The corresponding free cortisol fractions in these sera were 4.53
   +/- 0.15% and 8.16 +/- 0.23%, respectively.

In these cases, we encounter terms which are unlikely to be found in
any general purpose English lexicon. Moreover, we will have no success
in trying to syntactically analyse these strings using a standard
grammar of English. Now for some applications, we would like to
"bundle up" expressions such as
``alpha-galactosyl-1,4-beta-galactosyl-specific adhesin`` and ``4.53
+/- 0.15%`` so that they are presented as unanalysable atoms to the
parser. That is, we want to treat them as single "words" for the
purposes of subsequent processing.  The upshot is that, even if we
confine our attention to English text, the question of what we treat
as word may depend a great deal on what our purposes are.

.. Note:: If we turn to languages other than English, segmenting words
   can be even more of a challenge. For example, in Chinese
   orthography, characters correspond to monosyllabic morphemes. Many
   morphemes are words in their own right, but many words contain more
   than one morpheme; most of them consist of two morphemes. However,
   there is no visual representation of word boundaries in Chinese
   text.

Let's look in more detail at the words in the WSJ text.
Suppose we use white space as the delimiter for words, and then list
all the words of the text in alphabetical order; we would expect to
get something like the following::

  120, 1992, And, Barney, But, European, European, Fund, I, It, It,
  Japanese, Korea, Mr, Porter, Smith, South, Spain, a, a, a, ...

Now, if we ask a program utility to tell us how many words there in
the text, it will probably return the answer: 90.  This calculation
depends on treating each of the three occurrences of ``a`` as a
separate word. Yet what do we mean by saying that is some object ``a``
which occurs three times? Are there three words ``a`` or just one? We
can in fact answer "Both" if we draw a distinction between a word
*token* versus a word *type*.  A word type is somewhat abstract; it's
what we're talking about when we say that we know the meaning of the
word ``deprecate``, or when we say that the words ``barf`` and
``vomit`` are synonyms. On the other hand, a word token is something
which exists in time and space. For example, we could talk about my
uttering a token of the word ``grunge`` in Edinburgh on July 14, 2003;
equally, we can say that the second word token in the WSJ text is a
token of the word type ``probably``, or that there are two tokens of
the type ``European`` in the text.  More generally, we want to say
that there are 90 word tokens in the WSJ text, but only 76 word types.

The terms *token* and *type* can also be applied to other linguistic
entities.  For example, a *sentence token* is an individual occurrence
of a sentence; but a *sentence type* is an abstract sentence, without
context.  If someone repeats a sentence twice, they have uttered two
sentence tokens, but only one sentence type.  When the kind of token
or type is obvious from context, we will simply use the terms token
and type.

------------
Tokenization
------------

Representing tokens
-------------------

When written language is stored in a computer file it is normally
represented as a sequence or *string* of characters.  That is, in a
standard text file, individual words are strings, sentences are
strings, and indeed the whole text is one long string. The characters
in a string don't have to be just the ordinary alphanumerics; strings
can also include special characters which represent space, tab and
newline.

Most computational processing is performed above the level of
characters.  In compiling a programming language, for example, the
compiler expects its input to be a sequence of tokens that it knows
how to deal with; for example, the classes of identifiers, string
constants and numerals.  Analogously, a parser will expect its input
to be a sequence of word tokens rather than a sequence of individual
characters.  At its simplest, then, tokenization of a text involves
searching for locations in the string of characters containing
whitespace (space, tab, or newline) or certain punctuation symbols,
and breaking the string into word tokens at these points.  For
example, suppose we have a file containing the following two lines::

  The cat climbed
  the tree.

From the parser's point of view, this file is just a string of
characters:

  'The_cat_climbed\\n_the_tree.'

Note that we use single quotes to delimit strings, "_" to represent
space and "\n" to represent newline.

As we just pointed out, to tokenize this text for consumption by the
parser, we need to explicitly indicate which substrings are words. One
convenient way to do this in Python is to split the string into a
*list* of words, where each word is a string, such as
``'dog'``. [#]_ 
In Python, lists are printed as a series of objects
(in this case, strings), surrounded by square brackets and separated
by commas::

  >>> words = ['the', 'cat', 'climbed', 'the', 'tree']
  >>> words
  ['the', 'cat', 'climbed', 'the', 'tree']

.. [#] We say "convenient" because Python makes it easy to iterate
       through a list, processing the items one by one.

Notice that we have introduced a new variable ``words`` which is bound
to the list, and that we entered the variable on a new line to check
its value.

To summarize, we have just illustrated how, at its simplest,
tokenization of a text can be carried out by converting the single
string representing the text into a list of strings, each of which
corresponds to a word.

Texts: Sequences of Tokens
--------------------------

Many natural language processing tasks involve analyzing texts of
varying sizes, ranging from single sentences to very large corpora.
There are a number of ways to represent texts using NLTK.  The
simplest is as a single string.  These strings can be loaded directly
from files::

  >>> text_str = open('corpus.txt').read() 
  >>> text_str
  'Hello world.  This is a test file.\n'

However, as noted above, it is usually preferrable to represent a text
as a list of tokens.  These lists are typically created using a
*tokenizer*, such as ``tokenize.whitespace`` which splits strings into
words at whitespaces::

  >>> from nltk_lite import tokenize
  >>> text = 'Hello world.  This is a test string.'
  >>> list(tokenize.whitespace(text))
  ['Hello', 'world.', 'This', 'is', 'a', 'test', 'string.']

.. Note:: By "whitespace", we mean not only interword space, but
   also tab and line-end.

Note that tokenization may normalize the text, mapping all words to lowercase,
expanding contractions, and possibly even stemming the words.  An
example for stemming is shown below::

     >>> text = 'stemming can be fun and exciting'
     >>> tokens = tokenize.whitespace(text)
     >>> porter = tokenize.PorterStemmer()
     >>> for token in tokens:
     ...     print porter.stem(token),
     stem can be fun and excit

Tokenization based on whitespace is too simplistic for most
applications; for instance, it fails to separate the last word of a
phrase or sentence from punctuation characters, such as comma, period,
exclamation mark and question mark.  As its name suggests,
``tokenize.regexp`` employs a regular expression to determine how text
should be split up.  This regular expression specifies the characters
that can be included in a valid word.  To define a tokenizer that
includes punctuation as separate tokens, we could use:

  >>> text = '''Hello.  Isn't this fun?'''
  >>> pattern = r'\w+|[^\w\s]+'
  >>> list(tokenize.regexp(text, pattern))
  ['Hello', '.', 'Isn', "'", 't', 'this', 'fun', '?']

.. Tip:: Recall that ``\w+|[^\w\s]+`` is a disjunction of
   two subexpressions, namely ``w+`` and ``[^\w\s]+``. The first of
   these matches one or more "word" characters; i.e., characters other
   than whitespace or punctuation. The second pattern is a negated
   range expression; it matches on or more characters which are not
   word characters (i.e., not a match for ``\w``) and not a whitespace
   character (i.e., not a match for ``\s``).

.. Tip:: The regular expression in this example will match
   a sequence consisting of one or more word characters ``\w+``.  It
   will also match a sequence consisting of one or more punctuation
   characters (or non-word, non-space characters ``[^\w\s]+``).

There are a number of ways we might want to improve this regular
expression.  For example, it currently breaks the string
``'$22.50'`` into four tokens; but we might want it to include this
as a single token.  One approach to making this change would be
to add a new clause to the tokenizer's regular expression, which
is specialized for handling strings of this form::

  >>> text = 'That poster costs $22.40.'
  >>> pattern = r'\w+|\$\d+\.\d+|[^\w\s]+'
  >>> list(tokenize.regexp(text, pattern))
  ['That', 'poster', 'costs', '$22.40', '.']

It is sometimes more convenient to write a regular expression
matching the material that appears *between* tokens, such as whitespace
and punctuation.  The ``tokenize()`` function constructor permits
an optional boolean parameter ``gaps``; when set to ``True`` the
pattern is matched against the gaps.  For example, here is how
``whitespaceTokenize()`` is defined::

  >>> list(tokenize.regexp(text, pattern=r'\s+', gaps=True))
  ['That', 'poster', 'costs', '$22.40.']

The ``nltk_lite.corpora`` package provides ready access to several
corpora included with NLTK, along with built-in tokenizers.  For
example, ``brown.raw()`` is an iterator over sentences from
the Brown Corpus.  We use ``extract()`` to extract a sentence of
interest::

  >>> from nltk_lite.corpora import brown, extract
  >>> print extract(0, brown.raw('a'))
  ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', "Atlanta's", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', "''", 'that', 'any', 'irregularities', 'took', 'place', '.']

Exercises
---------

#. **Accessing and tokenizing a text file:** Obtain some plain text data
   (e.g. visit a web-page and save it as plain text), and store it in a
   file 'corpus.txt'.

   a. Using the ``open()`` and ``read()`` functions, load the text into
      a string variable and print it.

   #. Now, tokenize the text with ``tokenize.whitespace()``, and print
      the result.

   #. Next, compute the number of tokens, using the ``len()`` function,
      and print the result.

   #. Finally, discuss shortcomings of this method for tokenizing text.
      In particular, identify any material which has not been correctly
      tokenized.  (You may need to look for a more complex text.)

#. **Tokenizing text using regular expressions:**
   Obtain some plain text data (e.g. the Wall Street Journal example
   found at the start of this chapter, or else visit a web-page and
   save it as plain text), and store it in a file 'corpus.txt', then
   answer the following questions, using ``tokenize.regexp()``.

   a. Develop a regular expression to correctly identify the tokens.
      Pay particular attention to the treatment of punctation and numbers.

   #. Word processors typically hyphenate words when they are split
      across a linebreak.  When word-processed documents are converted to
      plain text, the pieces are usually not recombined.  It is easy to
      discover such texts by searching on the web for broken words,
      e.g. ``depart- ment``.  Create a tokenizer which treats such
      broken words as a single token.

   #. Consider the following book title: *This Is the Beat Generation:
      New York-San Francisco-Paris*.  What would it take to be able to
      tokenize such strings so that each city name was stored as a single
      token?

#. **Concordancing:**
   Write a function which takes a word as its argument, and which
   searches the Brown Corpus for instances of this word.  For each
   instance, generate a line of text with the target word in the middle.

-----------------------------
Simple Statistics with Tokens
-----------------------------

Counting Tokens
---------------

Perhaps the simplest thing to do once we have pulled tokens out of
text is to count them.  We can do this as follows, to compare the
lengths of the English and Finnish translations of the book of
Genesis::

  >>> from nltk_lite.corpora import genesis
  >>> len(list(genesis.raw('english-kjv')))
  38240
  >>> len(list(genesis.raw('finnish')))
  26595

We can do more sophisticated counting using *frequency distribution*.
In general, a frequency distribution records the number of times each
outcome of an experiment has occured.  For instance, a frequency
distribution could be used to record the frequency of each word in a
document.  Frequency distributions are generally initialized by
repeatedly running an experiment, and incrementing the count for a
sample every time it is an outcome of the experiment.  The following
program produces a frequency distribution that records how often each
word occurs in a text, and prints the most frequently occurring word:

  >>> from nltk_lite.probability import FreqDist
  >>> fd = FreqDist()
  >>> for token in genesis.raw():
  ...     fd.inc(token)
  >>> fd.max()
  'the'

Once we construct a frequency distribution that records
the outcomes of an experiment, we can use it to examine a number
of interesting properties of the experiment.  These properties
are summarized below:

==========  ==================  ===========================================
Frequency Distribution Module
---------------------------------------------------------------------------
Name        Sample              Description
==========  ==================  ===========================================
Count       fd.count('the')     number of times a given sample occurred
Frequency   fd.freq('the')      frequency of a given sample
N           fd.N()              number of samples
Samples     fd.samples()        list of distinct samples recorded
Max         fd.max()            sample with the greatest number of outcomes
==========  ==================  ===========================================

We can use a ``FreqDist`` to examine the distribution of word lengths
in a corpus.  For each word, we find its length, and increment the
count for words of this length.

  >>> def length_dist(text):
  ...     fd = FreqDist()                        # initialize an empty frequency distribution
  ...     for token in genesis.raw(text):        # for each token
  ...         fd.inc(len(token))                 # found another word with this length
  ...     for i in range(15):                    # for each length from 0 to 14
  ...         print "%2d" % int(100*fd.freq(i)), # print the percentage of words with this length
  ...     print

  >>> length_dist('english-kjv')
   0  2 14 28 21 13  7  5  2  2  0  0  0  0  0
  >>> length_dist('finnish')
   0  0  9  6 10 16 16 12  9  6  3  2  2  1  0

A *condition* specifies the context in which an experiment is
performed.  Often, we are interested in the effect that conditions
have on the outcome for an experiment.  For example, we might want to
examine how the distribution of a word's length (the outcome) is
affected by the word's initial letter (the condition).  Conditional
frequency distributions provide a tool for exploring this type of
question.

A *conditional frequency distribution*
is a collection of frequency distributions for the same
experiment, run under different conditions.  The individual
frequency distributions are indexed by the condition.

::

  >>> from nltk_lite.probability import ConditionalFreqDist
  >>> cfdist = ConditionalFreqDist()

  >>> for text in genesis.items:
  ...     for word in genesis.raw(text):
  ...         cfdist[text].inc(len(word))


To plot the results, we construct a list of points, where the x
coordinate is the word length, and the y coordinate is the frequency
with which that word length is used::

  >>> for cond in cfdist.conditions():
  ...     wordlens = cfdist[cond].samples()
  ...     wordlens.sort()
  ...     points = [(i, cfdist[cond].freq(i)) for i in wordlens]

We can plot these points using the ``Plot`` function defined in
``nltk_lite.draw.plot``, as follows: ``Plot(points).mainloop()``

Predicting the Next Word
------------------------

Conditional frequency distributions are often used for prediction.
*Prediction* is the problem of deciding a likely outcome for a given
run of an experiment.  The decision of which outcome to predict is
usually based on the context in which the experiment is performed.
For example, we might try to predict a word's text (outcome), based on
the text of the word that it follows (context).

To predict the outcomes of an experiment, we first examine a
representative *training corpus*, where the context and outcome for
each run of the experiment are known.  When presented with a new run
of the experiment, we simply choose the outcome that occured most
frequently for the experiment's context.

We can use a ``ConditionalFreqDist`` to find the most frequent
occurence for each context.  First, we record each outcome in the
training corpus, using the context that the experiment was run under
as the condition.  Then, we can access the frequency distribution for
a given context with the indexing operator, and use the ``max()``
method to find the most likely outcome.

We will now use a ``ConditionalFreqDist`` to predict the most likely
next word in a text.  To begin, we load a corpus from a text file, and
create an empty ``ConditionalFreqDist``::

  >>> from nltk_lite.probability import ConditionalFreqDist
  >>> cfdist = ConditionalFreqDist()

We then examine each token in the corpus, and increment the
appropriate sample's count.  We use the variable ``prev`` to record
the previous word.

  >>> prev = None
  >>> for word in genesis.raw():
  ...     cfdist[prev].inc(word)
  ...     prev = word

.. Note:: Sometimes the context for an experiment is unavailable, or
   does not exist.  For example, the first token in a text does not
   follow any word.  In these cases, we must decide what context to
   use.  For this example, we use ``None`` as the context for the
   first token.  Another option would be to discard the first token.

Once we have constructed a conditional frequency distribution for the
training corpus, we can use it to find the most likely word for any
given context. For example, taking the word ``living`` as our context,
we can inspect all the words that occurred in that context.

  >>> word = 'living'
  >>> cfdist['living'].samples()
  ['creature,', 'substance', 'soul.', 'thing', 'thing,', 'creature']

We can set up a simple loop to generate text: we set an initial
context, picking the most likely token in that context as our next
word, and then using that word as our new context:

  >>> word = 'living'
  >>> for i in range(20):
  ...     print word,
  ...     word = cfdist[word].max()
  living creature that he said, I will not be a wife of the land of the land of the land

This simple approach to text generation tends to get stuck in loops,
as demonstrated by the text generated above.  A more advanced approach
would be to randomly choose each word, with more frequent words chosen
more often.

Exercises
---------

#. **Zipf's Law**:
   Let *f(w)* be the frequency of a word *w* in free text. Suppose that
   all the words of a text are ranked according to their frequency,
   with the most frequent word first. Zipf's law states that the
   frequency of a word type is inversely proportional to its rank
   (i.e. f*r=k, for some constant k). For example, the 50th most
   common word type should occur three times as frequently as the
   150th most common word type.

   a) Write a Python function to process a large text and plot word
      frequency against word rank using the nltk_lite.draw.plot module. Do
      you confirm Zipf's law? (Hint: it helps to set the axes to log-log.) 
      What is going on at the extreme ends of the plotted line?

   #) Generate random text, e.g. using ``random.choice("abcdefg ")``,
      taking care to include the space character.  Use the string
      concatenation operator to accumulate characters into a (very)
      long string.  Then tokenize this string, and generate the Zipf
      plot as before, and compare the two plots.  What do you make of
      Zipf's Law in the light of this?

#. **Predicting the next word**: The word prediction program we saw in
   this chapter quickly gets stuck in a cycle.  Modify the program to
   choose the next word randomly, from a list of the *n* most likely
   words in the given context.  (Hint: store the *n* most likely words in
   a list ``lwords`` then randomly choose a word from the list using
   ``random.choice()``.)

   a) Select a particular genre, such as a section of the Brown Corpus,
      or a genesis translation, or one of the newsgroups corpora.  Train
      your system on this corpus and get it to generate random text.  You
      may have to experiment with different start words. How intelligible
      is the text?  Examine the strengths and weaknesses of this method of
      generating random text.

   #) Try the same approach with different genres, and with different
      amounts of training data.  What do you observe?

   #) Now train your system using two distinct genres and experiment
      with generating text in the hybrid genre.  As before, discuss your
      observations.

#. Write a program to implement one or more text readability scores
   (see ``http://en.wikipedia.org/wiki/Readability``).

#. (Advanced) **Statistically Improbable Phrases:**
   Design an algorithm to find the statistically improbable
   phrases of a document collection.
   http://www.amazon.com/gp/search-inside/sipshelp.html/

---------------
Further Reading
---------------

John Hopkins Center for Language and Speech Processing, 1999
Summer Workshop on Normalization of Non-Standard Words: Final Report
http://www.clsp.jhu.edu/ws99/projects/normal/report.pdf

SIL Glossary of Linguistic Terms:
http://www.sil.org/linguistics/GlossaryOfLinguisticTerms/

Language Files: Materials for an Introduction to Language and
Linguistics (Eighth Edition), The Ohio State University Department of
Linguistics, http://www.ling.ohio-state.edu/publications/files/



----

NLTK_

.. _NLTK: http://nltk.sourceforge.net/

