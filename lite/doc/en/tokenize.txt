.. -*- mode: rst -*-
.. include:: ../definitions.txt
   
=====
Words
=====

:Authors: Steven Bird, Ewan Klein, Edward Loper
:Version: |version|
:Revision: $Revision$
:Date: $Date$
:Copyright: |copy| |copyrightinfo|
:License: |license|

.. Note:: This is a draft.  Please send any feedback to the authors.

------------
Introduction
------------

Language can be divided up into pieces at a variety of different grain
sizes, ranging from sounds up to utterances and even to documents.  In
this chapter, we will focus on a very important level for much work in
computational linguistics: words.  Just what are words, and how should
we represent them in a machine?  At first, these may seem like trivial
questions, but it turns out that there are some important issues
involved in defining and encoding words.

.. It looks like the corpora module has changed a lot since the last
   time I looked -- do we not have access to *any* texts in a
   non-tokenized form??  That doesn't give the users much to play
   with.  

To help us explore these issues, consider the following simple task:
*count the number of words that appear in a document*.  We'll use the
following article (``wsj_0063``) from the Wall Street Journal as our
example text::

  Sea Containers Ltd. said it might increase the price of its
  $70-a-share buy-back plan if pressed by Temple Holdings Ltd., which
  made an earlier tender offer for Sea Containers.

      ...
  
  Mr. Sherwood said reaction to Sea Containers' proposal has been
  "very positive." In New York Stock Exchange composite trading
  yesterday, Sea Containers closed at $62.625, up 62.5 cents.

.. Should we use a smaller example, where we can include the whole
   text?  I picked this example by searching for a variety of
   interesting features (contains '--', contractions, numbers,
   quotes, the same word used in lower case & capitalized, etc).

To count the number of words in this article, we can simply divide it
up into a list of individual word strings, and check the length of
that list.  The most obvious way to divide the article up is to split
its text string on any sequence of spaces, newlines, or other
"whitespace" characters:

  >>> words = text.split()
  >>> print words
  ['Sea', 'Containers', 'Ltd.', 'said', 'it', 'might', ...]
  >>> len(words)
  426

However, there are a few problems with this solution.  First, it is not
clear that every sequence of non-whitespace characters should be
considered a word.  For example, our example article contains the
following sentence::

  A spokesman for Temple estimated that Sea Containers' plan -- if all
  the asset sales materialize -- would result in shareholders
  receiving only $36 to $45 a share in cash.

Our simple algorithm treats each occurrence of the sequence `--`:lx: as
a word; but most people would consider them to be punctuation marks,
not words.

Second, there may be some character sequences that are not separated
by whitespace, but which we still want to consider separate words.
One example of this is contractions, such as `didn't`:lx:.  If we're
analyzing the meaning of a sentence, it might be more useful to treat
this character string as two separate words, `did`:lx: and `not`:lx:.
Similarly, it sometimes makes more sense to treat hyphenated strings
as two words.  For example, in the expression `pro-New York`:lx:,
it seems quite unnatural to say that `pro-New`:lx: is a single word.

.. I know I've seen better examples of this than "pro-New York, but
   I can't think of one right now..
   
Finally, there may be times when we want to treat a character sequence
that contains whitespace as a single word.  For example, it is
sometimes useful to treat proper names such as "John Smith" or "New
York" as individual words, even though they contain spaces.

As an especially subtle example, consider the numeric expressions in
the following sentence (drawn from the MedLine corpus)::

  The corresponding free cortisol fractions in these sera were 4.53
  +/- 0.15% and 8.16 +/- 0.23%, respectively.

Should we say that the numeric expression "4.53 +/- 0.15%" is three
words?  Or should we say that it's a single compound word?  Or should
we say that it is actually *nine* words, since it's read "four point
five three, plus or minus fifteen percent"?  Or should we say that
it's not a "real" word at all, since it wouldn't appear in any
dictionary?  The answer will most likely depend on what task we're
trying to solve.

.. Note:: If we turn to languages other than English, segmenting words
   can be even more of a challenge. For example, in Chinese
   orthography, characters correspond to monosyllabic morphemes. Many
   morphemes are words in their own right, but many words contain more
   than one morpheme; most of them consist of two morphemes. However,
   there is no visual representation of word boundaries in Chinese
   text. For example, consider the following three-character string:
   |ai4| |guo3| |ren2| (in pinyin plus tones: ai4 'love' (verb), guo3
   'country', ren2 'person'). This could either be segmented as
   [|ai4| |guo3|] |ren2| |mdash| 'country-loving person' or as 
   |ai4| [|guo3| |ren2|] |mdash| 'love country-person'.

----------------
Tokens and Types
----------------
Let us return to our task: *count the number of words that appear in a
document*.  In the previous section, we interpreted "the number of
words" to mean "the number of times any word was used."  However,
another interpretation is possible: we might want to know how many
distinct "dictionary entries" were used, regardless of the number of
times that each dictionary item was repeated.  For example, the
following sentence contains seven "word uses" but only uses four
"dictionary entries"::

  John likes Mary and Mary likes John.

Two distinguish between these two interpretations, we will define two
new terms, "token" and "type."  A word *token* is an individual
occurrence of a word in a concrete context.  A word *type* is a
"dictionary entry" for a word.  A word type is somewhat abstract; it's
what we're talking about when we say that we know the meaning of the
word `deprecate`:lx:, or when we say that the words `barf`:lx: and
`vomit`:lx: are synonyms. On the other hand, a word token is something
which exists in time and space. For example, we could talk about my
uttering a token of the word `grunge`:lx: in Edinburgh on July 14, 2003;
equally, we can say that the last word token in the WSJ text is a
token of the word type `cents`:lx:, or that there are four tokens of
the type `Sea`:lx: in the text.

The terms *token* and *type* can also be applied to other linguistic
entities.  For example, a *sentence token* is an individual occurrence
of a sentence; but a *sentence type* is an abstract sentence, without
context.  If someone repeats a sentence twice, they have uttered two
sentence tokens, but only one sentence type.  When the kind of token
or type is obvious from context, we will simply use the terms token
and type.

We can use this new terminology to express the two interpretations for
our task unambiguously:

1.  Count the number of word tokens that appear in a document.
2.  Count the number of word types that appear in a document.

In the previous section, we considered how we might accomplish the
first of these tasks; now, let's turn our attention to the second
task, that of counting word types.  Again, we will start with a simple
approach: split the article's text on any sequence of whitespace, and
count the number of times each substring occurs.  But this time, we'll
use a `set`:lx: instead of a `list`:lx: to collect the words:

  >>> word_types = set()
  >>> for word in text.split():
  ...     word_types.add(word)
  >>> print word_types
  set(['$777', 'August,', 'surplus', 'issues,"', 'price.', 'The',
       'the', 'price', '--', 'legal', "didn't", 'asset', 'assets', ...])
  >>> print len(word_types)
  252

A quick glance at the contents of `word_types`:lx: reveals some of the
shortcomings of this approach.  These shortcomings can be divided into
two categories:

- *Tokenization*: Which substrings of the original text contain
  word tokens?
- *Type definition*: How do we decide whether two tokens have the
  same type?

We encountered many of the issues with tokenization when we looked at
the task of counting tokens.  For example, should `--`:lx: be counted
as a word token?  In addition to the issues we've already seen, our
algorithm's ignorance about punctuation can cause it to count the same
word type multiple times.  For example, the substrings `price.`:lx:
and `price`:lx: will be treated as two different word types. When we
were counting tokens, this didn't affect our overall answer, since
both `price.`:lx: and `price`:lx: count as a single token.  But since the
two strings are not identical, our algorithm counts them as two
separate types.  These tokenization issues could be addressed by
defining a more advanced algorithm for splitting the text into word
tokens (i.e., a *tokenizer*).

The type definition issues are exemplified by the fact that `the`:lx:
and `The`:lx: are listed as two separate word types.  We would like to
say that these two tokens have the same type, but just happen to be
written differently.  But since our simple algorithm uses strict
equality to divide word tokens into types, it treats them as two
distinct types.  A more subtle question is whether the two tokens
`asset`:lx: and `assets`:lx: should be considered to share a type or
not.  On the one hand, they would both be listed under the same entry
in a dictionary; but on the other hand, there is a definite semantic
difference between them.  As with some of the questions about
tokenization, our decision about whether to treat these two tokens as
having the same type or not will depend on the problem we're trying to
solve.

Example: Stylistics
-------------------

So far, we've seen how to count the number of tokens or types in a
document.  But it's much more interesting to look at *which* tokens or
types appear in a document.  We can use a Python dictionary to count the
number of occurrences of each word type in a document:

  >>> counts = {}
  >>> for word in text.split():
  ...     counts[word] = counts.get(word, 0) + 1

.. Do we need to remind the reader about dictionaries at this point?

The first statement, ``counts = {}``, initializes the dictionary,
while the next two lines successively add entries to it and increment
the count each time we encounter a new token of a given type.
In the expression ``counts.get(word, 0)``, ``0`` is a default value:
the expression will return ``counts[word]`` if word is already in the
dictionary, or 0 if it is not.  To view the contents of the
dictionary, we can iterate over its keys and print each entry:
    
  >>> for word in sorted(counts):
  ...     print counts[word], word
  1 $777
  1 August,
  1 surplus
  1 issues,"
    ...
  3 In
  1 at

We can also print the number of times that a specific word we're
interested in appeared:

  >>> print counts['might']
  3

Applying this same approach to document collections that are
categorized by genre, we can learn something about the patterns of word
usage in those genres.  For example, the following table was
constructed by counting the number of times various modal words appear
in different genres in the Brown Corpus:

==================  ===  =====  ===  =====  ====  ====
Use of Modals in Brown Corpus, by Genre
------------------------------------------------------
Genre               can  could  may  might  must  will 
==================  ===  =====  ===  =====  ====  ====
skill and hobbies   273  59     130  22     83    259 
humor               17   33     8    8      9     13 
fiction: science    16   49     4    12     8     16 
press: reportage    94   86     66   36     50    387 
fiction: romance    79   195    11   51     46    43 
religion            84   59     79   12     54    64 
==================  ===  =====  ===  =====  ====  ====

.. Discuss the table?

.. This might be a good place to put a couple exercises, to let them
   wet their feet.  E.g., look at the ratio of types:tokens in
   different genres.

------------
Tokenization
------------

*Tokenization* is the task of extracting a list of elementary tokens
that constitute a piece of language data.  As we've seen, tokenization
based solely on whitespace is too simplistic for most applications.
More advanced tokenization can be performed by using a regular
expression to specify which character sequences should be treated as
words.

.. Note:: We mainly focus on tokenization of text data into words,
   since it's such a common task, but tokenization can also be applied
   to different modalities, such as speech (or even sign language);
   and to different levels, such as phonemes or sentences.

.. tokenization of speech is a bit odd - this is usually called segmentation

.. I don't approve of defining a 'regexp' function inside a
   'regexp' module and then importing it into the parent package
   so that it's impossible to access the module. :-/

.. I strongly object to having the regexp tokenizer need an
   advanced=True argument to deal with standard regexps.

The function ``tokenize.regexp()`` takes a text string and a
regular expression, and returns the list of substrings that match the
regular expression.  To define a tokenizer that includes punctuation
as separate tokens, we could use:

  >>> text = '''Hello.  Isn't this fun?'''
  >>> pattern = r'\w+|[^\w\s]+'
  >>> list(tokenize.regexp(text, pattern))
  ['Hello', '.', 'Isn', "'", 't', 'this', 'fun', '?']

.. Tip:: Recall that ``\w+|[^\w\s]+`` is a disjunction of
   two subexpressions, namely ``w+`` and ``[^\w\s]+``. The first of
   these matches one or more "word" characters; i.e., characters other
   than whitespace or punctuation. The second pattern is a negated
   range expression; it matches on or more characters which are not
   word characters (i.e., not a match for ``\w``) and not a whitespace
   character (i.e., not a match for ``\s``).

.. Tip:: The regular expression in this example will match
   a sequence consisting of one or more word characters ``\w+``.  It
   will also match a sequence consisting of one or more punctuation
   characters (or non-word, non-space characters ``[^\w\s]+``).

There are a number of ways we might want to improve this regular
expression.  For example, it currently breaks the string
``'$22.50'`` into four tokens; but we might want it to treat this
as a single token.  One approach to making this change would be
to add a new clause to the tokenizer's regular expression, which
is specialized for handling strings of this form:

  >>> text = 'That poster costs $22.40.'
  >>> pattern = r'\w+|\$\d+\.\d+|[^\w\s]+'
  >>> list(tokenize.regexp(text, pattern))
  ['That', 'poster', 'costs', '$22.40', '.']

It is sometimes more convenient to write a regular expression
matching the material that appears *between* tokens, such as whitespace
and punctuation.  The ``tokenize.regexp()`` function permits
an optional boolean parameter ``gaps``; when set to ``True`` the
pattern is matched against the gaps.  For example, here is how
``tokenize.whitespace()`` is defined:

  >>> list(tokenize.regexp(text, pattern=r'\s+', gaps=True))
  ['That', 'poster', 'costs', '$22.40.']

.. exercises here

---------------
What is a Word?
---------------

Linguists have devoted considerable effort to distinguishing different
ways in which the term `word` is used. What we have focussed on so far
are often called `orthographic words`: strings of characters which can
be separated by various textual criteria such as the presence of
whitespace.
However, for linguists, the spoken word is often
regarded as having greater primacy, in the sense that natural
languages have all
evolved as spoken mediums, and children learn to speak long before
they can write and read (if indeed they ever do). Subjectively, we
hear spoken utterances as a succession of words, but it is rarely the
case that there are perceptible gaps between spoken words in
conversational speech. Nevertheless, spoken words can and do occur in
isolation. Using a variety of (sometime language-dependent) criteria,
certain phonological units are classed as `phonological words`, and
these units need not correspond to orthographic words: an example is
the utterance `wanna`:lx: as a contraction of the words `want`:lx: and `to`:lx:.
Returning to an example
mentioned before,    note that the form  `n't`:lx: in `didn't`:lx: cannot
form a phonological word by itself, and is sometimes called a `clitic`
or `leaner`; it needs to to combine with a 'host' word before it can be
uttered in normal speech.

Independent of the distinction between spoken and written language,
an important notion is that of a `lexeme` or `lexical item`. This
corresponds broadly to the notion of a word that you might look up in
a dictionary of English or some other language. For example, in order
to find the meaning of the word `said`:lx:, you need to know first that
it is a particular `grammatical form` of the lexeme `SAY`:lx:. (We adopt
the standard convention of representing lexemes with upper-case
forms.) Similarly,  `say`:lx:, `says`:lx: and `saying`:lx: are also
grammatical forms of `SAY`:lx:. While `said`:lx:, `says`:lx: and `saying`:lx:
are morphologically inflected, `say`:lx: lacks any morphological
inflection and is therefore termed the `base` form. In English, the
base form is conventionally used as the `lemma` (or `citation form`)
for a word. It is the lemma that is chosen to represent the
corresponding lexeme. (For example, the main entry for lexical item
will be listed under the word's lemma.)

Many of the word-like forms that occur in text have received little
attention from linguists but are nevertheless so prevalent that they
need to be dealt with in many NLP applications. These include
abbreviations such as `Dept.`:lx: or `Mr.`:lx: and acronyms such as `NATO`:lx:,
`Interpol`:lx: or `SQL`:lx:. Also of interest are symbols such as $ (`dollar`:lx:)
and @ (`at`:lx:). Unlike ordinary punctuation marks, these symbols
(sometimes called `logograms`) stand for words, and would be spoken as
such if the text were read aloud.


-------------
Normalisation
-------------

Earlier in this chapter, we talked about counting occurrences of word
in a text.  However, we may also be interesting in counting sequences
of words. Given any sequence of entities (e.g., letters, words,
numbers), an `N-gram`:dt: is any subsequence of length
`n`:math:. An N-gram of length 1 is called a `unigram`:dt:, 
of length 2 is a `bigram`:dt:, of length 3 is a `trigram`:dt:, and so
on. For example, in the sentence `She sells sea shells on the sea
shore`:lx:,
the unigrams are 
`She`:lx:,
`sells`:lx:,
`sea`:lx:,
`shells`:lx:,
`on`:lx:,
`the`:lx:,
`shore`:lx:;
the bigrams are
`She sells`:lx:,
`sells sea`:lx:,
`sea shells`:lx:,
`shells on`:lx:,
`on the`:lx:,
`the sea`:lx:,
`sea shore`:lx:;
and the trigrams are
`She sells sea`:lx:,
`sells sea shells`:lx:,
`sea shells on`:lx:,
`shells on the`:lx:,
`on the sea`:lx:,
`the sea shore`:lx:.

As a first approximation to discovering the distribution of a word, we
can look all the N-grams it occurs in.  More specifically, let's
consider all bigrams from the Brown Corpus which have the word
`often`:lx: as first element; that is, all the words which
are immediately preceded by `often`:lx:.
A small selection of these, ordered by their counts, is shown below::

  ,		16
  a		10
  in		8
  than		7
  the		7
  been		6
  do		5
  called  	4
  appear	3
  were		3
  appeared	2
  are		2
  did		2
  is		2
  appears	1
  call   	1

We can make a number of inferences from information like this. For
instance, the fact that `often`:lx: is frequently followed by a comma
suggests that it frequently occurs at the end of phrases. Not surprisingly, we
also see that `often`:lx: precedes verbs, presumably as an
adverbial modifier. Note, however, that although the list shows us how
many tokens of a specific type follow `often`:lx:, distinct
grammatical forms are treated as distinct types; for example,
`been`:lx:, `were`:lx:, `are`:lx: and `is`:lx: are listed
separately. If we were interested in what verbs can be modified by a
particular adverb, we might want to `lemmatise`:dt: them; that is,
replace the inflected forms by their lemma. Applied to the above list,
lemmatisation would yield the following results, which gives us a more compact
picture of the distribution of `often`:lx:. (As before, lexemes have
been represented as upper-case forms.)

  ::

   ,		16
   a		10
   BE		13
   in		8
   than		7
   the		7
   DO		7
   APPEAR	6
   CALL   	5

Lemmatisation is a relatively sophisticated process which requires a
mixture of rules for regular inflections and table look-up for irregular
morphological patterns. Within NLTK, a simpler approach is offered by
the Porter stemmer, which strips inflectional suffixes from words, but
does not attempt, for example, to identify `was`:lx: as a form of the
lexeme `BE`:lx:. Stemming in this fashion will succeed in collapsing
the different forms of `APPEAR`:lx: and `CALL`:lx:.

Lemmatisation and stemming can be regarded as special cases of
`normalisation`:dt:, in the sense of choosing one linguistic
expression as the canonical representative of a group of variants.  By
its nature, normalisation collapses distinctions. An example is case
normalisation, where all variants are mapped into a single
format. What counts as the normalised form will vary according to
context. Often, we convert everything into lower case, so that words
which were capitalized by virtue of being sentence-initial are treated
the same as those which occur elsewhere in the sentence. Case
normlisation will also collapse the `New`:lx: of `New York`:lx: with
the `new`:lx: of `my new car`:lx:.

Term variation is a particularly pervasive, and challenging, factor in
biomedical texts. To give just one example, the following are just
some of the possible
variants for nuclear factor |kappa| B, the name for a family of proteins::

  nuclear factor kappa B, nuclear factor Kappa-B, nuclear factor kappaB,
  nuclear factor kB, NF-KB, NF-kb, NF-kB, NFKB, NFkB, NF kappa-B

Although work is ongoing to standardise biomedical nomenclature, the
rate at which new entities are discovered and described vastly
outstrips these efforts at present.


.. This section should discuss how we define types.  I.e., how do we
   decide whether two tokens have the same type or not.  Note that this
   is a more general concept than normalization & stemming, but it's
   certainly related.

.. Is this notion of type the same as lexeme, or something else?

.. This section should talk about:

   - normalization (eg case normalization)
   - stemming
   - roots & inflection
   - porter stemmer
   - morphology?

.. Why is stemming stuff in the tokenization module?  Tokenization 
   and normalization are two totally separate concepts in my mind.

-----------------
Lexical Resources
-----------------
Discuss lexical resources:
  - what are they?
  - different notions of type
  - examples
    - finding rhyming words
    - using cmudict
    - pronunciation variation

-----------------------------
Simple Statistics with Tokens
-----------------------------

We can do more sophisticated counting using *frequency distributions*.
Abstractly, a frequency distribution is a record of the number of
times each *outcome* of an *experiment* has occurred.  For instance, a
frequency distribution could be used to record the frequency of each
word in a document (where the "experiment" is examining a word, and
the "outcome" is the word's type).  Frequency distributions are
generally created by repeatedly running an experiment, and
incrementing the count for a sample every time it is an outcome of the
experiment.  The following program produces a frequency distribution
that records how often each word type occurs in a text, and prints the
most frequently occurring word:

  >>> from nltk_lite.probability import FreqDist
  >>> fd = FreqDist()
  >>> for token in genesis.raw():
  ...     fd.inc(token)
  >>> fd.max()
  'the'

Once we construct a frequency distribution that records
the outcomes of an experiment, we can use it to examine a number
of interesting properties of the experiment.  Some of these properties
are summarized below:

==========  ==================  ===========================================
Frequency Distribution Module
---------------------------------------------------------------------------
Name        Sample              Description
==========  ==================  ===========================================
Count       fd.count('the')     number of times a given sample occurred
Frequency   fd.freq('the')      frequency of a given sample
N           fd.N()              number of samples
Samples     fd.samples()        list of distinct samples recorded
Max         fd.max()            sample with the greatest number of outcomes
==========  ==================  ===========================================

We can also use a ``FreqDist`` to examine the distribution of word lengths
in a corpus.  For each word, we find its length, and increment the
count for words of this length.

  >>> def length_dist(text):
  ...     fd = FreqDist()                        # initialize an empty frequency distribution
  ...     for token in genesis.raw(text):        # for each token
  ...         fd.inc(len(token))                 # found another word with this length
  ...     for i in range(15):                    # for each length from 0 to 14
  ...         print "%2d" % int(100*fd.freq(i)), # print the percentage of words with this length
  ...     print

  >>> length_dist('english-kjv')
   0  2 14 28 21 13  7  5  2  2  0  0  0  0  0
  >>> length_dist('finnish')
   0  0  9  6 10 16 16 12  9  6  3  2  2  1  0

Conditional Frequency Distributions
-----------------------------------

A *condition* specifies the context in which an experiment is
performed.  Often, we are interested in the effect that conditions
have on the outcome for an experiment.  For example, we might want to
examine how the distribution of a word's length (the outcome) is
affected by the word's initial letter (the condition).  Conditional
frequency distributions provide a tool for exploring this type of
question.

A *conditional frequency distribution*
is a collection of frequency distributions for the same
experiment, run under different conditions.  The individual
frequency distributions are indexed by the condition.

  >>> from nltk_lite.corpora import genesis
  >>> from nltk_lite.probability import ConditionalFreqDist
  >>> cfdist = ConditionalFreqDist()

  >>> for text in genesis.items:
  ...     for word in genesis.raw(text):
  ...         cfdist[text].inc(len(word))


To plot the results, we construct a list of points, where the x
coordinate is the word length, and the y coordinate is the frequency
with which that word length is used:

  >>> for cond in cfdist.conditions():
  ...     wordlens = cfdist[cond].samples()
  ...     wordlens.sort()
  ...     points = [(i, cfdist[cond].freq(i)) for i in wordlens]

We can plot these points using the ``Plot`` function defined in
``nltk_lite.draw.plot``, as follows: ``Plot(points).mainloop()``

Predicting the Next Word
------------------------

Conditional frequency distributions are often used for prediction.
*Prediction* is the problem of deciding a likely outcome for a given
run of an experiment.  The decision of which outcome to predict is
usually based on the context in which the experiment is performed.
For example, we might try to predict a word's text (outcome), based on
the text of the word that it follows (context).

To predict the outcomes of an experiment, we first examine a
representative *training corpus*, where the context and outcome for
each run of the experiment are known.  When presented with a new run
of the experiment, we simply choose the outcome that occurred most
frequently for the experiment's context.

We can use a ``ConditionalFreqDist`` to find the most frequent
occurrence for each context.  First, we record each outcome in the
training corpus, using the context that the experiment was run under
as the condition.  Then, we can access the frequency distribution for
a given context with the indexing operator, and use the ``max()``
method to find the most likely outcome.

We will now use a ``ConditionalFreqDist`` to predict the most likely
next word in a text.  To begin, we load a corpus from a text file, and
create an empty ``ConditionalFreqDist``::

  >>> from nltk_lite.corpora import genesis
  >>> from nltk_lite.probability import ConditionalFreqDist
  >>> cfdist = ConditionalFreqDist()

We then examine each token in the corpus, and increment the
appropriate sample's count.  We use the variable ``prev`` to record
the previous word.

  >>> prev = None
  >>> for word in genesis.raw():
  ...     cfdist[prev].inc(word)
  ...     prev = word

.. Note:: Sometimes the context for an experiment is unavailable, or
   does not exist.  For example, the first token in a text does not
   follow any word.  In these cases, we must decide what context to
   use.  For this example, we use ``None`` as the context for the
   first token.  Another option would be to discard the first token.

Once we have constructed a conditional frequency distribution for the
training corpus, we can use it to find the most likely word for any
given context. For example, taking the word `living`:lx: as our context,
we can inspect all the words that occurred in that context.

  >>> word = 'living'
  >>> cfdist[word].samples()
  ['creature,', 'substance', 'soul.', 'thing', 'thing,', 'creature']

We can set up a simple loop to generate text: we set an initial
context, picking the most likely token in that context as our next
word, and then using that word as our new context:

  >>> word = 'living'
  >>> for i in range(20):
  ...     print word,
  ...     word = cfdist[word].max()
  living creature that he said, I will not be a wife of the land of the land of the land

This simple approach to text generation tends to get stuck in loops,
as demonstrated by the text generated above.  A more advanced approach
would be to randomly choose each word, with more frequent words chosen
more often.

Exercises
---------

#. **Zipf's Law**:
   Let *f(w)* be the frequency of a word *w* in free text. Suppose that
   all the words of a text are ranked according to their frequency,
   with the most frequent word first. Zipf's law states that the
   frequency of a word type is inversely proportional to its rank
   (i.e. f*r=k, for some constant k). For example, the 50th most
   common word type should occur three times as frequently as the
   150th most common word type.

   a) Write a function to process a large text and plot word
      frequency against word rank using the nltk_lite.draw.plot module. Do
      you confirm Zipf's law? (Hint: it helps to set the axes to log-log.) 
      What is going on at the extreme ends of the plotted line?

   #) Generate random text, e.g. using ``random.choice("abcdefg ")``,
      taking care to include the space character.  Use the string
      concatenation operator to accumulate characters into a (very)
      long string.  Then tokenize this string, and generate the Zipf
      plot as before, and compare the two plots.  What do you make of
      Zipf's Law in the light of this?

#. **Predicting the next word**: The word prediction program we saw in
   this chapter quickly gets stuck in a cycle.  Modify the program to
   choose the next word randomly, from a list of the *n* most likely
   words in the given context.  (Hint: store the *n* most likely words in
   a list ``lwords`` then randomly choose a word from the list using
   ``random.choice()``.)

   a) Select a particular genre, such as a section of the Brown Corpus,
      or a genesis translation, or one of the Gutenberg texts.  Train
      your system on this corpus and get it to generate random text.  You
      may have to experiment with different start words. How intelligible
      is the text?  Discuss the strengths and weaknesses of this method of
      generating random text.

   #) Try the same approach with different genres, and with different
      amounts of training data.  What do you observe?

   #) Now train your system using two distinct genres and experiment
      with generating text in the hybrid genre.  As before, discuss your
      observations.

#. Write a program to implement one or more text readability scores
   (see http://en.wikipedia.org/wiki/Readability ).

#. (Advanced) **Statistically Improbable Phrases:**
   Design an algorithm to find the statistically improbable
   phrases of a document collection.
   http://www.amazon.com/gp/search-inside/sipshelp.html/

-------
Summary
-------

- Add a summary and/or conclusions section here that links into the
  following chapter on tagging.  Some possibilities:
  
    - word types collapse distinctions between tokens.  But
      sometimes we want to group tokens while preserving those
      distinctions.  I.e., tagging.

    - transition via lexical resources?

---------------
Further Reading
---------------

John Hopkins Center for Language and Speech Processing, 1999
Summer Workshop on Normalization of Non-Standard Words: Final Report
http://www.clsp.jhu.edu/ws99/projects/normal/report.pdf

SIL Glossary of Linguistic Terms:
http://www.sil.org/linguistics/GlossaryOfLinguisticTerms/

Language Files: Materials for an Introduction to Language and
Linguistics (Eighth Edition), The Ohio State University Department of
Linguistics, http://www.ling.ohio-state.edu/publications/files/

----

NLTK_

.. _NLTK: http://nltk.sourceforge.net/


.. ===================== UNUSED =====================

   Some of the following material might belong in other chapters

   Programming?
   ------------
   The ``nltk_lite.corpora`` package provides ready access to several
   corpora included with NLTK, along with built-in tokenizers.  For
   example, ``brown.raw()`` is an iterator over sentences from
   the Brown Corpus.  We use ``extract()`` to extract a sentence of
   interest:
   
     >>> from nltk_lite.corpora import brown, extract
     >>> print extract(0, brown.raw('a'))
     ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', "Atlanta's", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', "''", 'that', 'any', 'irregularities', 'took', 'place', '.']
   
   Old intro material
   ------------------
   How do we know that piece of text is a *word*, and how do we represent
   words and associated information in a machine?  It might seem
   needlessly picky to ask what a word is. Can't we just say that a word
   is a string of characters which has white space before and after it?
   However, it turns out that things are quite a bit more complex. To get
   a flavour of the problems, consider the following text from the Wall
   Street Journal::
   
     Let's start with the string `aren't`:lx:. According to our naive
     definition, it counts as only one word. But consider a situation where
     we wanted to check whether all the words in our text occurred in a
     dictionary, and our dictionary had entries for `are`:lx: and `not`:lx:,
     but not for `aren't`:lx:.  In this case, we would probably be happy to
     say that `aren't`:lx: is a contraction of two distinct words.
   
   
   .. We can make a similar point about `1992's`:lx:. We might want to run
      a small program over our text to extract all words which express
      dates. In this case, we would get achieve more generality by first
      stripping except in this case, we would not expect to find
      `1992`:lx: in a dictionary.
   
   If we take our naive definition of word literally (as we should, if we
   are thinking of implementing it in code), then there are some other
   minor but real problems. For example, assuming our file consists of a
   number of separate lines, as in the WSJ text, then all the
   words which come at the beginning of a line will fail to be preceded
   by whitespace (unless we treat the newline character as a
   whitespace). Second, according to our criterion, punctuation symbols
   will form part of words; that is, a string like `investors,`:lx: will
   also count as a word, since there is no whitespace between
   `investors`:lx: and the following comma. Consequently, we run the risk
   of failing to recognise that `investors,`:lx: (with appended comma) is a
   token of the same type as `investors`:lx: (without appended comma). More
   importantly, we would like punctuation to be a "first-class citizen"
   for tokenization and subsequent processing. For example, we might want
   to implement a rule which says that a word followed by a period is
   likely to be an abbreviation if the immediately following word has a
   lowercase initial. However, to formulate such a rule, we must be able
   to identify a period as a token in its own right.
   
   A slightly different challenge is raised by examples such as the
   following (drawn from the MedLine corpus):
   
   #. This is a alpha-galactosyl-1,4-beta-galactosyl-specific adhesin.
   
   #. The corresponding free cortisol fractions in these sera were 4.53
      +/- 0.15% and 8.16 +/- 0.23%, respectively.
   
   In these cases, we encounter terms which are unlikely to be found in
   any general purpose English lexicon. Moreover, we will have no success
   in trying to syntactically analyse these strings using a standard
   grammar of English. Now for some applications, we would like to
   "bundle up" expressions such as
   `alpha-galactosyl-1,4-beta-galactosyl-specific adhesin`:lx: and `4.53
   +/- 0.15%`:lx: so that they are presented as unanalysable atoms to the
   parser. That is, we want to treat them as single "words" for the
   purposes of subsequent processing.  The upshot is that, even if we
   confine our attention to English text, the question of what we treat
   as word may depend a great deal on what our purposes are.
   
   Representing tokens
   -------------------
   When written language is stored in a computer file it is normally
   represented as a sequence or *string* of characters.  That is, in a
   standard text file, individual words are strings, sentences are
   strings, and indeed the whole text is one long string. The characters
   in a string don't have to be just the ordinary alphanumerics; strings
   can also include special characters which represent space, tab and
   newline.
   
   Most computational processing is performed above the level of
   characters.  In compiling a programming language, for example, the
   compiler expects its input to be a sequence of tokens that it knows
   how to deal with; for example, the classes of identifiers, string
   constants and numerals.  Analogously, a parser will expect its input
   to be a sequence of word tokens rather than a sequence of individual
   characters.  At its simplest, then, tokenization of a text involves
   searching for locations in the string of characters containing
   whitespace (space, tab, or newline) or certain punctuation symbols,
   and breaking the string into word tokens at these points.  For
   example, suppose we have a file containing the following two lines::
   
     The cat climbed
     the tree.
   
   From the parser's point of view, this file is just a string of
   characters:
   
     'The_cat_climbed\\n_the_tree.'
   
   Note that we use single quotes to delimit strings, "_" to represent
   space and "\n" to represent newline.
   
   As we just pointed out, to tokenize this text for consumption by the
   parser, we need to explicitly indicate which substrings are words. One
   convenient way to do this in Python is to split the string into a
   *list* of words, where each word is a string, such as
   `'dog'`:lx:. [#]_ 
   In Python, lists are printed as a series of objects
   (in this case, strings), surrounded by square brackets and separated
   by commas:
   
     >>> words = ['the', 'cat', 'climbed', 'the', 'tree']
     >>> words
     ['the', 'cat', 'climbed', 'the', 'tree']
   
   .. [#] We say "convenient" because Python makes it easy to iterate
          through a list, processing the items one by one.
   
   Notice that we have introduced a new variable `words`:lx: which is bound
   to the list, and that we entered the variable on a new line to check
   its value.
   
   To summarize, we have just illustrated how, at its simplest,
   tokenization of a text can be carried out by converting the single
   string representing the text into a list of strings, each of which
   corresponds to a word.
   
   Some of this could maybe be discussed in the programming chapter?
   ----------------------------------------------------------------
   Many natural language processing tasks involve analyzing texts of
   varying sizes, ranging from single sentences to very large corpora.
   There are a number of ways to represent texts using NLTK.  The
   simplest is as a single string.  These strings can be loaded directly
   from files:
   
     >>> text_str = open('corpus.txt').read() 
     >>> text_str
     'Hello world.  This is a test file.\n'
   
   However, as noted above, it is usually preferable to represent a text
   as a list of tokens.  These lists are typically created using a
   *tokenizer*, such as `tokenize.whitespace`:lx: which splits strings into
   words at whitespaces:
   
     >>> from nltk_lite import tokenize
     >>> text = 'Hello world.  This is a test string.'
     >>> list(tokenize.whitespace(text))
     ['Hello', 'world.', 'This', 'is', 'a', 'test', 'string.']
   
   .. Note:: By "whitespace", we mean not only interword space, but
      also tab and line-end.
   
   Note that tokenization may normalize the text, mapping all words to lowercase,
   expanding contractions, and possibly even stemming the words.  An
   example for stemming is shown below:
   
        >>> text = 'stemming can be fun and exciting'
        >>> tokens = tokenize.whitespace(text)
        >>> porter = tokenize.PorterStemmer()
        >>> for token in tokens:
        ...     print porter.stem(token),
        stem can be fun and excit
   
   Tokenization based on whitespace is too simplistic for most
   applications; for instance, it fails to separate the last word of a
   phrase or sentence from punctuation characters, such as comma, period,
   exclamation mark and question mark.  As its name suggests,
   `tokenize.regexp`:lx: employs a regular expression to determine how text
   should be split up.  This regular expression specifies the characters
   that can be included in a valid word.  To define a tokenizer that
   includes punctuation as separate tokens, we could use:

