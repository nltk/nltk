<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="Docutils 0.3.10: http://docutils.sourceforge.net/" />
<title>Introduction to Natural Language Processing</title>
<meta name="author" content="Steven Bird" />
<meta name="author" content="Ewan Klein and Edward Loper" />
<meta name="date" content="2005-06-13" />
<meta name="copyright" content="© 2001-2005 University of Pennsylvania" />
<link rel="stylesheet" href=".." type="text/css" />
</head>
<body>
<div class="document" id="introduction-to-natural-language-processing">
<h1 class="title">Introduction to Natural Language Processing</h1>
<table class="docinfo" frame="void" rules="none">
<col class="docinfo-name" />
<col class="docinfo-content" />
<tbody valign="top">
<tr><th class="docinfo-name">Author:</th>
<td>Steven Bird</td></tr>
<tr><th class="docinfo-name">Author:</th>
<td>Ewan Klein and Edward Loper</td></tr>
<tr><th class="docinfo-name">Contact:</th>
<td><a class="first last reference" href="mailto:sb&#64;csse.unimelb.edu.au">sb&#64;csse.unimelb.edu.au</a></td></tr>
<tr><th class="docinfo-name">Version:</th>
<td>0.1</td></tr>
<tr><th class="docinfo-name">Revision:</th>
<td>1.1</td></tr>
<tr><th class="docinfo-name">Date:</th>
<td>2005-06-13</td></tr>
<tr><th class="docinfo-name">Copyright:</th>
<td>© 2001-2005 University of Pennsylvania</td></tr>
<tr class="field"><th class="docinfo-name">License:</th><td class="field-body">Creative Commons Attribution-NonCommercial-ShareAlike License</td>
</tr>
</tbody>
</table>
<!-- -*- mode: rst -*- -->
<blockquote class="epigraph">
The single and shortest definition of civilization may
be the word <em>language</em>...  Civilization, if it means something
concrete, is the conscious but unprogrammed mechanism by which humans
communicate.  And through communication they live with each other,
think, create, and act.  - John Ralston Saul</blockquote>
<div class="section" id="the-language-challenge">
<h1><a name="the-language-challenge">The Language Challenge</a></h1>
<p>Language is the chief manifestation of human intelligence.  Through
language we express basic needs and lofty aspirations, technical
know-how and flights of fantasy.  Ideas are shared over great
separations of distance and time.  The following samples from English
illustrate the richness of language:</p>
<ol class="arabic simple">
<li>Overhead the day drives level and grey, hiding the sun by a flight
of grey spears.  (William Faulkner, <em>As I Lay Dying</em>, 1935)</li>
<li>When using the toaster please ensure that the exhaust fan is turned
on. (sign in dormitory kitchen)</li>
<li>Amiodarone weakly inhibited CYP2C9, CYP2D6, and CYP3A4-mediated
activities with Ki values of 45.1-271.6 &amp;mu;M (Medline)</li>
<li>Iraqi Head Seeks Arms (spoof headline, <tt class="docutils literal"><span class="pre">http://www.snopes.com/humor/nonsense/head97.htm</span></tt></li>
<li>The earnest prayer of a righteous man has great power and wonderful
results. (James 5:16b)</li>
<li>Twas brillig, and the slithy toves did gyre and gimble in the wabe
(Lewis Carroll, <em>Jabberwocky</em>, 1872)</li>
<li>There are two ways to do this, AFAIK :smile:  (internet discussion archive)</li>
</ol>
<p>Thanks to this richness, the study of language is part of many
disciplines outside of linguistics, including translation, literary
criticism, philosophy, anthropology and psychology.  Many less obvious
disciplines investigate language use, such as law, hermeneutics,
forensics, telephony, pedagogy, archaeology, cryptanalysis and speech
pathology.  Each applies distinct methodologies to gather
observations, develop theories and test hypotheses.  Yet all serve to
deepen our understanding of language and of the intellect which is
manifested in language.</p>
<p>The importance of language to science and the arts is matched in
significance by the cultural treasure that is inherent in language.
Each of the world's ~7,000 human languages is rich in unique respects,
in its oral histories and creation legends, down to its grammatical
constructions and its very words and their nuances of meaning.
Threatened remnant cultures have words to distinguish plant subspecies
according to therapeutic uses which are unknown to science.  Languages
evolve over time as they come into contact with each other and they
provide a unique window onto human pre-history.  Technological change
gives rise to new words like <em>weblog</em> and new morphemes like <em>e-</em> and
<em>cyber-</em>.  In many parts of the world, small linguistic variations
from one town to the next add up to a completely different language in
the space of a half-hour drive.  For its breathtaking complexity and
diversity, human language is as a colourful tapestry stretching
through time and space.</p>
<p>Each new wave of computing technology has faced new challenges for
language analysis.  Early machine languages gave way to high-level
programming languages which are automatically parsed and interpreted.
Databases are interrogated using linguistic expressions like <tt class="docutils literal"><span class="pre">SELECT</span>
<span class="pre">age</span> <span class="pre">FROM</span> <span class="pre">employee</span></tt>.  Recently, computing devices have become
ubiquitous and are often equipped with multimodal interfaces
supporting text, speech, dialogue and pen gestures.  One way or
another, building new systems for natural linguistic interaction will
require sophisticated language analysis.</p>
<p>Today, the greatest challenge for language analysis is presented by
the explosion of text and multimedia content on the world-wide web.
For many people, a large and growing fraction of work and leisure time
is spent navigating and accessing this universe of information.  <em>What
tourist sites can I visit between Philadelphia and Pittsburgh on a
limited budget?  What do expert critics say about Canon digital
cameras?  What predictions about the steel market were made by
credible commentators in the past week?</em> Answering such questions
requires a combination of language processing tasks including
information extraction, inference, and summarisation.  The scale of
such tasks often calls for high-performance computing.</p>
<p>As we have seen, <em>natural language processing</em>, or NLP, is important
for scientific, economic, social, and cultural reasons.  NLP is
experiencing rapid growth as its theories and methods are deployed in
a variety of new language technologies.  For this reason it is
important for a wide range of people to have a working knowledge of
NLP.  Within academia, this includes people in areas from humanities
computing and corpus linguistics through to computer science and
artificial intelligence.  Within industry, this includes people in
human-computer interaction, business information analysis, and web
software development.  We hope that you, a member of this diverse
audience reading these materials, will come to appreciate the workings
of this rapidly growing field of NLP and will apply its techniques in
the solution of real-world problems.  The following chapters present a
carefully-balanced selection of theoretical foundations and practical
application, and equips readers to work with large datasets, to create
robust models of linguistic phenomena, and to deploy them in working
language technologies.  By integrating all of this with the Natural
Language Toolkit (NLTK), we hope this book opens up the exciting
endeavour of practical natural language processing to a broader
audience than ever before.</p>
<div class="note">
<p class="first admonition-title">Note</p>
<p class="last">An important aspect of learning NLP using these materials is
to experience both the challenge and &amp;mdash; we hope &amp;mdash; the
satisfaction of creating software to process natural language.  The
accompanying software, NLTK, is available for free and runs on most
operating systems including Linux/Unix, Mac OSX and Microsoft Windows.
You can download NTLK from <tt class="docutils literal"><span class="pre">nltk.sourceforge.net</span></tt>, along with
extensive documentation.  We encourage you to install NLTK on your
machine before reading beyond the end of this chapter.</p>
</div>
</div>
<div class="section" id="a-brief-history-of-natural-language-processing">
<h1><a name="a-brief-history-of-natural-language-processing">A Brief History of Natural Language Processing</a></h1>
<p>A long-standing challenge within computer science has been to build
intelligent machines.  The chief measure of machine intelligence has
been a linguistic one, namely the Turing Test.  Can a dialogue system,
responding to a user's typed input with its own textual output,
perform so naturally that users cannot distinguish it from a human
interlocutor using the same interface?  Today, there is substantial
ongoing research and development in such areas as machine translation
and spoken dialogue, and significant commercial systems are in
widespread use.  The following dialogue illustrates a typical
application:</p>
<blockquote>
<div class="line-block">
<div class="line">S: How may I help you?</div>
<div class="line">U: When is Saving Private Ryan playing?</div>
<div class="line">S: For what theater?</div>
<div class="line">U: The Paramount theater.</div>
<div class="line">S: Saving Private Ryan is not playing at the Paramount theater, but</div>
<div class="line-block">
<div class="line">it's playing at the Madison theater at 3:00, 5:30, 8:00, and 10:30. </div>
</div>
</div>
</blockquote>
<p>Today's commercial dialogue systems are strictly limited to
narrowly-defined domains. We could not ask the above system to provide
driving instructions or details of nearby restaurants unless the
requisite information had already been stored and suitable question
and answer sentences had been incorporated into the language
processing system.  Observe that the above system appears to
understand the user's goals: the user asks when a movie is showing and
the system correctly determines from this that the user wants to see
the movie. This inference seems so obvious to humans that we usually
do not even notice it has been made, yet a natural language system
needs to be endowed with this capability in order to interact
naturally. Without it, when asked &quot;Do you know when Saving Private
Ryan is playing&quot;, a system might simply &amp;mdash; and unhelpfully
&amp;mdash; respond with a cold &quot;Yes&quot;. While it appears that this
dialogue system can perform simple inferences, such sophistication is
only found in cutting edge research prototypes.  Instead, the
developers of commercial dialogue systems use contextual assumptions
and simple business logic to ensure that the different ways in which a
user might express requests or provide information are handled in a
way that makes sense for the particular application.  Thus, whether
the user says &quot;When is ...&quot;, or &quot;I want to know when ...&quot;, or
&quot;Can you tell me when ...&quot;, simple rules will always result in
users being presented with screening times.  This is sufficient for
the system to provide a useful service.</p>
<p>Despite some recent advances, it is generally true that those natural
language systems which have been fully deployed still cannot perform
common-sense reasoning or draw on world knowledge.  We can wait for
these difficult artificial intelligence problems to be solved, but in
the meantime it is necessary to live with some severe limitations on
the reasoning and knowledge capabilities of natural language
systems. Accordingly, right from the beginning, an important goal of
NLP research has been to make progress on the holy grail of natural
linguistic interaction <em>without</em> recourse to this unrestricted
knowledge and reasoning capability.  This is an old challenge, and so
it is instructive to review the history of the field.</p>
<p>The very notion that natural language could be treated in a
computational manner grew out of a research program, dating back to
the early 1900s, to reconstruct mathematical reasoning using logic,
most clearly manifested in the work by Frege, Russell, Wittgenstein,
Tarski, Lambek and Carnap.  This work led to the notion of language as
a formal system amenable to automatic processing.  Three later
developments laid the foundation for natural language processing.  The
first was <em>formal language theory</em>.  This defined a language as a set
of strings accepted by a class of automata, such as context-free
languages and pushdown automata, and provided the underpinnings for
computational syntax.</p>
<p>The second development was <em>symbolic logic</em>. This provided a formal
method for capturing selected aspects of natural language that are
relevant for expressing logical proofs. A formal calculus in symbolic
logic provides the syntax of a language, together with rules of
inference and, possibly, rules of interpretation in a set-theoretic
model; examples are propositional logic and first-order logic.  Given
such a calculus, with a well-defined syntax and semantics, it becomes
possible to associate meanings with expressions of natural language by
translating them into expressions of the formal calculus. For example,
if we translate <em>John saw Mary</em> into a formula <tt class="docutils literal"><span class="pre">saw(j,m)</span></tt>, we
(implicitly or explicitly) intepret the English verb <em>saw</em> as a binary
relation, and <em>John</em> and <em>Mary</em> as denoting individuals.  More general
statements like <em>All birds fly</em> require quantifiers, in this case
∠ meaning <em>for all</em>: * ∠ x: bird(x) → fly(x).*
This use of logic provided the technical machinery to perform
inferences that are an important part of language understanding. The
third development was the <em>principle of compositionality</em>. This was
the notion that the meaning of a complex expression is comprised of
the meaning of its parts and their mode of combination. This principle
provided a useful correspondence between syntax and semantics, namely
that the meaning of a complex expression could be computed
recursively.  Given the representation of <em>It is not true that
&amp;blank;:subscript:`p`</em> as <tt class="docutils literal"><span class="pre">not(p)</span></tt> and <em>John saw Mary</em> as
<tt class="docutils literal"><span class="pre">saw(j,m)</span></tt>, we can compute the interpretation of <em>It is not true
that John saw Mary</em> recursively using the above information to get
<tt class="docutils literal"><span class="pre">not(saw(j,m))</span></tt>. Today, this approach is most clearly manifested in
a family of grammar formalisms known as unification-based grammar, and
NLP applications implemented in the Prolog programming language.</p>
<p>A separate strand of development in the 1960s and 1970s eschewed the
declarative/procedural distinction and the principle of
compositionality.  They only seemed to get in the way of building
practical systems.  For example, early question answering systems
employed fixed pattern-matching templates such as: <tt class="docutils literal"><span class="pre">How</span> <span class="pre">many</span>
<span class="pre">&amp;blank;:subscript:`i`</span> <span class="pre">does</span> <span class="pre">&amp;blank;:subscript:`j`</span> <span class="pre">have?</span></tt>, where slot
<tt class="docutils literal"><span class="pre">i</span></tt> is a feature or service, and slot <tt class="docutils literal"><span class="pre">j</span></tt> is a person or place.
Each template came with a predefined semantic function, such as
<tt class="docutils literal"><span class="pre">count(i,j)</span></tt>.  A user's question which matched the template would be
mapped to the corresponding semantic function and then &quot;executed&quot; to
obtain an answer, <tt class="docutils literal"><span class="pre">k</span> <span class="pre">=</span> <span class="pre">count(i,j)</span></tt>.  This answer would be
substituted into a new template: <tt class="docutils literal"><span class="pre">&amp;blank;:subscript:`j`</span> <span class="pre">has</span>
<span class="pre">&amp;blank;:subscript:`k`</span> <span class="pre">&amp;blank;:subscript:`i`</span></tt>.  For example, the
question <em>How many airports:subscript:`i` does London:subscript:`j`
have?</em> can be mapped onto a template (as shown by the subscripts) and
translated to an executable program.  The result can be substituted
into a new template and returned to the user: <em>London has five
airports</em>.  Finally, the subscripts are removed and the natural
language answer is returned to the user.</p>
<p>This approach to NLP is known as <em>semantic grammar</em>.  Such grammars
are formalized like phrase-structure grammars, but their constituents
are no longer grammatical categories like noun phrase, but semantic
categories like <em>Airport</em> and <em>City</em>.  These grammars work very well
in limited domains, and are still widely used in spoken language
systems.  However, they suffer from brittleness, duplication of
grammatical structure in different semantic categories, and lack of
portability.</p>
<p>The contrasting approaches to NLP described in the preceding
paragraphs relates back to early metaphysical debates about
<em>rationalism</em> versus <em>empiricism</em> and <em>realism</em> versus <em>idealism</em> that
occurred in the Enlightenment period of Western philosophy.  These
debates took place against a backdrop of orthodox thinking in which
the source of all knowledge was believed to be divine revelation.
During this period of the seventeenth and eighteenth centuries,
philosophers argued that human reason or sensory experience has
priority over revelation.  Descartes and Leibniz, amongst others, took
the rationalist position, asserting that all truth has its origins in
human thought, and in the existence of &quot;innate ideas&quot; implanted in
our minds from birth.  For example, they saw that the principles of
Euclidean geometry were developed using human reason, and were not the
result of supernatural revelation or sensory experience.  In contrast,
Locke and others took the empiricist view, that our primary source of
knowledge is the experience of our faculties, and that human reason
plays a secondary role in reflecting on that experience.  Prototypical
evidence for this position was Galileo's discovery &amp;mdash; based on
careful observation of the motion of the planets &amp;mdash; that the
solar system is heliocentric and not geocentric.  In the context of
linguistics, this debate leads to the following question: to what
extent does human linguistic experience, versus our innate &quot;language
faculty&quot;, provide the basis for our knowledge of language?  In NLP
this matter surfaces as differences in the priority of corpus data
versus linguistic introspection in the construction of computational
models.</p>
<p>A further concern, enshrined in the debate between <em>realism</em> and
<em>idealism</em>, was the metaphysical status of the constructs of a theory.
Kant argued for a distinction between phenomena, the manifestations we
can experience, and &quot;things in themselves&quot; which can never been
known directly.  A linguistic realist would take a theoretical
construct like &quot;noun phrase&quot; to be real world entity that exists
independently of human perception and reason, and which actually
<em>causes</em> the observed linguistic phenomena.  A linguistic idealist, on
the other hand, would argue that noun phrases, along with more
abstract constructs like semantic representations, are intrinsically
unobservable, and simply play the role of useful fictions.  The way
linguists write about theories often betrays a realist position, while
NLP practitioners occupy neutral territory or else lean towards the
idealist position.</p>
<p>These issues are still alive today, and show up in the distinctions
between symbolic vs statistical methods, deep vs shallow processing,
binary vs gradient classifications, and scientific vs engineering
goals.  However, these contrasts are highly nuanced, and the debate is
no longer as polarised as it once was.  In fact, most of the
discussions -- and most of the advances even -- involve a
<em>balancing act</em> of the two extremes.  For example, one intermediate
position is to assume that humans are innately endowed with analogical
and memory-based learning methods (weak rationalism), and use these
methods to identify meaningful patterns in their sensory language
experience (empiricism).  For a more concrete illustration, consider
the way in which statistics from large corpora may serve as evidence
for binary choices in a symbolic grammar.  For instance, dictionaries
describe the words <em>absolutely</em> and <em>definitely</em> as nearly synonymous,
yet their patterns of usage are quite distinct when combined with a
following verb, as shown below:</p>
<table border="1" class="docutils">
<colgroup>
<col width="22%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="35%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head" colspan="5">Absolutely vs Definitely (Liberman 2005, LanguageLog.org)</th>
</tr>
<tr><th class="head">Google hits</th>
<th class="head">adore</th>
<th class="head">love</th>
<th class="head">like</th>
<th class="head">prefer</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>absolutely</td>
<td>289,000</td>
<td>905,000</td>
<td>16,200</td>
<td>644</td>
</tr>
<tr><td>definitely</td>
<td>1,460</td>
<td>51,000</td>
<td>158,000</td>
<td>62,600</td>
</tr>
<tr><td>ratio</td>
<td>198/1</td>
<td>18/1</td>
<td>1/10</td>
<td>1/97</td>
</tr>
</tbody>
</table>
<!-- http://itre.cis.upenn.edu/~myl/languagelog/archives/002022.html -->
<p>Observe that <em>absolutely adore</em> is about 200 times as popular as
<em>definitely adore</em>, while <em>absolutely prefer</em> is about 100 times rarer
then <em>definitely prefer</em>.  This information is used by statistical
language models, but it also counts as evidence for a symbolic account
of word combination in which <em>absolutely</em> can only modify extreme
actions or attributes.  This information could be represented as a
binary-valued feature of certain lexical items.  Thus, we see
statistical data informing symbolic models.  Now that this information
is codified, it is available to be exploited as a contextual feature
for a statistical language modelling, alongside many other rich
sources of symbolic information, like hand-constructed parse trees and
semantic representations.  Now the circle is closed, and we see
symbolic information informing statistical models.</p>
<p>This new rapprochement between high-church and low-church NLP is
giving rise to many exciting new developments.  We will touch on some
of these in the ensuing pages.  We too will perform this balancing
act, employing approaches to NLP that integrate these
historically-opposed philosophies and methodologies.</p>
</div>
<div class="section" id="an-nlp-application-information-extraction">
<h1><a name="an-nlp-application-information-extraction">An NLP Application: Information Extraction</a></h1>
<p>For many NLP applications, the priority is to extract meaning from
written text. This must be done <em>robustly</em>; processing should not fail
when unexpected or ill-formed input is received.  Today, robust
semantic interpretation is easiest when shallow processing methods are
used.  As we saw in the previous section, these methods severely limit
(or even omit) syntactic analysis and restrict the complexity of the
target semantic representations.</p>
<p>One well-known instance of shallow semantics is Information Extraction
(IE). In contrast to full text understanding, IE systems only try to
recognise a limited number of pre-specified semantic topics and use a
constrained semantic representation.  The initial phase of information
extraction involves the detection of named entities, expressions that
denote locations, people, companies, times, and monetary amounts.  (In
fact, these named entities are nothing other than the low-level
categories of the semantic grammars we saw in the previous section,
but under a new guise.)  To illustrate, the following text contains
several expressions which have been tagged as entities of types
<tt class="docutils literal"><span class="pre">time</span></tt>, <tt class="docutils literal"><span class="pre">company</span></tt> and <tt class="docutils literal"><span class="pre">location</span></tt>:</p>
<blockquote>
<div class="line-block">
<div class="line">The incident occurred &lt;time&gt;around 5.30pm&lt;/time&gt; when a man walked into</div>
<div class="line">&lt;company&gt;William Hill bookkeepers&lt;/company&gt; on &lt;location&gt;Ware Road&lt;/location&gt;,</div>
<div class="line">and threatened a member of staff with a small black handgun. </div>
</div>
</blockquote>
<p>The final output of an information extraction system, usually referred
to as a template, consists of fixed number of slots that must be
filled. More semantically, we can think of the template as denoting an
event in which the participating entities play specific roles.  Here
is an example corresponding to the above text:</p>
<blockquote>
<div class="line-block">
<div class="line">Crime event</div>
<div class="line-block">
<div class="line">Time: around 5.30pm</div>
<div class="line">Location: William Hill bookkeepers on Ware Road</div>
<div class="line">Suspect: man</div>
<div class="line">Weapon: small black handgun</div>
</div>
</div>
</blockquote>
<p>Early work in IE demonstrated that hand-built rules could achieve good
accuracy in the tasks defined by the DARPA Message Understanding
Conferences (MUC).  In general, highest accuracy has been achieved in
identifying named entities, such as <tt class="docutils literal"><span class="pre">William</span> <span class="pre">Hill</span> <span class="pre">bookkeepers</span></tt>.
However, it is harder to identify which slots the entities should
fill; and harder still to accurately identify which event the entities
are participating in.  In recent years attention has shifted to making
IE systems more portable to new domains by using some form of machine
learning.  These systems work moderately well on marked-up text
(e.g. HTML-formatted text), but their performance on free text still
needs improvement before they can be adopted more widely.</p>
<p>IE systems commonly use external knowledge sources in addition to the
input text.  For example, named entity recognizers often use name or
location gazetteers, extensive catalogues of people and places such as
the Getty Thesaurus of Geographic Names.  In the bioinformatics domain
it is common to draw on external knowledge bases for standard names of
genes and proteins.  A general purpose lexical ontology called WordNet
is often used in IE systems.  Sometimes formal domain ontologies are
used as a knowledge source for IE, permitting systems to perform
simple taxonomic inferences and resolve ambiguities in the
classification of slot-fillers.</p>
<p>For example, an IE system for extracting basketball statistics from
news reports on the web might have to interpret the sentence <em>Quincy
played three games last weekend</em>, where the ambiguous subject <em>Quincy</em>
might be either a player or a team.  An ontology in which <em>Quincy</em> is
identified as the name of a college basketball team would permit the
ambiguity to be resolved.  In other cases the template disambiguates
the slot filler.  Thus, in the basketball domain, <em>Washington</em> might
refer to one of many possible players, teams or locations.  If we know
that the verb <em>won</em> requires an entity whose role in the taxonomy is a
team, then we can resolve the ambiguity in the sentence <em>Washington won</em>.</p>
<!-- A Sample of NLP Tasks -->
<!-- a selection of: machine translation, dialog systems, document summarization,
information extraction, text retrieval, question answering -->
</div>
<div class="section" id="the-architecture-of-linguistic-and-nlp-systems">
<h1><a name="the-architecture-of-linguistic-and-nlp-systems">The Architecture of linguistic and NLP systems</a></h1>
<p>Within the approach to linguistic theory known as generative grammar,
it is claimed that humans have distinct kinds of linguistic knowledge,
organised into different modules: for example, knowledge of a
language's sound structure (phonology), knowledge of word structure
(morphology), knowledge of phrase structure (syntax), and knowledge of
meaning (semantics). In a formal linguistic theory, each kind of
linguistic knowledge is made explicit as different <em>module</em> of the
theory, consisting of a collection of basic elements together with a
way of combining them into complex structures. For example, a
phonological module might provide a set of phonemes together with an
operation for concatenating phonemes into phonological
strings. Similarly, a syntactic module might provide labelled nodes as
primitives together wih a mechanism for assembling them into trees. A
set of linguistic primitives, together with some operators for
defining complex elements, is often called a level of representation.</p>
<p>As well as defining modules, a generative grammar will prescribe how
the modules interact. For example, well-formed phonological strings
will provide the phonological content of words, and words will provide
the terminal elements of syntax trees. Well-formed syntactic trees
will be mapped to semantic representations, and contextual or
pragmatic information will ground these semantic representations in
some real-world situation.</p>
<p>As we indicated above, an important aspect of theories of generative
grammar is that they are intended to model the linguistic knowledge of
speakers and hearers; they are not intended to explain how humans
actually process linguistic information. This is, in part, reflected
in the claim that a generative grammer encodes the <em>competence</em> of an
idealized native speaker, rather than the speaker's <em>performance</em>. A
closely related distinction is to say that a generative grammar
encodes <em>declarative</em> rather than <em>procedural</em> knowledge.  As you
might expect, computational linguistics has the crucial role of
proposing procedural models of language. A central example is parsing,
where we have to develop computational mechanisms which convert
strings of words into structural representations such as syntax
trees. Nevertheless, it is widely accepted that well-engineered
computational models of language contain both declarative and
procedural aspects. Thus, a full account of parsing will say how
declarative knowledge in the form of a grammar and lexicon combines
with procedural knowledge which determines how a syntactic analysis
should be assigned to a given string of words. This procedural
knowledge will be expressed as an algorithm: that is, an explicit
recipe for mapping some input into an appropriate output in a finite
number of steps.</p>
<p>A simple parsing algorithm for context-free gramars, for instance,
looks first for a rule of the form <em>S |rarr| X:subscript:`1` &amp;mldr;
X:subscript:`n`</em>, and builds a partial tree structure. It then steps
through the grammar rules one-by-one, looking for a rule of the form
<em>X:subscript:`1` |rarr| Y:subscript:`1` ...  Y:subscript:`j`</em> which
will expand the leftmost daughter introduced by the <tt class="docutils literal"><span class="pre">S</span></tt> rule, and
further extends the partial tree. This process continues, for example
by looking for a rule of the form <em>Y:subscript:`1` |rarr|
Z:subscript:`1` ...  Z:subscript:`k`</em> and expanding the partial tree
appropriately, until the leftmost node label in the partial tree is a
lexical category; the parser then checks to see if the first word of
the input can belong to the category. To illustrate, let's suppose
that the first grammer rule chosen by the parser is <em>S |rarr| NP VP</em>
and the second rule chosen is <em>NP |rarr| Det N</em>; then the partial
tree will be:</p>
<div class="figure">
<img alt="../images/partialtree.png" src="../images/partialtree.png" />
<p class="caption">Partial Parse Tree</p>
</div>
<p>If we assume that the input string we are trying to parse is <em>the cat
slept</em>, we will succeed in identifying <em>the</em> as a word which can
belong to the category <tt class="docutils literal"><span class="pre">Det</span></tt>. In this case, the parser goes on to
the next node of the tree, <tt class="docutils literal"><span class="pre">N</span></tt>, and next input word, <em>cat</em>. However,
if we had built the same partial tree with an input string <em>did the
cat sleep</em>, the parse would fail at this point, since <em>did</em> is not of
category <tt class="docutils literal"><span class="pre">Det</span></tt>.  The parser would throw away the structure built so
far and look for an alternative way of going from the <tt class="docutils literal"><span class="pre">S</span></tt> node down
to a leftmost lexical category (e.g., using a rule <tt class="docutils literal"><span class="pre">S</span> <span class="pre">|rarr|</span> <span class="pre">V</span> <span class="pre">NP</span>
<span class="pre">VP</span></tt>). The important point for now is not the details of this or other
parsing algorithms; we discuss this topic much more fully in the
chapter on parsing. Rather, we just want to illustrate the idea that
an algorithm can be broken down into a fixed number of steps which
produce a definite result at the end.</p>
<p>In the following figure we further illustrate some of these points in
the context of a spoken dialogue system, such as our earlier example
of an application that offers the user information about movies
currently on show.</p>
<div class="figure">
<img alt="../images/dialogue.png" src="../images/dialogue.png" />
<p class="caption">Architecture of Spoken Dialogue System</p>
</div>
<p>Down the lefthand side of the diagram we have shown a pipeline of some
representative speech understanding <em>components</em>.  These map from
speech input via syntactic parsing to some kind of meaning
representation. Up the righthand side is an inverse pipeline of
components for concept-to-speech generation. These components
constitute the procedural aspect of the system's natural language
processing. In the central column of the diagram are some
representative declaratives aspects: the repositories of
language-related information which are called upon by the processing
components.</p>
<p>In addition to embodying the declarative/procedural distinction, the
diagram also illustrates that linguistically motivated ways of
modularizing linguistic knowledge are often reflected in computational
systems. That is, the various components are organized so that the
data which they exchange corresponds roughly to different levels of
representation. For example, the output of the speech analysis
component will contain sequences of phonological representations of
words, and the output of the parser will be a semantic representation.
Of course the parallel is not precise, in part because it is often a
matter of practical expedience where to place the boundaries between
different processing components. For example, we can assume that
within the parsing component there is a level of syntactic
representation, although we have chosen not to expose this at the
level of the system diagram.  Despite such idiosyncracies, most NLP
systems break down their work into a series of discrete steps.  In the
process of natural language understanding, these steps go from more
concrete levels to more abstract ones, while in natural language
production, the direction is reversed.</p>
</div>
<div class="section" id="the-python-programming-language">
<h1><a name="the-python-programming-language">The Python Programming Language</a></h1>
<p>NLTK is written in the Python language, a simple yet powerful
scripting language with excellent functionality for processing
linguistic data.  Python can be downloaded for free from
<tt class="docutils literal"><span class="pre">www.python.org</span></tt>.  Here is a five-line Python program which takes
text input and prints all the words ending in <tt class="docutils literal"><span class="pre">ing</span></tt>:</p>
<blockquote>
<div class="line-block">
<div class="line">import sys                            # load the system library</div>
<div class="line">for line in sys.stdin.readlines():    # for each line of input</div>
<div class="line-block">
<div class="line">for word in line.split():         # for each word in the line</div>
<div class="line-block">
<div class="line">if word.endswith('ing'):      # does the word end in 'ing'?</div>
<div class="line-block">
<div class="line">print word                # if so, print the word</div>
</div>
</div>
</div>
</div>
</blockquote>
<p>This program illustrates some of the main features of Python.  First,
whitespace is used to <em>nest</em> lines of code, thus the line starting
with <tt class="docutils literal"><span class="pre">if</span></tt> falls inside the scope of the previous line starting with
<tt class="docutils literal"><span class="pre">for</span></tt>, so the <tt class="docutils literal"><span class="pre">ing</span></tt> test is performed for each word.  Second,
Python is <em>object-oriented</em>; each variable is an entity which has
certain defined attributes and methods.  For example, <tt class="docutils literal"><span class="pre">line</span></tt> is more
than a sequence of characters.  It is a string object that has a
method (or operation) called <tt class="docutils literal"><span class="pre">split</span></tt> that we can use to break a line
into its words.  To apply a method to an object, we give the object
name, followed by a period, followed by the method name.  Third,
methods have <em>arguments</em> expressed inside parentheses.  For instance,
<tt class="docutils literal"><span class="pre">split</span></tt> had no argument because we were splitting the string
wherever there was white space.  To split a string into sentences
delimited by a period, we could write <tt class="docutils literal"><span class="pre">split('.')</span></tt>.  Finally, and
most importantly, Python is highly readable, so much so that it is
fairly easy to guess what the above program does even if you have
never written a program before.</p>
<p>This readability of Python is striking in comparison to other
languages which have been used for NLP, such as Perl.  Here is a Perl
program which prints words ending in <tt class="docutils literal"><span class="pre">ing</span></tt>:</p>
<blockquote>
<div class="line-block">
<div class="line">while (&lt;&gt;) {                          # for each line of input</div>
<div class="line-block">
<div class="line">foreach my $word (split) {        # for each word in a line</div>
<div class="line-block">
<div class="line">if ($word =~ /ing$/) {        # does the word end in 'ing'?</div>
<div class="line-block">
<div class="line">print &quot;$wordn&quot;;          # if so, print the word</div>
</div>
<div class="line">}</div>
</div>
<div class="line">}</div>
</div>
<div class="line">}</div>
</div>
</blockquote>
<p>Like Python, Perl is a scripting language.  However, it is not a real
object-oriented language, and its syntax is obscure.  For instance, it
is difficult to guess what kind of entities are represented by:
<tt class="docutils literal"><span class="pre">&amp;lt;&amp;gt;</span></tt>, <tt class="docutils literal"><span class="pre">$</span></tt>, <tt class="docutils literal"><span class="pre">my</span></tt>, and <tt class="docutils literal"><span class="pre">split</span></tt>.  We agree that &quot;it is
quite easy in Perl to write programs that simply look like raving
gibberish, even to experienced Perl programmers&quot; (Hammond 2003:47).
Having used Perl ourselves in research and teaching since the 1980s,
we have found that Perl programs of any size are inordinately
difficult to maintain and re-use.  Therefore we believe Perl is not an
optimal choice of programming language for linguists or for language
processing.  Several other languages are used for NLP, including
Prolog, Java, LISP and C.  In the appendix we have provided
translations of our five-line Python program into these and other
languages, and invite you to compare them for readability.</p>
<p>We chose Python as the implementation language for NLTK because it has
a shallow learning curve, its syntax and semantics are transparent,
and it has good string-handling functionality.  As a scripting
language, Python facilitates interactive exploration.  As an
object-oriented language, Python permits data and methods to be
encapsulated and re-used easily.  Python comes with an extensive
standard library, including components for graphical programming,
numerical processing, and web data processing.</p>
<p>NLTK defines a basic infrastructure that can be used to build NLP
programs in Python.  It provides:</p>
<ul class="simple">
<li>Basic classes for representing data relevant to natural language
processing.</li>
<li>Standard interfaces for performing tasks, such
as tokenization, tagging, and parsing.</li>
<li>Standard implementations for each task, which
can be combined to solve complex problems.</li>
<li>Extensive documentation, including tutorials
and reference documentation.</li>
</ul>
</div>
<div class="section" id="further-reading">
<h1><a name="further-reading">Further Reading</a></h1>
<p>The Association for Computational Linguistics (ACL) is the peak
professional body in NLP.  Its journal and conference proceedings,
approximately 10,000 articles, are available online with a full-text
search interface, via <tt class="docutils literal"><span class="pre">http://www.aclweb.org/anthology/</span></tt>.</p>
<p>Several NLP systems have online interfaces that you might like to
experiment with, e.g.:</p>
<ul class="simple">
<li>WordNet: <tt class="docutils literal"><span class="pre">http://wordnet.princeton.edu/</span></tt></li>
<li>Translation: <tt class="docutils literal"><span class="pre">http://world.altavista.com/</span></tt></li>
<li>ChatterBots: <tt class="docutils literal"><span class="pre">http://www.loebner.net/Prizef/loebner-prize.html</span></tt></li>
<li>Question Answering: <tt class="docutils literal"><span class="pre">http://www.answerbus.com/</span></tt></li>
<li>Summarisation: <tt class="docutils literal"><span class="pre">http://tangra.si.umich.edu/clair/md/demo.cgi</span></tt></li>
</ul>
<p>Useful websites with substantial information about NLP:
<tt class="docutils literal"><span class="pre">http://www.hltcentral.org/</span></tt>, <tt class="docutils literal"><span class="pre">http://www.lt-world.org/</span></tt>,
<tt class="docutils literal"><span class="pre">http://www.aclweb.org/</span></tt>, <tt class="docutils literal"><span class="pre">http://www.elsnet.org/</span></tt>.  The ACL
website contains an overview of computational linguistics, including
copies of introductory chapters from recent textbooks, at
<tt class="docutils literal"><span class="pre">http://www.aclweb.org/archive/what.html</span></tt>.</p>
<!-- Key papers that cover historical developments, ... -->
<p>Acknowledgements: The dialogue example is taken from Bob Carpenter and
Jennifer Chu-Carroll's ACL-99 Tutorial on Spoken Dialogue Systems; the
following people kindly provided program samples: Tim Baldwin, Trevor
Cohn, Rod Farmer, Edward Ivanovic, Olivia March, and Lars Yencken.</p>
<div class="section" id="nlp-textbooks-and-surveys">
<h2><a name="nlp-textbooks-and-surveys">NLP Textbooks and Surveys</a></h2>
<p>This section will give a brief overview of other NLP textbooks, and
field-wide surveys <em>(to be written).</em></p>
<ul class="simple">
<li>Recent textbooks: Manning &amp;amp; Schutze, Jurafsky &amp;amp; Martin.</li>
<li>Older textbooks: Allen (1995), Charniak (1993), Grishman.
Prolog-based: Covington (1994), Gazdar and Mellish (1989)
Pereira and Shieber; Mathematical foundations: Partee et al.</li>
<li>Recent field-wide surveys: Mitkov, Dale et al</li>
</ul>
</div>
</div>
<div class="section" id="appendix-nlp-in-other-programming-languages">
<h1><a name="appendix-nlp-in-other-programming-languages">Appendix: NLP in other Programming Languages</a></h1>
<p>Earlier we explained the thinking that lay behind our choice of the
Python programming language.  We showed a simple Python program that
reads in text and prints the words that end with <tt class="docutils literal"><span class="pre">ing</span></tt>.  In this
appendix we provide equivalent programs in other languages, so that
readers can gain a sense of the appeal of Python.</p>
<p>Prolog is a logic programming language which has been popular for
developing natural language parsers and feature-based grammars, given
the inbuilt support for search and the <em>unification</em> operation which
combines two feature structures into one.  Unfortunately Prolog is not
easy to use for string processing or input/output, as the following
program code demonstrates:</p>
<blockquote>
<div class="line-block">
<div class="line">main :-</div>
<div class="line-block">
<div class="line">current_input(InputStream),</div>
<div class="line">read_stream_to_codes(InputStream, Codes),</div>
<div class="line">codesToWords(Codes, Words),</div>
<div class="line">maplist(string_to_list, Words, Strings),</div>
<div class="line">filter(endsWithIng, Strings, MatchingStrings),</div>
<div class="line">writeMany(MatchingStrings),</div>
<div class="line">halt.</div>
<div class="line"><br /></div>
</div>
<div class="line">codesToWords([], []).</div>
<div class="line">codesToWords([Head | Tail], Words) :-</div>
<div class="line-block">
<div class="line">( char_type(Head, space) -&gt;</div>
<div class="line-block">
<div class="line">codesToWords(Tail, Words)</div>
</div>
<div class="line">;</div>
<div class="line-block">
<div class="line">getWord([Head | Tail], Word, Rest),</div>
<div class="line">codesToWords(Rest, Words0),</div>
<div class="line">Words = [Word | Words0]</div>
</div>
<div class="line">).</div>
<div class="line"><br /></div>
</div>
<div class="line">getWord([], [], []).</div>
<div class="line">getWord([Head | Tail], Word, Rest) :-</div>
<div class="line-block">
<div class="line">(</div>
<div class="line-block">
<div class="line">( char_type(Head, space) ; char_type(Head, punct) )</div>
</div>
<div class="line">-&gt;  Word = [], Tail = Rest</div>
<div class="line">;   getWord(Tail, Word0, Rest), Word = [Head | Word0]</div>
<div class="line">).</div>
<div class="line"><br /></div>
</div>
<div class="line">filter(Predicate, List0, List) :-</div>
<div class="line-block">
<div class="line">( List0 = [] -&gt; List = []</div>
<div class="line">;   List0 = [Head | Tail],</div>
<div class="line-block">
<div class="line">( apply(Predicate, [Head]) -&gt;</div>
<div class="line-block">
<div class="line">filter(Predicate, Tail, List1),</div>
<div class="line">List = [Head | List1]</div>
</div>
<div class="line">;   filter(Predicate, Tail, List)</div>
<div class="line">)</div>
</div>
<div class="line">).</div>
<div class="line"><br /></div>
</div>
<div class="line">endsWithIng(String) :- sub_string(String, _Start, _Len, 0, 'ing').</div>
<div class="line"><br /></div>
<div class="line">writeMany([]).</div>
<div class="line">writeMany([Head | Tail]) :- write(Head), nl, writeMany(Tail).</div>
</div>
</blockquote>
<p>Java is an object-oriented language incorporating native support for
the internet, that was originally designed to permit the same
executable program to be run on most computer platforms.  Java is
quickly replacing COBOL as the standard language for business
enterprise software.</p>
<blockquote>
<div class="line-block">
<div class="line">import java.io.*;</div>
<div class="line">public class IngWords {</div>
<div class="line-block">
<div class="line">public static void main(String[] args) {</div>
<div class="line-block">
<div class="line">BufferedReader in = new BufferedReader(new</div>
<div class="line">InputStreamReader(</div>
<div class="line-block">
<div class="line">System.in));</div>
</div>
<div class="line">String line = in.readLine();</div>
<div class="line">while (line != null) {</div>
<div class="line-block">
<div class="line">for (String word : line.split(&quot; &quot;)) {</div>
<div class="line-block">
<div class="line">if (word.endsWith(&quot;ing&quot;))</div>
<div class="line-block">
<div class="line">System.out.println(word);</div>
</div>
</div>
<div class="line">}</div>
<div class="line">line = in.readLine();</div>
</div>
<div class="line">}</div>
</div>
<div class="line">}</div>
</div>
<div class="line">}</div>
</div>
</blockquote>
<p>The C programming language is a highly-efficient low-level language
that is popular for operating system software and for teaching the
fundamentals of computer science.</p>
<blockquote>
<div class="line-block">
<div class="line">#include &lt;sys/types.h&gt;</div>
<div class="line">#include &lt;regex.h&gt;</div>
<div class="line">#include &lt;stdio.h&gt;</div>
<div class="line">#define BUFFER_SIZE 1024</div>
<div class="line"><br /></div>
<div class="line">int main(int argc, char **argv) {</div>
<div class="line-block">
<div class="line">regex_t space_pat, ing_pat;</div>
<div class="line">char buffer[BUFFER_SIZE];</div>
<div class="line">regcomp(&amp;space_pat, &quot;[, tn]+&quot;, REG_EXTENDED);</div>
<div class="line">regcomp(&amp;ing_pat, &quot;ing$&quot;, REG_EXTENDED | REG_ICASE);</div>
<div class="line"><br /></div>
<div class="line">while (fgets(buffer, BUFFER_SIZE, stdin) != NULL) {</div>
<div class="line-block">
<div class="line">char *start = buffer;</div>
<div class="line">regmatch_t space_match;</div>
<div class="line">while (regexec(&amp;space_pat, start, 1, &amp;space_match, 0) == 0) {</div>
<div class="line-block">
<div class="line">if (space_match.rm_so &gt; 0) {</div>
<div class="line-block">
<div class="line">regmatch_t ing_match;</div>
<div class="line">start[space_match.rm_so] = '0';</div>
<div class="line">if (regexec(&amp;ing_pat, start, 1, &amp;ing_match, 0) == 0)</div>
<div class="line-block">
<div class="line">printf(&quot;%sn&quot;, start);</div>
</div>
</div>
<div class="line">}</div>
<div class="line">start += space_match.rm_eo;</div>
</div>
<div class="line">}</div>
</div>
<div class="line">}</div>
<div class="line">regfree(&amp;space_pat);</div>
<div class="line">regfree(&amp;ing_pat);</div>
<div class="line"><br /></div>
<div class="line">return 0;</div>
</div>
<div class="line">}</div>
</div>
</blockquote>
<p>LISP is a so-called functional programming language, in which all
objects are lists, and all operations are performed by (nested)
functions of the form <tt class="docutils literal"><span class="pre">(function</span> <span class="pre">arg1</span> <span class="pre">arg2</span> <span class="pre">...)</span></tt>.  Many of the
earliest NLP systems were implemented in LISP.</p>
<blockquote>
<div class="line-block">
<div class="line">(defpackage &quot;REGEXP-TEST&quot; (:use &quot;LISP&quot; &quot;REGEXP&quot;))</div>
<div class="line">(in-package &quot;REGEXP-TEST&quot;)</div>
<div class="line"><br /></div>
<div class="line">(defun has-suffix (string suffix)</div>
<div class="line-block">
<div class="line">&quot;Open a file and look for words ending in _ing.&quot;</div>
<div class="line">(with-open-file (f string)</div>
<div class="line-block">
<div class="line">(with-loop-split (s f &quot; &quot;)</div>
<div class="line-block">
<div class="line">(mapcar #'(lambda (x) (has_suffix suffix x)) s))))</div>
<div class="line"><br /></div>
</div>
</div>
</div>
<div class="line">(defun has_suffix (suffix string)</div>
<div class="line-block">
<div class="line">(let* ((suffix_len (length suffix))</div>
<div class="line-block">
<div class="line">(string_len (length string))</div>
<div class="line-block">
<div class="line">(base_len (- string_len suffix_len)))</div>
<div class="line">(if (string-equal suffix string :start1 0 :end1 NIL :start2 base_len :end2 NIL)</div>
<div class="line-block">
<div class="line">(print string))))</div>
<div class="line"><br /></div>
</div>
</div>
</div>
</div>
<div class="line">(has-suffix &quot;test.txt&quot; &quot;ing&quot;)</div>
</div>
</blockquote>
<p>Haskell is another functional programming language which permits
a much more compact solution of our simple task:</p>
<blockquote>
<div class="line-block">
<div class="line">module Main</div>
<div class="line-block">
<div class="line">where main = interact (unlines.(filter ing).(map (filter isAlpha)).words)</div>
<div class="line-block">
<div class="line">where ing = (==&quot;gni&quot;).(take 3).reverse</div>
</div>
</div>
</div>
</blockquote>
</div>
</div>
</body>
</html>
