<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="Docutils 0.3.10: http://docutils.sourceforge.net/" />
<title>Elementary Language Processing: Tokenizing Text and Classifying Words</title>
<meta name="author" content="Steven Bird" />
<meta name="author" content="Ewan Klein" />
<meta name="author" content="Edward Loper" />
<meta name="date" content="2005-06-18" />
<meta name="copyright" content="© 2001-2005 University of Pennsylvania" />
<link rel="stylesheet" href="../default.css" type="text/css" />
</head>
<body>
<div class="document" id="elementary-language-processing-tokenizing-text-and-classifying-words">
<h1 class="title">Elementary Language Processing: Tokenizing Text and Classifying Words</h1>
<table class="docinfo" frame="void" rules="none">
<col class="docinfo-name" />
<col class="docinfo-content" />
<tbody valign="top">
<tr><th class="docinfo-name">Author:</th>
<td>Steven Bird</td></tr>
<tr><th class="docinfo-name">Author:</th>
<td>Ewan Klein</td></tr>
<tr><th class="docinfo-name">Author:</th>
<td>Edward Loper</td></tr>
<tr><th class="docinfo-name">Contact:</th>
<td><a class="first last reference" href="mailto:sb&#64;csse.unimelb.edu.au">sb&#64;csse.unimelb.edu.au</a></td></tr>
<tr><th class="docinfo-name">Version:</th>
<td>0.1</td></tr>
<tr><th class="docinfo-name">Revision:</th>
<td>1.2</td></tr>
<tr><th class="docinfo-name">Date:</th>
<td>2005-06-18</td></tr>
<tr><th class="docinfo-name">Copyright:</th>
<td>© 2001-2005 University of Pennsylvania</td></tr>
<tr class="field"><th class="docinfo-name">License:</th><td class="field-body">Creative Commons Attribution-NonCommercial-ShareAlike License</td>
</tr>
</tbody>
</table>
<!-- -*- mode: rst -*- -->
<div class="section" id="id1">
<span id="introduction"></span><h1><a name="id1">Introduction</a></h1>
<p>Texts are usually represented in a computer as files containing a
potentially long sequence of characters.  For most kinds of linguistic
processing, we need to identify and categorize the words of the text.
This turns out to be a non-trivial task.  In this chapter we introduce
<em>tokens</em> as the building blocks of text, and show how texts can be
<em>tokenized</em>.  Next we consider the categorization of tokens according
to their parts-of-speech, and do some preliminary exploration of the
Brown Corpus, a collection of over a million words of tagged English
text.  Along the way we take a look at some interesting applications:
generating random text, classifying words automatically, and analyzing
the modal verbs of different genres.</p>
</div>
<div class="section" id="tokens-the-building-blocks-of-text">
<h1><a name="tokens-the-building-blocks-of-text">Tokens: the building blocks of text</a></h1>
<p>How do we know that piece of text is a <em>word</em>, and how do we represent
words and associated information in a machine?  It might seem
needlessly picky to ask what a word is. Can't we just say that a word
is a string of characters which has white space before and after it?
However, it turns out that things are quite a bit more complex. To get
a flavour of the problems, consider the following text from the Wall
Street Journal:</p>
<p id="wsj-0034">blah:</p>
<pre class="literal-block">
Paragraph 12 from ``wsj_0034``

It's probably worth paying a premium for funds that invest in markets
that are partially closed to foreign investors, such as South Korea,
some specialists say.  But some European funds recently have
skyrocketed; Spain Fund has surged to a startling 120% premium.  It has
been targeted by Japanese investors as a good long-term play tied to
1992's European economic integration.  And several new funds that aren't
even fully invested yet have jumped to trade at big premiums.

&quot;I'm very alarmed to see these rich valuations,&quot; says Smith Barney's
Mr. Porter.
</pre>
<p>Let's start with the string <tt class="docutils literal"><span class="pre">aren't</span></tt>. According to our naive
definition, it counts as only one word. But consider a situation where
we wanted to check whether all the words in our text occurred in a
dictionary, and our dictionary had entries for <tt class="docutils literal"><span class="pre">are</span></tt> and <tt class="docutils literal"><span class="pre">not</span></tt>,
but not for <tt class="docutils literal"><span class="pre">aren't</span></tt>.  In this case, we would probably be happy to
say that <tt class="docutils literal"><span class="pre">aren't</span></tt> is a contraction of two distinct words.</p>
<!-- We can make a similar point about ``1992's``. We might want to run
a small program over our text to extract all words which express
dates. In this case, we would get achieve more generality by first
stripping oexcept in this case, we would not expect to find
``1992`` in a dictionary. -->
<p>If we take our naive definition of word literally (as we should, if we
are thinking of implementing it in code), then there are some other
minor but real problems. For example, assuming our file consists of a
number of separate lines, as indicated in <a class="reference" href="#wsj-0034">wsj_0034</a>, then all the
words which come at the beginning of a line will fail to be preceded
by whitespace (unless we treat the newline character as a
whitespace). Second, according to our criterion, punctuation symbols
will form part of words; that is, a string like <tt class="docutils literal"><span class="pre">investors,</span></tt> will
also count as a word, since there is no whitespace between
<tt class="docutils literal"><span class="pre">investors</span></tt> and the following comma. Consequently, we run the risk
of failing to recognise that <tt class="docutils literal"><span class="pre">investors,</span></tt> (with appended comma) is a
token of the same type as <tt class="docutils literal"><span class="pre">investors</span></tt> (without appended comma). More
importantly, we would like punctuation to be a &quot;first-class citizen&quot;
for tokenization and subsequent processing. For example, we might want
to implement a rule which says that a word followed by a period is
likely to be an abbreviation if the immediately following word has a
lowercase initial. However, to formulate such a rule, we must be able
to identify a period as a token in its own right.</p>
<p>A slightly different challenge is raised by examples such as the
following (drawn from the MedLine [ref] corpus):</p>
<ol class="arabic simple">
<li>This is a alpha-galactosyl-1,4-beta-galactosyl-specific adhesin.</li>
<li>The corresponding free cortisol fractions in these sera were 4.53
+/- 0.15% and 8.16 +/- 0.23%, respectively.</li>
</ol>
<p>In these cases, we encounter terms which are unlikely to be found in
any general purpose English lexicon. Moreover, we will have no success
in trying to syntactically analyse these strings using a standard
grammar of English. Now for some applications, we would like to
&quot;bundle up&quot; expressions such as
<tt class="docutils literal"><span class="pre">alpha-galactosyl-1,4-beta-galactosyl-specific</span> <span class="pre">adhesin</span></tt> and <tt class="docutils literal"><span class="pre">4.53</span>
<span class="pre">+/-</span> <span class="pre">0.15%</span></tt> so that they are presented as unanalysable atoms to the
parser. That is, we want to treat them as single &quot;words&quot; for the
purposes of subsequent processing.  The upshot is that, even if we
confine our attention to English text, the question of what we treat
as word may depend a great deal on what our purposes are.</p>
<div class="note">
<p class="first admonition-title">Note</p>
<p class="last">If we turn to languages other than English, segmenting words
can be even more of a challenge. For example, in Chinese
orthography, characters correspond to monosyllabic morphemes. Many
morphemes are words in their own right, but many words contain more
than one morpheme; most of them consist of two morphemes. However,
there is no visual representation of word boundaries in Chinese
text.</p>
</div>
<p>Let's look in more detail at the words in <a class="reference" href="#wsj-0034">wsj_0034</a>.
Suppose we use white space as the delimiter for words, and then list
all the words of the text in alphabetical order; we would expect to
get something like the following:</p>
<pre class="literal-block">
120, 1992, And, Barney, But, European, European, Fund, I, It, It,
Japanese, Korea, Mr, Porter, Smith, South, Spain, a, a, a, ...
</pre>
<p>Now, if we ask a program utility to tell us how many words there in
the text, it will probably return the answer: 90.  This calculation
depends on treating each of the three occurrences of <tt class="docutils literal"><span class="pre">a</span></tt> as a
separate word. Yet what do we mean by saying that is some object <tt class="docutils literal"><span class="pre">a</span></tt>
which occurs three times? Are there three words <tt class="docutils literal"><span class="pre">a</span></tt> or just one? We
can in fact answer &quot;Both&quot; if we draw a distinction between a word
<em>token</em> versus a word <em>type</em>.  A word type is somewhat abstract; it's
what we're talking about when we say that we know the meaning of the
word <tt class="docutils literal"><span class="pre">deprecate</span></tt>, or when we say that the words <tt class="docutils literal"><span class="pre">barf</span></tt> and
<tt class="docutils literal"><span class="pre">vomit</span></tt> are synonyms. On the other hand, a word token is something
which exists in time and space. For example, we could talk about my
uttering a token of the word <tt class="docutils literal"><span class="pre">grunge</span></tt> in Edinburgh on July 14, 2003;
equally, we can say that the second word token in &lt;xref <a class="reference" href="#wsj-0034">wsj_0034</a> is a
token of the word type <tt class="docutils literal"><span class="pre">probably</span></tt>, or that there are two tokens of
the type <tt class="docutils literal"><span class="pre">European</span></tt> in the text.  More generally, we want to say
that there are 90 word tokens in <a class="reference" href="#wsj-0034">wsj_0034</a>, but only 76 word
types.</p>
<p>The terms <em>token</em> and <em>type</em> can also be applied to other linguistic
entities.  For example, a <em>sentence token</em> is an individual occurrence
of a sentence; but a <em>sentence type</em> is an abstract sentence, without
context.  If someone repeats a sentence twice, they have uttered two
sentence tokens, but only one sentence type.  When the kind of token
or type is obvious from context, we will simply use the terms token
and type.</p>
<div class="section" id="representing-tokens">
<span id="token-representing"></span><h2><a name="representing-tokens">Representing tokens</a></h2>
<p>When written language is stored in a computer file it is normally
represented as a sequence or <em>string</em> of characters.  That is, in a
standard text file, individual words are strings, sentences are
strings, and indeed the whole text is one long string. The characters
in a string don't have to be just the ordinary alphanumerics; strings
can also include special characters which represent space, tab and
newline.</p>
<p>Most computational processing is performed above the level of
characters.  In compiling a programming language, for example, the
compiler expects its input to be a sequence of tokens that it knows
how to deal with; for example, the classes of identifiers, string
constants and numerals.  Analogously, a parser will expect its input
to be a sequence of word tokens rather than a sequence of individual
characters.  At its simplest, then, tokenization of a text involves
searching for locations in the string of characters containing
whitespace (space, tab, or newline) or certain punctuation symbols,
and breaking the string into word tokens at these points.  For
example, suppose we have a file containing the following two lines:</p>
<pre class="literal-block">
The cat climbed
the tree.
</pre>
<p>From the parser's point of view, this file is just a string of
characters:</p>
<pre class="literal-block">
'The ␣ cat ␣ climbed\nthe ␣ tree.'
</pre>
<p><em>[NB remove extra space from this???]</em></p>
<p>Note that we use single quotes to delimit strings,
&quot;␣&quot; to represent space and &quot;n&quot; to represent newline.</p>
<p>As we just pointed out, to tokenize this text for consumption by the
parser, we need to explicitly indicate which substrings are words. One
convenient way to do this in Python is to split the string into a
<em>list</em> of words, where each word is a string, such as
<tt class="docutils literal"><span class="pre">'dog'</span></tt>. <a class="footnote-reference" href="#id3" id="id2" name="id2">[1]</a> 
In Python, lists are printed as a series of objects
(in this case, strings), surrounded by square brackets and separated
by commas:</p>
<pre class="literal-block">
&gt;&gt;&gt; words = ['the', 'cat', 'climbed', 'the', 'tree']
&gt;&gt;&gt; words
['the', 'cat', 'climbed', 'the', 'tree']
</pre>
<table class="docutils footnote" frame="void" id="id3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2" name="id3">[1]</a></td><td>We say &quot;convenient&quot; because Python makes it easy to iterate
through a list, processing the items one by one.</td></tr>
</tbody>
</table>
<p>Notice that we have introduced a new variable <tt class="docutils literal"><span class="pre">words</span></tt> which is bound
to the list, and that we entered the variable on a new line to check
its value.</p>
<p>To summarize, we have just illustrated how, at its simplest,
tokenization of a text can be carried out by converting the single
string representing the text into a list of strings, each of which
corresponds to a word.</p>
</div>
</div>
<div class="section" id="tokenization">
<h1><a name="tokenization">Tokenization</a></h1>
<p>Many natural language processing tasks involve analyzing texts of
varying sizes, ranging from single sentences to very large corpora.
There are a number of ways to represent texts using NLTK.  The
simplest is as a single string.  These strings can be loaded directly
from files:</p>
<pre class="literal-block">
&gt;&gt;&gt; text_str = open('corpus.txt').read() 
&gt;&gt;&gt; text_str
'Hello world.  This is a test file.\n'
</pre>
<p>However, as we noted in <a class="reference" href="#token-representing">token.representing</a>, it is usually preferrable
to represent a text as a list of tokens.  These lists are typically
created using a <em>tokenizer</em>, such as <tt class="docutils literal"><span class="pre">tokenize.whitespace</span></tt> which
splits strings into words at whitespaces:</p>
<pre class="literal-block">
&gt;&gt;&gt; from nltk_lite import tokenize
&gt;&gt;&gt; text = 'Hello world.  This is a test string.'
&gt;&gt;&gt; list(tokenize.whitespace(text))
['Hello', 'world.', 'This', 'is', 'a', 'test', 'string.']
</pre>
<div class="note">
<p class="first admonition-title">Note</p>
<p class="last">By &quot;whitespace&quot;, we mean not only interword space, but
also tab and line-end.</p>
</div>
<div class="note">
<p class="first admonition-title">Note</p>
<p>Tokenization may normalize the text, mapping all words to
lowercase, expanding contractions, and possibly even stemming the
words.  An example for stemming is shown below:</p>
<pre class="last literal-block">
&gt;&gt;&gt; text = 'stemming can be fun and exciting'
&gt;&gt;&gt; tokens = tokenize.whitespace(text)
&gt;&gt;&gt; porter = tokenize.PorterStemmer()
&gt;&gt;&gt; for token in tokens:
...     print porter.stem(token),
stem can be fun and excit
</pre>
</div>
<p>We noted in the <a class="reference" href="#introduction">Introduction</a> that tokenization based on whitespace is
too simplistic for most applications; for instance, it fails to
separate the last word of a phrase or sentence from punctuation
characters, such as comma, period, exclamation mark and question mark.
As its name suggests, <tt class="docutils literal"><span class="pre">tokenize.regexp</span></tt> employs a regular expression
to determine how text should be split up.  This regular expression
specifies the characters that can be included in a valid word.  To
define a tokenizer that includes punctuation as separate tokens, we
could use:</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; text = '''Hello.  Isn't this fun?'''
&gt;&gt;&gt; pattern = r'\w+|[^\w\s]+'
&gt;&gt;&gt; list(tokenize.regexp(text, pattern))
['Hello', '.', 'Isn', &quot;'&quot;, 't', 'this', 'fun', '?']
</pre>
</blockquote>
<div class="tip">
<p class="first admonition-title">Tip</p>
<p class="last">Recall that <tt class="docutils literal"><span class="pre">\w+|[^\w\s]+</span></tt> is a disjunction of
two subexpressions, namely <tt class="docutils literal"><span class="pre">w+</span></tt> and <tt class="docutils literal"><span class="pre">[^\w\s]+</span></tt>. The first of
these matches one or more &quot;word&quot; characters; i.e., characters other
than whitespace or punctuation. The second pattern is a negated
range expression; it matches on or more characters which are not
word characters (i.e., not a match for <tt class="docutils literal"><span class="pre">\w</span></tt>) and not a whitespace
character (i.e., not a match for <tt class="docutils literal"><span class="pre">\s</span></tt>).</p>
</div>
<div class="tip">
<p class="first admonition-title">Tip</p>
<p class="last">The regular expression in this example will match
a sequence consisting of one or more word characters <tt class="docutils literal"><span class="pre">\w+</span></tt>.  It
will also match a sequence consisting of one or more punctuation
characters (or non-word, non-space characters <tt class="docutils literal"><span class="pre">[^\w\s]+</span></tt>).</p>
</div>
<p>There are a number of ways we might want to improve this regular
expression.  For example, it currently breaks the string
<tt class="docutils literal"><span class="pre">'$22.50'</span></tt> into four tokens; but we might want it to include this
as a single token.  One approach to making this change would be
to add a new clause to the tokenizer's regular expression, which
is specialized for handling strings of this form:</p>
<pre class="literal-block">
&gt;&gt;&gt; text = 'That poster costs $22.40.'
&gt;&gt;&gt; pattern = r'\w+|\$\d+\.\d+|[^\w\s]+'
&gt;&gt;&gt; list(tokenize.regexp(text, pattern))
['That', 'poster', 'costs', '$22.40', '.']
</pre>
<p>It is sometimes more convenient to write a regular expression
matching the material that appears <em>between</em> tokens, such as whitespace
and punctuation.  The <tt class="docutils literal"><span class="pre">tokenize()</span></tt> function constructor permits
an optional boolean parameter <tt class="docutils literal"><span class="pre">gaps</span></tt>; when set to <tt class="docutils literal"><span class="pre">True</span></tt> the
pattern is matched against the gaps.  For example, here is how
<tt class="docutils literal"><span class="pre">whitespaceTokenize()</span></tt> is defined:</p>
<pre class="literal-block">
&gt;&gt;&gt; list(tokenize.regexp(text, pattern=r'\s+', gaps=True))
['That', 'poster', 'costs', '$22.40.']
</pre>
<p>The <tt class="docutils literal"><span class="pre">nltk_lite.corpora</span></tt> package provides ready access to several
corpora included with NLTK, along with built-in tokenizers.  For
example, <tt class="docutils literal"><span class="pre">brown.tagged()</span></tt> is an iterator over tagged sentences
from the Brown Corpus.  We use <tt class="docutils literal"><span class="pre">islice()</span></tt> to specify the range
of sentences of interest:</p>
<pre class="literal-block">
&gt;&gt;&gt; from nltk_lite.corpora import brown
&gt;&gt;&gt; from itertools import islice
&gt;&gt;&gt; for sent in islice(brown.tagged('a'), 1):
...     print sent
[('The', 'at'), ('Fulton', 'np-tl'), ('County', 'nn-tl'), ('Grand', 'jj-tl'), ('Jury', 'nn-tl'), ('said', 'vbd'), ('Friday', 'nr'), ('an', 'at'), ('investigation', 'nn'), ('of', 'in'), (&quot;Atlanta's&quot;, 'np$'), ('recent', 'jj'), ('primary', 'nn'), ('election', 'nn'), ('produced', 'vbd'), ('``', '``'), ('no', 'at'), ('evidence', 'nn'), (&quot;''&quot;, &quot;''&quot;), ('that', 'cs'), ('any', 'dti'), ('irregularities', 'nns'), ('took', 'vbd'), ('place', 'nn'), ('.', '.')]
</pre>
<p>In particular, <tt class="docutils literal"><span class="pre">brown.tagged()</span></tt> doesn't just read the
relevant text into a string, it also invokes the appropriate
tokenizer.</p>
<!-- frequency: -->
</div>
<div class="section" id="counting-tokens">
<h1><a name="counting-tokens">Counting Tokens</a></h1>
<p>Perhaps the simplest thing to do once we have pulled tokens out of
text is to count them.  We can do this as follows, to compare the
lengths of the English and Finnish translations of the book of
Genesis:</p>
<pre class="literal-block">
&gt;&gt;&gt; from nltk_lite.corpora import genesis
&gt;&gt;&gt; len(list(genesis.raw('english-kjv')))
38240
&gt;&gt;&gt; len(list(genesis.raw('finnish')))
26595
</pre>
<p>We can do more sophisticated counting using <em>frequency distribution</em>.
In general, a frequency distribution records the number of times each
outcome of an experiment has occured.  For instance, a frequency
distribution could be used to record the frequency of each word in a
document.  Frequency distributions are generally initialized by
repeatedly running an experiment, and incrementing the count for a
sample every time it is an outcome of the experiment.  The following
program produces a frequency distribution that records how often each
word occurs in a text, and prints the most frequently occurring word:</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; from nltk.probability import FreqDist
&gt;&gt;&gt; fd = FreqDist()
&gt;&gt;&gt; for token in genesis.raw():
...     fd.inc(token)
&gt;&gt;&gt; fd.max()
'the'
</pre>
</blockquote>
<p>Once we construct a frequency distribution that records
the outcomes of an experiment, we can use it to examine a number
of interesting properties of the experiment.  These properties
are summarized below:</p>
<table border="1" class="docutils">
<colgroup>
<col width="14%" />
<col width="25%" />
<col width="61%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head" colspan="3">Frequency Distribution Module</th>
</tr>
<tr><th class="head">Name</th>
<th class="head">Sample</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>Count</td>
<td>fd.count('the')</td>
<td>number of times a given sample occurred</td>
</tr>
<tr><td>Frequency</td>
<td>fd.freq('the')</td>
<td>frequency of a given sample</td>
</tr>
<tr><td>N</td>
<td>fd.N()</td>
<td>number of samples</td>
</tr>
<tr><td>Samples</td>
<td>fd.samples()</td>
<td>list of distinct samples recorded</td>
</tr>
<tr><td>Max</td>
<td>fd.max()</td>
<td>sample with the greatest number of outcomes</td>
</tr>
</tbody>
</table>
<p>We can use a <tt class="docutils literal"><span class="pre">FreqDist</span></tt> to examine the distribution of word lengths
in a corpus.  For each word, we find its length, and increment the
count for words of this length.</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; def length_dist(text):
...     fd = FreqDist()                        # initialize an empty frequency distribution
...     for token in genesis.raw():            # for each token
...         fd.inc(len(token))                 # found another word with this length
...     for i in range(15):                    # for each length from 0 to 14
...         print &quot;%2d&quot; % int(100*fd.freq(i)), # print the percentage of words with this length
...     print
</pre>
<pre class="doctest-block">
&gt;&gt;&gt; length_dist('english-kjv.txt')
 0  2 14 28 21 13  7  5  2  2  0  0  0  0  0
&gt;&gt;&gt; length_dist('finnish.txt')
 0  2 14 28 21 13  7  5  2  2  0  0  0  0  0
</pre>
</blockquote>
<p>A <em>condition</em> specifies the context in which an experiment is
performed.  Often, we are interested in the effect that conditions
have on the outcome for an experiment.  For example, we might want to
examine how the distribution of a word's length (the outcome) is
affected by the word's initial letter (the condition).  Conditional
frequency distributions provide a tool for exploring this type of
question.</p>
<p>A <em>conditional frequency distribution</em>
is a collection of frequency distributions for the same
experiment, run under different conditions.  The individual
frequency distributions are indexed by the condition.</p>
<pre class="literal-block">
&gt;&gt;&gt; from nltk.probability import ConditionalFreqDist
&gt;&gt;&gt; cfdist = ConditionalFreqDist()

&gt;&gt;&gt; for text in genesis.items:
...     for word in genesis.raw(text):
...         cfdist[text].inc(len(word))
</pre>
<p>To plot the results, we construct a list of points, where the x
coordinate is the word length, and the y coordinate is the frequency
with which that word length is used:</p>
<pre class="literal-block">
&gt;&gt;&gt; for cond in cfdist.conditions():
...     wordlens = cfdist[cond].samples()
...     wordlens.sort()
...     points = [(i, cfdist[cond].freq(i)) for i in wordlens]
</pre>
<div class="note">
<p class="first admonition-title">Note</p>
<p class="last">Once the plotting module is ported to nltk_lite, it will be
possible to do: <tt class="docutils literal"><span class="pre">Plot(points).mainloop()</span></tt></p>
</div>
<!-- frequency.predicting: -->
</div>
<div class="section" id="an-application-predicting-the-next-word">
<h1><a name="an-application-predicting-the-next-word">An Application: Predicting the Next Word</a></h1>
<p>Conditional frequency distributions are often used for prediction.
<em>Prediction</em> is the problem of deciding a likely outcome for a given
run of an experiment.  The decision of which outcome to predict is
usually based on the context in which the experiment is performed.
For example, we might try to predict a word's text (outcome), based on
the text of the word that it follows (context).</p>
<p>To predict the outcomes of an experiment, we first examine a
representative <em>training corpus</em>, where the context and outcome for
each run of the experiment are known.  When presented with a new run
of the experiment, we simply choose the outcome that occured most
frequently for the experiment's context.</p>
<p>We can use a <tt class="docutils literal"><span class="pre">ConditionalFreqDist</span></tt> to find the most frequent
occurence for each context.  First, we record each outcome in the
training corpus, using the context that the experiment was run under
as the condition.  Then, we can access the frequency distribution for
a given context with the indexing operator, and use the <tt class="docutils literal"><span class="pre">max()</span></tt>
method to find the most likely outcome.</p>
<p>We will now use a <tt class="docutils literal"><span class="pre">ConditionalFreqDist</span></tt> to predict the most likely
next word in a text.  To begin, we load a corpus from a text file, and
create an empty <tt class="docutils literal"><span class="pre">ConditionalFreqDist</span></tt>:</p>
<pre class="literal-block">
&gt;&gt;&gt; from nltk.probability import ConditionalFreqDist
&gt;&gt;&gt; cfdist = ConditionalFreqDist()
</pre>
<p>We then examine each token in the corpus, and increment the
appropriate sample's count.  We use the variable <tt class="docutils literal"><span class="pre">prev</span></tt> to record
the previous word.</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; prev = None
&gt;&gt;&gt; for word in genesis.raw():
...     cfdist[prev].inc(word)
...     prev = word
</pre>
</blockquote>
<div class="note">
<p class="first admonition-title">Note</p>
<p class="last">Sometimes the context for an experiment is unavailable, or
does not exist.  For example, the first token in a text does not
follow any word.  In these cases, we must decide what context to
use.  For this example, we use <tt class="docutils literal"><span class="pre">None</span></tt> as the context for the
first token.  Another option would be to discard the first token.</p>
</div>
<p>Once we have constructed a conditional frequency distribution for the
training corpus, we can use it to find the most likely word for any
given context. For example, taking the word <tt class="docutils literal"><span class="pre">living</span></tt> as our context,
we can inspect all the words that occurred in that context.</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; word = 'living'
&gt;&gt;&gt; cfdist['living'].samples()
['creature,', 'substance', 'soul.', 'thing', 'thing,', 'creature']
</pre>
</blockquote>
<p>We can set up a simple loop to generate text: we set an initial
context, picking the most likely token in that context as our next
word, and then using that word as our new context:</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; word = 'living'
&gt;&gt;&gt; for i in range(20):
...     print word,
...     word = cfdist[word].max()
living creature that he said, I will not be a wife of the land of the land of the land
</pre>
</blockquote>
<p>This simple approach to text generation tends to get stuck in loops,
as demonstrated by the text generated above.  A more advanced approach
would be to randomly choose each word, with more frequent words chosen
more often.</p>
</div>
<div class="section" id="word-classes-and-parts-of-speech">
<h1><a name="word-classes-and-parts-of-speech">Word Classes and Parts of Speech</a></h1>
<p>In the preceding sections, we have pretty much treated all words
alike: things are either tokens or not. However, for many
applications, we want to distinguish between <em>different kinds</em> of
tokens. For example, we want to be able to make explicit that we have
recognized one string as an ordinary lexical item, another as
numerical expression, and yt another as a punctuation
character. Morever, we want to distinguish different kinds of lexical
items.  In fact, there is a long tradition within linguistics of
classifying words into categories called <em>parts of speech</em>. These are
sometimes also called word classes or <em>lexical categories</em>.  Familiar
examples are <em>noun</em>, <em>verb</em>, <em>preposition</em>, <em>adjective</em> and <em>adverb</em>.
In this section we present the standard criteria for categorising
words in this way, then discuss the main classes of words in English.</p>
<div class="section" id="categorising-words">
<h2><a name="categorising-words">Categorising Words</a></h2>
<p>How do we decide what category a word should belong to? In general,
linguists invoke three kinds of criteria for making the decision:
formal; syntactic (or distributional); notional (or semantic).  A
<em>formal</em> criterion is one which looks at the internal structure of a
word. For example, <tt class="docutils literal"><span class="pre">-ness</span></tt> is a suffix which combines with an
adjective to produce a noun. Examples are <tt class="docutils literal"><span class="pre">happy</span></tt> &gt; <tt class="docutils literal"><span class="pre">happiness</span></tt>,
<tt class="docutils literal"><span class="pre">ill</span></tt> &gt; <tt class="docutils literal"><span class="pre">illness</span></tt>. <a class="footnote-reference" href="#id5" id="id4" name="id4">[2]</a> So if we encounter a word which ends in
<tt class="docutils literal"><span class="pre">-ness</span></tt>, this is very likely to be a noun.  &lt;/para&gt;</p>
<table class="docutils footnote" frame="void" id="id5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id4" name="id5">[2]</a></td><td>We use &gt; to mean 'is derived from'.</td></tr>
</tbody>
</table>
<p>A <em>syntactic</em> criterion refers to the syntactic contexts in which a
word can occur. For example, assume that we have already determined
the category of nouns. Then we might say that a syntactic criterion
for an adjective in English is that it can occur immediately before a
noun, or immediately following the words <tt class="docutils literal"><span class="pre">be</span></tt> or
<tt class="docutils literal"><span class="pre">very</span></tt>. According to these tests,
<tt class="docutils literal"><span class="pre">near</span></tt> should be categorised as an adjective:</p>
<ol class="arabic simple">
<li>the near window</li>
<li>The end is (very) near.</li>
</ol>
<p>A familiar example of a <em>notional</em> criterion is that a noun is &quot;the
name of a person, place or thing&quot;. Within modern linguistics, notional
criteria for word classes have be viewed with considerable suspicion,
mainly because they are hard to formalise. Nevertheless, notional
criteria underpin many of our intuitions about word classes, and
enable us to make a good guess about the categorisation of words in
languages that we are unfamiliar with; that is, if we all we know
about the Dutch <tt class="docutils literal"><span class="pre">verjaardag</span></tt> is that it means the same as the
English word <tt class="docutils literal"><span class="pre">birthday</span></tt>, then we can guess that <tt class="docutils literal"><span class="pre">verjaardag</span></tt> is a
noun in Dutch. However, some care is needed: although we might
translate <tt class="docutils literal"><span class="pre">zij</span> <span class="pre">is</span> <span class="pre">van</span> <span class="pre">dag</span> <span class="pre">jarig</span></tt> as <tt class="docutils literal"><span class="pre">it's</span> <span class="pre">her</span> <span class="pre">birthday</span> <span class="pre">today</span></tt>, the
word <tt class="docutils literal"><span class="pre">jarig</span></tt> is in fact an adjective in Dutch, and has no exact
equivalent in English.</p>
<!-- http://www.askoxford.com/pressroom/archive/odelaunch/ -->
<p>All languages acquire new lexical items. A list of words recently
added to the Oxford Dictionary of English includes <tt class="docutils literal"><span class="pre">cyberslacker,</span>
<span class="pre">fatoush,</span> <span class="pre">blamestorm,</span> <span class="pre">SARS,</span> <span class="pre">cantopop,</span> <span class="pre">bupkis,</span> <span class="pre">noughties,</span> <span class="pre">muggle</span></tt>, and
<tt class="docutils literal"><span class="pre">robata</span></tt>. Notice that all these new words are nouns, and this is
reflected in calling nouns an <em>open class</em>. By contrast, prepositions
are regarded as a <em>closed class</em>. That is, there is a limited set of
words belonging to the class (e.g., <tt class="docutils literal"><span class="pre">above,</span> <span class="pre">along,</span> <span class="pre">at,</span> <span class="pre">below,</span> <span class="pre">beside,</span>
<span class="pre">between,</span> <span class="pre">during,</span> <span class="pre">for,</span> <span class="pre">from,</span> <span class="pre">in,</span> <span class="pre">near,</span> <span class="pre">on,</span> <span class="pre">outside,</span> <span class="pre">over,</span> <span class="pre">past,</span>
<span class="pre">through,</span> <span class="pre">towards,</span> <span class="pre">under,</span> <span class="pre">up,</span> <span class="pre">with</span></tt>), and membership of the set only
changes very gradually over time.</p>
<!-- Some word classes consist of a limited set of so-called
*function* words. Prepositions are one such class, comprising
items like etc.  These are called *closed classes*, in the sense
that although languages acquire new lexical items.  Content words
such as nouns are not limited in this way, and are continually
being extended with the invention of new words.  These are called
*open classes*. -->
</div>
<div class="section" id="english-word-classes">
<h2><a name="english-word-classes">English Word Classes</a></h2>
<p>This section presents a brief overview of English word classes.
Readers requiring more detail are encouraged to consult a grammar of
English.</p>
<p>Linguists commonly recognize four major categories of open class words
in English, namely nouns, verbs, adjectives and adverbs.  Nouns
generally refer to people, places, things, or concepts, e.g.: <em>woman,
Scotland, book, intelligence</em>.  In the context of a sentence, nouns
can appear after determiners and adjectives, and can be the subject or
object of the verb:</p>
<table border="1" class="docutils">
<colgroup>
<col width="11%" />
<col width="42%" />
<col width="47%" />
</colgroup>
<tbody valign="top">
<tr><td colspan="3">Syntactic Patterns involving some Nouns</td>
</tr>
<tr><td>Word</td>
<td>After a determiner</td>
<td>Subject of the verb</td>
</tr>
<tr><td>woman</td>
<td><em>the</em> woman who I saw yesterday ...</td>
<td>the woman <em>sat</em> down</td>
</tr>
<tr><td>Scotland</td>
<td><em>the</em> Scotland I remember as a child ...</td>
<td>Scotland <em>has</em> five million people</td>
</tr>
<tr><td>book</td>
<td><em>the</em> book I bought yesterday ...</td>
<td>this book <em>recounts</em> the colonisation of Australia</td>
</tr>
<tr><td>intelligence</td>
<td><em>the</em> intelligence displayed by the child ...</td>
<td>Mary's intelligence <em>impressed</em> her teachers</td>
</tr>
</tbody>
</table>
<p>English nouns can be morphologically complex.  For example, words like
<tt class="docutils literal"><span class="pre">books</span></tt> and <tt class="docutils literal"><span class="pre">women</span></tt> are plural.  As we saw earlier, words with the
<tt class="docutils literal"><span class="pre">-ness</span></tt> suffix are nouns that have been derived from adjectives,
e.g. <tt class="docutils literal"><span class="pre">happiness</span></tt> and <tt class="docutils literal"><span class="pre">illness</span></tt>.  The <tt class="docutils literal"><span class="pre">-ment</span></tt> suffix appears on
certain nouns derived from verbs, e.g. <tt class="docutils literal"><span class="pre">government</span></tt> and
<tt class="docutils literal"><span class="pre">establishment</span></tt>.</p>
<p>Nouns are usually further classified as <em>common nouns</em> and
<em>proper nouns</em>.  Proper nouns identify particular individuals or entities,
e.g. <tt class="docutils literal"><span class="pre">Moses</span></tt> and <tt class="docutils literal"><span class="pre">Scotland</span></tt>, while common nouns are all the rest.
Another important distinction exists between <em>count nouns</em> and
<em>mass nouns</em>. Count nouns are thought of as distinct entities which can be
counted, such as <tt class="docutils literal"><span class="pre">pig</span></tt> (e.g. <tt class="docutils literal"><span class="pre">one</span> <span class="pre">pig,</span> <span class="pre">two</span> <span class="pre">pigs,</span> <span class="pre">many</span> <span class="pre">pigs</span></tt>).
They cannot occur with the word <tt class="docutils literal"><span class="pre">much</span></tt> (i.e. *``much pigs``).  Mass
nouns, on the other hand, are not thought of as distinct entities
(e.g.  <tt class="docutils literal"><span class="pre">sand</span></tt>).  They cannot be pluralised, and do not occur with
numbers (e.g. *``two sands``, *``many sands``).  However, they can
occur with <tt class="docutils literal"><span class="pre">much</span></tt> (i.e. <tt class="docutils literal"><span class="pre">much</span> <span class="pre">sand</span></tt>).</p>
<p>Verbs are words which describe events and actions, e.g. <tt class="docutils literal"><span class="pre">fall</span></tt>,
<tt class="docutils literal"><span class="pre">eat</span></tt>.  In the context of a sentence, verbs express a relation
involving the referents of one or more noun phrases.</p>
<table border="1" class="docutils">
<colgroup>
<col width="10%" />
<col width="24%" />
<col width="66%" />
</colgroup>
<tbody valign="top">
<tr><td colspan="3">Syntactic Patterns involving some Verbs</td>
</tr>
<tr><td>Word</td>
<td>Simple</td>
<td>With modifiers and adjuncts (italicised)</td>
</tr>
<tr><td>fall</td>
<td>Rome fell</td>
<td>Dot com stocks <em>suddenly</em> fell <em>like a stone</em></td>
</tr>
<tr><td>eat</td>
<td>Mice eat cheese</td>
<td>John ate the pizza <em>with gusto</em></td>
</tr>
</tbody>
</table>
<p>Verbs can be classified according to the number of arguments (usually
noun phrases) that they co-occur with.  The word <tt class="docutils literal"><span class="pre">fall</span></tt> is
<em>intransitive</em>, requiring exactly one argument (the entity which
falls).  The word <tt class="docutils literal"><span class="pre">eat</span></tt> is <em>transitive</em>, requiring two arguments
(the eater and the eaten).  Other verbs are more complex; for instance
<tt class="docutils literal"><span class="pre">put</span></tt> requires three arguments, the agent doing the putting, the
entity being put somewhere, and a location.  The <tt class="docutils literal"><span class="pre">-ing</span></tt> suffix
appears on nouns derived from verbs, e.g. <tt class="docutils literal"><span class="pre">the</span> <span class="pre">falling</span> <span class="pre">of</span> <span class="pre">the</span>
<span class="pre">leaves</span></tt> (this is known as the <em>gerund</em>).</p>
<p>English verbs can be morphologically complex.  For instance, the
<em>present participle</em> of a verb ends in <tt class="docutils literal"><span class="pre">-ing</span></tt>, and expresses the
idea of ongoing, incomplete action (e.g. <tt class="docutils literal"><span class="pre">falling,</span> <span class="pre">eating</span></tt>).  The
<em>past participle</em> of a verb often ends in <tt class="docutils literal"><span class="pre">-ed</span></tt>, and expresses the
idea of a completed action (e.g. <tt class="docutils literal"><span class="pre">fell,</span> <span class="pre">ate</span></tt>).</p>
<p>Two other important word classes are <em>adjectives</em> and <em>adverbs</em>.
Adjectives describe nouns, and can be used as modifiers
(e.g. <tt class="docutils literal"><span class="pre">large</span></tt> in <tt class="docutils literal"><span class="pre">the</span> <span class="pre">large</span> <span class="pre">pizza</span></tt>), or in predicates (e.g. <tt class="docutils literal"><span class="pre">the</span>
<span class="pre">pizza</span> <span class="pre">is</span> <span class="pre">large</span></tt>).  English adjectives can be morphologically complex
(e.g.  <tt class="docutils literal"><span class="pre">fall&lt;subscript&gt;V&lt;/subscript&gt;+ing</span></tt> in <tt class="docutils literal"><span class="pre">the</span> <span class="pre">falling</span>
<span class="pre">stocks</span></tt>).  Adverbs modify verbs to specify the time, manner, place or
direction of the event described by the verb (e.g. <tt class="docutils literal"><span class="pre">quickly</span></tt> in
<tt class="docutils literal"><span class="pre">the</span> <span class="pre">stocks</span> <span class="pre">fell</span> <span class="pre">quickly</span></tt>).  Adverbs may also modify adjectives
(e.g. <tt class="docutils literal"><span class="pre">really</span></tt> in <tt class="docutils literal"><span class="pre">Mary's</span> <span class="pre">teacher</span> <span class="pre">was</span> <span class="pre">really</span> <span class="pre">nice</span></tt>).</p>
<p>English has several categories of closed class words in addition to
prepositions, and each dictionary and grammar classifies them
differently.  The following table gives a sample of
closed class words, following the classification of the Brown
Corpus. <a class="footnote-reference" href="#id7" id="id6" name="id6">[3]</a></p>
<table class="docutils footnote" frame="void" id="id7" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id6" name="id7">[3]</a></td><td>Note that part-of-speech tags may be presented as either
upper-case or lower-case strings -- there is no significance
attached to this difference.</td></tr>
</tbody>
</table>
<table border="1" class="docutils">
<colgroup>
<col width="2%" />
<col width="21%" />
<col width="77%" />
</colgroup>
<tbody valign="top">
<tr><td colspan="3">Some English Closed Class Words, with Brown Tag</td>
</tr>
<tr><td>ap</td>
<td>determiner/pronoun, post-determiner</td>
<td>many other next more last former little several enough most least only very few fewer past same</td>
</tr>
<tr><td>at</td>
<td>article</td>
<td>the an no a every th' ever' ye</td>
</tr>
<tr><td>cc</td>
<td>conjunction, coordinating</td>
<td>and or but plus &amp; either neither nor yet 'n' and/or minus an'</td>
</tr>
<tr><td>cs</td>
<td>conjunction, subordinating</td>
<td>that as after whether before while like because if since for than until so unless though providing once lest till whereas whereupon supposing albeit then</td>
</tr>
<tr><td>in</td>
<td>preposition</td>
<td>of in for by considering to on among at through with under into regarding than since despite ...</td>
</tr>
<tr><td>md</td>
<td>modal auxiliary</td>
<td>should may might will would must can could shall ought need wilt</td>
</tr>
<tr><td>pn</td>
<td>pronoun, nominal</td>
<td>none something everything one anyone nothing nobody everybody everyone anybody anything someone no-one nothin'</td>
</tr>
<tr><td>ppl</td>
<td>pronoun, singular, reflexive</td>
<td>itself himself myself yourself herself oneself ownself</td>
</tr>
<tr><td>pp$</td>
<td>determiner, possessive</td>
<td>our its his their my your her out thy mine thine</td>
</tr>
<tr><td>pp$$</td>
<td>pronoun, possessive</td>
<td>ours mine his hers theirs yours</td>
</tr>
<tr><td>pps</td>
<td>pronoun, personal, nom, 3rd pers sng</td>
<td>it he she thee</td>
</tr>
<tr><td>ppss</td>
<td>pronoun, personal, nom, not 3rd pers sng</td>
<td>they we I you ye thou you'uns</td>
</tr>
<tr><td>wdt</td>
<td>WH-determiner</td>
<td>which what whatever whichever</td>
</tr>
<tr><td>wps</td>
<td>WH-pronoun, nominative</td>
<td>that who whoever whosoever what whatsoever</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="part-of-speech-tag-sets">
<h2><a name="part-of-speech-tag-sets">Part-of-Speech Tag Sets</a></h2>
<p>Most part-of-speech tag sets make use of the same basic categories,
such as noun, verb, adjective, and preposition. However, tag sets
differ both in how finely they divide words into categories; and in
how they define their categories. For example, <tt class="docutils literal"><span class="pre">is</span></tt> might be tagged
as a verb in one tag set; but as a distinct form of <tt class="docutils literal"><span class="pre">to</span> <span class="pre">be</span></tt> in
another tag set -- in fact, we just observed the latter situation
in the Brown Corpus tag set.  This variation in tag sets is
unavoidable, since part-of-speech tags are used in different ways for
different tasks. In other words, there is no one 'right way' to assign
tags, only more or less useful ways, depending on one's goals.</p>
<!-- <note><para> There are several part-of-speech tag sets in widespread
use, because there are different schemes for classifying words
(owing to the different weight given to formal, syntactic and
notional criteria), and because different processing tasks call for
finer or coarser classification.</para></note> -->
<p>Observe that the tagging process simultaneously collapses distinctions
(i.e., lexical identity is usually lost when all personal pronouns are
tagged <tt class="docutils literal"><span class="pre">prp</span></tt>), while introducing distinctions and removing
ambiguities (e.g. <tt class="docutils literal"><span class="pre">deal</span></tt> tagged as <tt class="docutils literal"><span class="pre">vb</span></tt> or <tt class="docutils literal"><span class="pre">nn</span></tt>).  This move
facilitates classification and prediction.  Observe that when we
introduce finer distinctions in a tag set, we get better information
about linguistic context, but we have to do more work to classify the
current token (there are more tags to choose from).  Conversely, with
fewer distinctions, we have less work to do for classifying the
current token, but less information about the context to draw on.</p>
<p>In this tutorial, we will use the following tags: <tt class="docutils literal"><span class="pre">at</span></tt> (article)
<tt class="docutils literal"><span class="pre">nn</span></tt> (Noun), <tt class="docutils literal"><span class="pre">vb</span></tt> (Verb), <tt class="docutils literal"><span class="pre">jj</span></tt> (Adjective), <tt class="docutils literal"><span class="pre">in</span></tt>
(Preposition), <tt class="docutils literal"><span class="pre">cd</span></tt> (Number), and <tt class="docutils literal"><span class="pre">end</span></tt> (Sentence-ending
punctuation).  As we mentioned, this is a radically simplified version
of the Brown Corpus tag set, which in its entirety has 87 basic tags
plus many combinations.  A fuller list is given in the Appendix of the
tagging tutorial.</p>
</div>
</div>
<div class="section" id="representing-tagged-tokens-and-tagged-corpora">
<h1><a name="representing-tagged-tokens-and-tagged-corpora">Representing Tagged Tokens and Tagged Corpora</a></h1>
<p>The preceding sections have discussed the nature and use of tags in
language processing.  In this section the computational representation
of tags is presented.  First we consider individual tagged tokens, and
show how they are created and accessed.  Then we address tagged
corpora.</p>
<p>By convention, a tagged token is represented using a Python tuple:</p>
<pre class="literal-block">
&gt;&gt;&gt; tok = ('fly', 'nn')
&gt;&gt;&gt; tok
('fly', 'nn')
</pre>
<p>We can access the properties of this token in the usual way, as
shown below:</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; print tok[0]
fly
&gt;&gt;&gt; print tok[1]
nn
</pre>
</blockquote>
<p>Several large corpora (such as the Brown Corpus and portions of the
Wall Street Journal) have been manually tagged with part-of-speech
tags.  Before we can use these corpora, we must read them from files
and tokenize them.</p>
<p>Tagged texts are usually stored in files as sequences of
whitespace-separated tokens, where each token is of the form
<tt class="docutils literal"><span class="pre">text/tag</span></tt>, as illustrated below for a sample from the Brown Corpus:</p>
<pre class="literal-block">
The/at grand/jj jury/nn commented/vbd on/in a/at number/nn of/in
other/ap topics/nns ,/, among/in them/ppo the/at Atlanta/np and/cc
Fulton/np-tl County/nn-tl purchasing/vbg departments/nns which/wdt it/pps
said/vbd ``/`` are/ber well/ql operated/vbn and/cc follow/vb generally/rb
accepted/vbn practices/nns which/wdt inure/vb to/in the/at best/jjt
interest/nn of/in both/abx governments/nns ''/'' ./.
</pre>
<p>It is possible to use the <tt class="docutils literal"><span class="pre">nltk_lite.corpora</span></tt> module to read and
tokenize data from a tagged corpus, as we saw above.</p>
<p><em>MENTION SENTENCE STRUCTURE</em></p>
<p>Here is another example which constructs tokens from a string:</p>
<pre class="literal-block">
&gt;&gt;&gt; sent = &quot;&quot;&quot;
... John/nn saw/vb the/at book/nn on/in the/at table/nn ./end  He/nn sighed/vb ./end
... &quot;&quot;&quot;
&gt;&gt;&gt; from nltk_lite.tag import tag2tuple
&gt;&gt;&gt; for t in tokenize.whitespace(sent):
...     print tag2tuple(t),
('John', 'nn') ('saw', 'vb') ('the', 'at') ('book', 'nn') ('on', 'in') ('the', 'at') ('table', 'nn') ('.', 'end') ('He', 'nn') ('sighed', 'vb') ('.', 'end')
</pre>
</div>
<div class="section" id="more-applications">
<h1><a name="more-applications">More Applications</a></h1>
<p>Now that we can access tagged text, it is possible to do a variety of
useful processing tasks.  Here we consider just two: guessing the
part-of-speech tag of a word, and exploring the frequency distribution
of modal verbs according to text genre.</p>
<div class="section" id="classifying-words-automatically">
<h2><a name="classifying-words-automatically">Classifying Words Automatically</a></h2>
<p>A tagged corpus can be used to <em>train</em> a simple classifier, which can
then be used to guess the tag for untagged words.  For each word, we
can count the number of times it is tagged with each tag.  For
instance, the word <tt class="docutils literal"><span class="pre">deal</span></tt> is tagged 89 times as <tt class="docutils literal"><span class="pre">nn</span></tt> and 41 times
as <tt class="docutils literal"><span class="pre">vb</span></tt>.  On this evidence, if we were asked to guess the tag for
<tt class="docutils literal"><span class="pre">deal</span></tt> we would choose <tt class="docutils literal"><span class="pre">nn</span></tt>, and we would be right over two-thirds
of the time.  The following program performs this tagging task, when
trained on the &quot;g&quot; section of the Brown Corpus (so-called <em>belles
lettres</em>, creative writing valued for its esthetic content).</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; cfdist = ConditionalFreqDist()
&gt;&gt;&gt; for sentence in brown.tagged('g'):
...     for token in sentence:
...         word = token[0]
...         tag = token[1]
...         cfdist[word].inc(tag)
&gt;&gt;&gt; for word in &quot;John saw 3 polar bears&quot;.split():
...     print word, cfdist[word].max()
John np
saw vbd
3 cd-tl
polar jj
bears vbz
</pre>
</blockquote>
<p>Note that <tt class="docutils literal"><span class="pre">bears</span></tt> was incorrectly tagged as the 3rd person singular
form of a verb, since this word appears more frequently as a verb than
a noun in esthetic writing.</p>
<p>A problem with this approach is that it creates a huge model, with an
entry for every possible combination of word and tag.  For certain
tasks it is possible to construct reasonably good models which are
tiny in comparison.  For instance, let's try to guess whether a verb
is a noun or adjective from the last letter of the word alone.  We can
do this as follows:</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; tokens = []
&gt;&gt;&gt; for sent in brown.tagged('g'):
...     for (word,tag) in sent:
...         if tag in ['nn', 'jj'] and len(word) &gt; 3:
...             char = word[-1]
...             tokens.append((char,tag))
&gt;&gt;&gt; split = len(tokens)*9/10
&gt;&gt;&gt; train, test = tokens[:split], tokens[split:]
&gt;&gt;&gt; cfdist = ConditionalFreqDist()
&gt;&gt;&gt; for (char,tag) in train:
...     cfdist[char].inc(tag)
&gt;&gt;&gt; correct = total = 0
&gt;&gt;&gt; for (char,tag) in test:
...     if tag == cfdist[char].max():
...         correct += 1
...     total += 1
&gt;&gt;&gt; print correct*100/total
71
</pre>
</blockquote>
<p>This result of 71% is marginally better than the result of 65% that we
get if we assign the <tt class="docutils literal"><span class="pre">nn</span></tt> tag to every word.  We can inspect the
model to see which tag is assigned to a word given its final letter.
Here we learn that words which end in <tt class="docutils literal"><span class="pre">c</span></tt> or <tt class="docutils literal"><span class="pre">l</span></tt> are more likely
to be adjectives than nouns:</p>
<pre class="literal-block">
&gt;&gt;&gt; print [(c, cfdist[c].max()) for c in cfdist.conditions()]
[('%', 'nn'), (&quot;'&quot;, None), ('-', 'jj'), ('2', 'nn'), ('5', 'nn'), ('A', 'nn'), ('D', 'nn'), ('O', 'nn'), ('S', 'nn'), ('a', 'nn'), ('c', 'jj'), ('b', 'nn'), ('e', 'nn'), ('d', 'nn'), ('g', 'nn'), ('f', 'nn'), ('i', 'nn'), ('h', 'nn'), ('k', 'nn'), ('m', 'nn'), ('l', 'jj'), ('o', 'nn'), ('n', 'nn'), ('p', 'nn'), ('s', 'nn'), ('r', 'nn'), ('u', 'nn'), ('t', 'nn'), ('w', 'nn'), ('y', 'nn'), ('x', 'nn'), ('z', 'nn')]
</pre>
</div>
<div class="section" id="exploring-text-genres">
<h2><a name="exploring-text-genres">Exploring text genres</a></h2>
<p>Now that we can load a significant quantity of tagged text, we can
process it and extract items of interest.  The following code iterates
over the fifteen genres of the Brown Corpus (accessed using
<tt class="docutils literal"><span class="pre">brown.groups()</span></tt>).  The material for each genre lives in a set of
files (accessed using <tt class="docutils literal"><span class="pre">brown.items</span></tt>).  Each of these is tokenized
in turn.  The next step is to check if the token has the <tt class="docutils literal"><span class="pre">md</span></tt> tag.
For each of these words we increment a count.
This uses the conditional frequency distribution, where the condition
is the current genre, and the event is the modal.</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; cfdist = ConditionalFreqDist()
&gt;&gt;&gt; for genre in brown.items:                  # each genre
...     for sent in brown.tagged(genre):       # each sentence
...         for (word,tag) in sent:            # each tagged token
...             if tag == 'md':                # found a modal
...                  cfdist[genre].inc(word.lower())
</pre>
</blockquote>
<p>The conditional frequency distribution is nothing more than a mapping
from each genre to the distribution of modals in that genre.  The
following code fragment identifies a small set of modals of interest,
and processes the data structure to output the required counts.</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; modals = ['can', 'could', 'may', 'might', 'must', 'will']
&gt;&gt;&gt; print &quot;%-40s&quot; % 'Genre', ' '.join([(&quot;%6s&quot; % m) for m in modals])
Genre                                       can  could    may  might   must   will
&gt;&gt;&gt; for genre in cfdist.conditions():    # generate rows
...     print &quot;%-40s&quot; % brown.item_name[genre],
...     for modal in modals:
...         print &quot;%6d&quot; % cfdist[genre].count(modal),
...     print
press: reportage                             94     86     66     36     50    387
press: reviews                               44     40     45     26     18     56
press: editorial                            122     56     74     37     53    225
skill and hobbies                           273     59    130     22     83    259
religion                                     84     59     79     12     54     64
belles-lettres                              249    216    213    113    169    222
popular lore                                168    142    165     45     95    163
miscellaneous: government &amp; house organs    115     37    152     13     99    237
fiction: general                             39    168      8     42     55     50
learned                                     366    159    325    126    202    330
fiction: science                             16     49      4     12      8     16
fiction: mystery                             44    145     13     57     31     17
fiction: adventure                           48    154      6     58     27     48
fiction: romance                             79    195     11     51     46     43
humor                                        17     33      8      8      9     13
</pre>
</blockquote>
<p>There are some interesting patterns in this table.  For instance,
compare the rows for government literature and adventure literature;
the former is dominated by the use of <tt class="docutils literal"><span class="pre">can,</span> <span class="pre">may,</span> <span class="pre">must,</span> <span class="pre">will</span></tt> while
the latter is characterised by the use of <tt class="docutils literal"><span class="pre">could</span></tt> and <tt class="docutils literal"><span class="pre">might</span></tt>.
With some further work it might be possible to guess the genre of a
new text automatically, according to its distribution of modals.</p>
<p>Now that we have seen how tagged tokens and tagged corpora are
created and accessed, we are ready to take a look at the automatic
categorization of words.</p>
</div>
</div>
<div class="section" id="further-reading">
<h1><a name="further-reading">Further Reading</a></h1>
<p>John Hopkins Center for Language and Speech Processing, 1999
Summer Workshop on Normalization of Non-Standard Words: Final Report
<a class="reference" href="http://www.clsp.jhu.edu/ws99/projects/normal/report.pdf">http://www.clsp.jhu.edu/ws99/projects/normal/report.pdf</a></p>
<p>SIL Glossary of Linguistic Terms:
<a class="reference" href="http://www.sil.org/linguistics/GlossaryOfLinguisticTerms/">http://www.sil.org/linguistics/GlossaryOfLinguisticTerms/</a></p>
<p>Language Files: Materials for an Introduction to Language and
Linguistics (Eighth Edition), The Ohio State University Department of
Linguistics, <a class="reference" href="http://www.ling.ohio-state.edu/publications/files/">http://www.ling.ohio-state.edu/publications/files/</a></p>
</div>
<div class="section" id="exercises">
<h1><a name="exercises">Exercises</a></h1>
<p>#. Accessing and tokenizing a text file: Obtain some plain text data
(e.g. visit a web-page and save it as plain text), and store it in a
file 'corpus.txt'.</p>
<blockquote>
<p>#. Using the <tt class="docutils literal"><span class="pre">open()</span></tt> and <tt class="docutils literal"><span class="pre">read()</span></tt> functions, load the text into
a string variable and print it.</p>
<p>#. Now, initialize a new token with <tt class="docutils literal"><span class="pre">Token()</span></tt>, using this text.
Tokenize the text with <tt class="docutils literal"><span class="pre">WhitespaceTokenizer</span></tt>, and specify that the
result should be stored in the <tt class="docutils literal"><span class="pre">WORDS</span></tt> property.  Print the
result.</p>
<p>#. Next, compute the number of tokens, using the <tt class="docutils literal"><span class="pre">len()</span></tt> function,
and print the result.</p>
<p>#. Finally, discuss shortcomings of this method for tokenizing text.
In particular, identify any material which has not been correctly
tokenized.  (You may need to look for a more complex text.)</p>
</blockquote>
<ol class="arabic simple">
<li>Tokenizing text using regular expressions:
Obtain some plain text data (e.g. visit a web-page and
save it as plain text), and store it in a file 'corpus.txt'.
so that you can answer the following questions.</li>
</ol>
<blockquote>
<p>#. Word processors typically hyphenate words when they are split
across a linebreak.  When word-processed documents are converted to
plain text, the pieces are usually not recombined.  It is easy to
discover such texts by searching on the web for broken words,
e.g. <tt class="docutils literal"><span class="pre">depart-</span> <span class="pre">ment</span></tt>.  Create a <tt class="docutils literal"><span class="pre">RegexpTokenizer</span></tt> which treats
such broken words as a single token.</p>
<p>#. Consider the following book title: <em>This Is the Beat Generation:
New York-San Francisco-Paris</em>.  What would it take to be able to
tokenize such strings so that each city name was stored as a single
token?</p>
</blockquote>
<p>#. Working with tagged text: Write a program which loads the Brown
corpus, then, given a word, lists the possible tags for the word each
with a frequency count.  For example, for the word <tt class="docutils literal"><span class="pre">strike</span></tt> the
program would generate: <tt class="docutils literal"><span class="pre">[('nn',</span> <span class="pre">25),</span> <span class="pre">('vb',</span> <span class="pre">21)]</span></tt>.  (Hint: this
task involves sorting and reversing a list of tuples which has the
form <tt class="docutils literal"><span class="pre">[(21,</span> <span class="pre">'vb'),</span> <span class="pre">(25,</span> <span class="pre">'nn')]</span></tt>.  To convert such lists into the
required form, use <tt class="docutils literal"><span class="pre">word_freq</span> <span class="pre">=</span> <span class="pre">[(y,x)</span> <span class="pre">for</span> <span class="pre">(x,y)</span> <span class="pre">in</span> <span class="pre">freq_word]</span></tt>.)</p>
<blockquote>
<p>#. Use your program to print the tags and their frequencies for the
following words: <tt class="docutils literal"><span class="pre">can,</span> <span class="pre">fox,</span> <span class="pre">get,</span> <span class="pre">lift,</span> <span class="pre">like,</span> <span class="pre">but,</span> <span class="pre">frank,</span> <span class="pre">line,</span>
<span class="pre">interest</span></tt>.  Check that you know the meaning of the high-frequency
tags.</p>
<p>#. Write a program to find the 20 words which have the greatest
variety of different possible tags.</p>
<p>#. Pick words which can be either a noun or a verb (e.g. <tt class="docutils literal"><span class="pre">deal</span></tt>).
Guess which is the most likely tag for each word, then check whether
you were right.</p>
</blockquote>
<p>#. Predicting the next word: The word prediction program we saw in
this chapter quickly gets stuck in a cycle.  Modify the program to
choose the next word randomly, from a list of the <em>n</em> most likely
words in the given context.  (Hint: store the <em>n</em> most likely words in
a list <tt class="docutils literal"><span class="pre">lwords</span></tt> then randomly choose a word from the list using
<tt class="docutils literal"><span class="pre">random.choice()</span></tt>.)</p>
<blockquote>
<p>#. Select a particular genre, such as a section of the Brown Corpus,
or a genesis translation, or one of the newsgroups corpora.  Train
your system on this corpus and get it to generate random text.  You
may have to experiment with different start words. How intelligible
is the text?  Examine the strengths and weaknesses of this method of
generating random text.</p>
<p>#. Try the same approach with different genres, and with different
amounts of training data.  What do you observe?</p>
<p>#. Now train your system using two distinct genres and experiment
with generating text in the hybrid genre.  As before, discuss your
observations.</p>
</blockquote>
<p>#. Classifying words automatically: The program for classifying words
as nouns or adjectives scored 71%.  We will try to come up with better
conditions, to get the system to score 80% or better.</p>
<blockquote>
<p>#. Revise the condition to use a longer suffix of the word, such as
the last two characters, or the last three characters.  What happens
to the performance?  Which suffixes are diagnostic for adjectives?</p>
<p>#. Explore other conditions, such as variable length prefixes of a
word, or the length of a word, or the number of vowels in a word.</p>
<p>#. Finally, combine multiple conditions into a tuple, and explore
which combination of conditions gives the best result.</p>
</blockquote>
<hr class="docutils" />
<p><a class="reference" href="http://nltk.sourceforge.net/">NLTK</a></p>
</div>
</div>
<div class="footer">
<hr class="footer" />
Generated on: 2005-06-21.
Generated by <a class="reference" href="http://docutils.sourceforge.net/">Docutils</a> from <a class="reference" href="http://docutils.sourceforge.net/rst.html">reStructuredText</a> source.

</div>
</body>
</html>
