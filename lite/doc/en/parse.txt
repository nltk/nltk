.. -*- mode: rst -*-
.. include:: definitions.txt

=======================================
Elementary Language Processing: Parsing
=======================================

:Authors: Steven Bird, Ewan Klein, Edward Loper
:Contact: sb@csse.unimelb.edu.au
:Version: 0.1
:Revision: $Revision$
:Date: $Date$
:Copyright: |copy| 2001-2005 University of Pennsylvania
:License: Creative Commons Attribution-NonCommercial-ShareAlike License

.. Note:: This is a draft.  Please send any feedback to the authors.

------------
Introduction 
------------

Native speakers of any language have strong intuitions about the
well-formedness of putative sentences in that language.  These
intuitions are surprisingly detailed.  For example, consider the
following six sentences involving three synonymous verbs *loaded*,
*dumped*, and *filled*.

# The farmer *loaded* sand into the cart
# The farmer *loaded* the cart with sand
# The farmer *dumped* sand into the cart
# *The farmer *dumped* the cart with sand
# *The farmer *filled* sand into the cart
# The farmer *filled* the cart with sand

Two of the sentences (starred) are ill-formed.  As we shall see,
patterns of well-formedness and ill-formedness in a sequence of words
can be understood with respect to the internal **phrase structure** of
the sentences.  We can develop formal models of these structures using
grammars and parsers.

In the context of computational modelling, a language is often viewed
as a set of well-formed sentences.  Sequences of words which are not
grammatical are excluded from this set.  Now, since there is no
upper-bound on the length of a sentence, the number of possible
sentences is unbounded.  For example, it is possible to add an
unlimited amount of material to a sentence by using ``and`` or by
chaining relative clauses, as illustrated in the following example
from a children's story:


   You can imagine Piglet's joy when at last the ship came in sight of
   him. In after-years he liked to think that he had been in Very
   Great Danger during the Terrible Flood, but the only danger he had
   really been in was the last half-hour of his imprisonment, when
   Owl, who had just flown up, sat on a branch of his tree to comfort
   him, and told him a very long story about an aunt who had once laid
   a seagull's egg by mistake, and the story went on and on, rather
   like this sentence, until Piglet who was listening out of his
   window without much hope, went to sleep quietly and naturally,
   slipping slowly out of the window towards the water until he was
   only hanging on by his toes, at which moment, luckily, a sudden
   loud squawk from Owl, which was really part of the story, being
   what his aunt said, woke the Piglet up and just gave him time to
   jerk himself back into safety and say, "How interesting, and did
   she?"  when -- well, you can imagine his joy when at last he saw
   the good ship, Brain of Pooh (Captain, C. Robin; 1st Mate, P. Bear)
   coming over the sea to rescue him...  (from A.A. Milne *In which
   Piglet is Entirely Surrounded by Water*)


Given that the resources of a computer, however large, are still
finite, it is necessary to devise a finite description of this
infinite set.  Such descriptions are called **grammars**.  We have
already encountered this possibility in the context of regular
expressions.  For example, the expression ``a+`` describes the
infinite set ``{a, aa, aaa, aaaa, ...}``.  Apart from their
compactness, grammars usually capture important properties of the
language being studied, and can be used to systematically map between
sequences of words and abstract representations of their meaning.
Thus, even if we were to impose an upper bound on sentence length to
ensure the language was finite, we would still want to come up with a
compact representation in the form of a grammar.

A well-formed sentence of a language is more than an arbitrary
sequence of words from the language.  Certain kinds of words usually
go together.  For instance, determiners like ``the`` are typically
followed by adjectives or nouns, but not by verbs.  Groups of words
form intermediate structures called phrases or **constituents**.
These constituents can be identified using standard syntactic tests,
such as substitution, movement and coordination.  For example, if a
sequence of words can be replaced with a pronoun, then that sequence
is likely to be a constituent.  According to this test, we can infer
that the italicised string in the following example is a constituent:


1. *Ordinary daily multivitamin and mineral supplements* could 
   help adults with diabetes fight off some minor infections.
#. *They* could help adults with diabetes fight off some minor infections.

.. Note:: Readers are referred to any introductory text on syntax for
   fuller treatment of constituency, e.g. McCawley (1998) The Syntactic
   Phenomena of English.

The structure of a sentence may be represented using a phrase
structure tree, in which the terminal symbols are the words of the
sentence, the pre-terminal symbols are parts of speech, and the
remaining non-terminals are syntactic constituents.  An example of
such a tree is shown in `parse_tree`_. For those trained in
theoretical linguistics, the lexical categories (i.e., the nonterminal
symbols immediately dominating the leafs of the tree) may appear
strange.  This is because we have used used labels drawn from the
Brown Corpus part-of-speech tag set; see XREF(Tagging Chapter: Intro)
for discussion.

_`parse_tree`

.. figure:: ../images.parsing/parse_tree.png

   Phrase Structure Tree

A *grammar* is a formal system which specifies which sequences of
words are well-formed in the language, and which provides one or more
phrase structures for the sequence.  We will focus our attention on a
particular kind of grammar called a *context-free grammar* (CFG),
which is a collection of productions of the form ``S`` |rarr| ``NP VP``.
We interpret this productions as saying that a constituent of category
``S`` can consist of subconstituents of categories ``NP`` and
``VP``. Similarly, the production ``VB`` |rarr| ``'help'`` means that the
constituent of category ``VB`` can consist of the string ``help``.
For a phrase structure tree to be well-formed relative to a grammar,
each non-terminal node and its children must correspond to such a
production in the grammar.

A *parser* is a computational system which processes input sentences
according to the productions of the grammar, and builds one or more
constituent structures which conform to the grammar.  We take a
grammar to be a declarative specification of well-formedness, and a
parser to be a procedural interpretation of the grammar.  In this
chapter we will present context-free grammars, and describe some
simple parsers that work with them.


..  EXPAND THE ABOVE DISCUSSION 

Parsing is important in linguistics and natural language processing
for a variety of reasons.  A parser permits a grammar to be evaluated
against a potentially large collection of test sentences, helping the
linguist to identify shortcomings in their analysis.  A parser can
also be used as a model of psycholinguistic processing, with the goal
of explaining the processing difficulties that humans have with
certain syntactic constructions (e.g., the so-called "garden path"
sentences).  There are many NL applications which involve parsing at
some point; for example, we would expect the natural language input to
a question-answering system to undergo parsing as an initial step.


--------------------
Linguistic Overview 
--------------------

.. X.2 linguistic overview (for non-linguist readers)
   - how have linguists addressed the problem?
   - what are the shortcomings of the non-computational approach?
   - cover structural ambiguity and PP attachment
   - cover subcategorization, Levin classes

.. Parsing has a long and interesting history in computational
   linguistics.  [NOTES: the perspective of early generative grammar;
   early NLP approaches including ATNs and DCGs; grammar formalisms and
   development environments.]

.. Context-free grammars: CFG productions and their interpretation.

.. Discussion about how CFGs are created by hand.

Let's start off by looking at a simple context-free grammar:

| S |rarr| NP VP
| NP |rarr| Det N
| NP |rarr| Det N PP
| VP |rarr| V NP PP
| VP |rarr| V NP
| VP |rarr| V
| PP |rarr| P NP

| NP |rarr| 'I'
| Det |rarr| 'the'
| Det |rarr| 'a'
| N |rarr| 'man' | 'park' | 'dog' | 'telescope'
| V |rarr| 'saw'
| P |rarr| 'in' | 'with'

The productions are presented in two groups which we will call
**grammatical productions** and **lexical productions** respectively.
The labels in the grammatical productions are all **nonterminal
symbols**: these are the grammatical categories we use to label
constituents.  By contrast, **terminal symbols** are what we call the
words whose syntactic behaviour is defined by the grammar. Notice that
in the lexical productions, we have used disjunction '``|``' on the
righthand side of the productions as an abbreviatory device. For
example, ``P`` |rarr| ``'in' | 'with'`` means that a ``P``
(preposition) can be realized as either ``in`` or ``with``.

.. Note:: It is also possible to have disjunctions on the righthand
   sides of grammatical productions.

`parse_tree2`_ illustrates two possible phrase structure analyses of
the string ``The dog saw the man in the park``:

.. figure:: ../images.parsing/parse_tree2.png

   Structural Ambiguity
   _`parse_tree2`

.. figure:: ../images.parsing/parse_tree3.png

The fact that our grammar admits two distinct structures for this
sentence indicates that the sentence is **structurally ambiguous**.
The ambiguity in question is often called a **PP attachment
ambiguity**, since the issue at hand is how the ``PP`` (prepositional
phrase) ``in the park`` should be attached to the phrase structure
tree. In this case, we can see that the grammar admits just two
choices. The ``PP`` is either attached as a daughter of ``VP`` or else
as a daughter of ``NP``. There is also a difference in semantic
interpretation associated with the two structures, though the meaning
distinction is somewhat subtle in this case. Where the ``PP`` is
attached to ``VP``, the intended interpretation is that the event of
seeing took place in the park, while if the ``PP`` is attached to
``NP``, being in the park is a property of the ``NP`` referent; that
is, the man was in the park, but the agent of the seeing &mdash; the
dog &mdash; was somewhere else (e.g., sitting on the balcony of an
apartment overlooking the park).  As we will see, dealing with
ambiguity is a key challenge in parsing.

We remarked earlier that there is no upper bound on the length of
sentences of a natural language. Within context-free grammars, the
main mechanism which allows this unboundedness is **recursion**. A
grammar is said to be recursive if a category occurring on the
lefthand side of a production also appears on the righthand side of a
production. If this dual occurrence takes place in one and the same
production, then we have **direct recursion**; otherwise we have
**indirect recursion**. These two cases are both illustrated in the
following grammar:

| S |rarr| NP VP
| NP |rarr| Det N
| NP |rarr| Det N PP
| VP |rarr| VP Conj VP
| VP |rarr| V NP PP
| VP |rarr| V NP
| VP |rarr| V S
| VP |rarr| V
| PP |rarr| P NP

| NP |rarr| 'I'
| Det |rarr| 'the'
| Det |rarr| 'a'
| N |rarr| 'man' | 'park' | 'dog' | 'telescope'
| V |rarr| 'saw' | 'barked' | 'said'
| P |rarr| 'in' | 'with'
| Conj |rarr| 'and' | 'or'

That is, the production ``VP`` |rarr| ``VP Conj VP`` involves direct
recursion on the category``VP``, whereas undirect recursion on ``S``
arises from the combination of two productions, namely ``S`` |rarr|
``NP VP`` and ``VP`` |rarr| ``V S``.  _`recursive_parse1` shows a
phrase structure tree involving coordinated ``VP`` s:

.. figure:: ../images.parsing/recursive_parse1.png

   Coordinated VP Phrase Structure Tree `recursive_parse1`_

In this chapter, we only consider very small ('toy') context-free
grammars, in order to illustrate the key aspects of parsing such
grammars. But there is an obvious question as to whether the general
approach can be scaled up to cover large corpora of natural
languages. How hard would it be to construct such a set of rules by
hand? In general, the answer is: Very hard. Even if we allow ourselves
to use various formal devices that give much more succinct
representations of phrase structure rules (some of which will be
discussed in the next chapter), it is still extremely difficult to
keep control of the complex interactions between the many rules
required to cover the major constructions of a language. That is, it
is hard to modularize grammars, so that one portion can be worked on
independently of the other parts. This in turn means that it is
difficult to distribute the task of grammar writing across a team of
linguists. Another difficulty is that as the grammar expands to cover
a wider and wider range of constructions, there is a corresponding
increase in the number of analyses which are admitted for any one
sentence. In other words, ambiguity increases proportionally with
coverage.

.. 
 that w a broad coverage grammar
 . But we may want to ask how
 Shortcomings of this approach: unreliable, doesn't scale,
 complex interactions amongst productions makes manual debugging
 almost impossible.  Consequently linguists are unable to
 work with large-coverage grammars.-->


Despite the problems just alluded to, there are a number of
large-scale collaborative projects ongoing which appear to have
achieved interesting and often impressive results in developing
rule-based grammars for several languages. Examples are the Lexical
Functional Grammar (LFG) Pargram project
(http://www2.parc.com/istl/groups/nltt/pargram/), the Head-Driven
Phrase Structure Grammar (HPSG) LinGO Matrix framework
(http://www.delph-in.net/matrix/), and the Lexicalized Tree Adjoining
Grammar XTAG Project (http://www.cis.upenn.edu/~xtag/).


-----------------------------------
Computational Approaches to Parsing 
-----------------------------------

..  
 To do: general introduction to TD and BU -->

 X.3 computational model (gentle for linguistics ugrads)
 - what are some good data structures and algorithms?
 - just pick one or two approaches, not encyclopedic
 - NLTK demo - watch the execution of the algorithm
 (screen shots to show execution, side bars to say how
 to do it)


-------------------------
Recursive Descent Parsing 
-------------------------

The simplest kind of parser interprets the grammar as a specification
of how to break a high-level goal into several lower-level subgoals.
The top-level goal is to find an ``S``.  The ``S |rarr| NP VP``
production permits the parser to replace this goal with two subgoals:
find an ``NP``, then find a ``VP``.  Each of these subgoals can be
replaced in turn by sub-sub-goals, using productions that have ``NP``
and ``VP`` on their left-hand side.  Eventually, this expansion
process leads to subgoals such as: find the word ``telescope``.  Such
subgoals can be directly compared against the input string, and
succeed if the next word is matched.  If there is no match the parser
must back up and try a different alternative.

The recursive descent parser builds a parse tree during the above
process.  With the initial goal (find an ``S``), the ``S`` root node
is created.  As the above process recursively expands its goals using
the productions of the grammar, the parse tree is extended downwards
(hence the name *recursive descent*).  We can see this in action using
the parser demonstration ``nltk_lite.draw.rdparser``.  To run this
demonstration, use the following commands::

 from nltk_lite.draw.rdparser import demo
 demo()


Six stages of the execution of this parser are shown below:

.. figure::

.. image:: ../images.parsing/rdparser1.png 
.. image:: ../images.parsing/rdparser2.png
.. image:: ../images.parsing/rdparser3.png
.. image:: ../images.parsing/rdparser4.png
.. image:: ../images.parsing/rdparser5.png
.. image:: ../images.parsing/rdparser6.png

   Six Stages of a Recursive Descent Parser: initial, after two
   productions, after matching "the", failing to match "man",
   completed parse, backtracking

.. Discussion: choosing which of several possible productions to apply;
   backtracking; termination.

Problems with recursive descent parsing: considers structures and
words that are not attested; backtracking may discard parsed
constituents that need to be rebuilt; for example, backtracking over
``VP`` |rarr| ``V NP`` will discard the structures created for the ``V``
and ``NP`` non-terminals.  If the parser then proceeds with ``VP``
|rarr| ``V NP PP``, then the structures for the ``V`` and ``NP`` must be
created again.

Recursive descent parsing is a kind of *top-down parsing*.  These use
the grammar to *predict* what the input will be, before inspecting any
input.  However, since the input is available to the parser all along,
it would be more sensible to consider the input sentence from the very
beginning.  Such an approach is called *bottom-up parsing*, and is the
topic of the next section.


--------------------
Shift-Reduce Parsing 
--------------------


The simplest kind of bottom-up parsing is known as shift-reduce
parsing.  The parser repeadly pushes the next input word onto a stack;
this is the *shift* operation.  If the top
<replaceable>n</replaceable> items on the stack match the
<replaceable>n</replaceable> items on the right-hand side of some
production, then they are all popped off the stack, and the item on
the left-hand side of the production is pushed on the stack.  This
replacement of the top <replaceable>n</replaceable> items with a
single item is the *reduce* operation.  The parser finishes
when all the input is consumed and there is only one item remaining on the stack,
a parse tree with an ``S`` node as its root.


.. 
 To do: add examples and motivate more - what are we doing with
 bottom up - find little pieces and expand...


.. Note::
 Note that the reduce operation may only be applied to the top of the stack.
 Reducing items lower in the stack must be done before later items are pushed onto
 the stack.


The shift-reduce parser builds a parse tree during the above process.
If the top of stack holds the word ``dog`` and if the grammar has a
production ``N |rarr| dog`` then the reduce operation causes the word
to be replaced with the parse tree for this production.  For
convenience we will represent this tree as ``N(dog)``.  At a later
stage, if the top of the stack holds two items ``Det(the) N(dog)`` and
if the grammar has a production ``NP |rarr| Det N`` then the reduce
operation causes these two items to be replaced with ``NP(Det(the),
N(dog))``.  This process continues until a parse tree for the entire
sentence has been constructed.  We can see this in action using the
parser demonstration ``nltk_lite.draw.srparser``.  To run this
demonstration, use the following commands::

 from nltk_lite.draw.srparser import demo
 demo()

Six stages of the execution of this parser are shown below:

.. 
   To do: use letter identifiers for subfigures.

_srparser

.. figure::

.. image:: ../images.parsing/srparser1.png
.. image:: ../images.parsing/srparser2.png
.. image:: ../images.parsing/srparser3.png
.. image:: ../images.parsing/srparser4.png
.. image:: ../images.parsing/srparser5.png
.. image:: ../images.parsing/srparser6.png

   Six Stages of a Shift-Reduce Parser: initial, after one shift,
   after shift reduce shift reduce, after recognizing the second NP,
   complex NP, final step. 

A shift-reduce parser may fail to parse the sentence, even though the
sentence is well-formed according to the grammar.  In such cases,
there are no remaining input words to shift, and there is no way to
reduce the remaining items on the stack, as exemplified in the left
example below.  The parser entered this blind alley at an earlier
stage shown in the middle example below, when it reduced instead of
shifted.  This situation is called a *shift-reduce conflict*.  At
another possible stage of processing shown in the right example below,
the parser must choose between two possible reductions, both matching
the top items on the stack: ``V |rarr| V NP PP`` or ``NP |rarr| NP
PP``.  This situation is called a *reduce-reduce conflict*.

.. figure::

.. image:: ../images.parsing/srparser7.png 
.. image:: ../images.parsing/srparser8.png
.. image:: ../images.parsing/srparser9.png

   Conflict in Shift-Reduce Parsing

.. To do: diagram showing search tree with success and failure.

Shift-reduce parsers may implement policies for resolving such
conflicts.  For example, they may address shift-reduce conflicts by
shifting only when no reduces are possible, and they may address
reduce-reduce conflicts by favouring the reduce operation that removes
the most items from the stack.  No such policies are failsafe however.

The advantages of shift-reduce parsers over recursive descent parsers
is that they only build structure that corresponds to the words in the
input.  Furthermore, they only build each substructure once,
e.g. ``NP(Det(the), N(man))`` is only built and pushed onto the stack
a single time, regardless of whether it will later be used by the ``V
|rarr| V NP PP`` reduction or the ``NP |rarr| NP PP`` reduction.

.. 
 --------------------
 Advanced Topics in Parsing (optional) 
 --------------------

 Some of the following is out of date.  See the reference
 documentation for ``nltk_lite.parse`` for more up-to-date
 information.

 Parsing as a search problem.  Finding the right parse.

 late closure etc

 X.4 advanced topics (optional)
 - other approaches, evaluation, problems
 - challenges for particular languages / language families
 - research questions

----------------
Parsing in NLTK 
----------------

.. 
 X.5 implementation
 - how does NLTK do it?
 - simple problems and worked solutions
 - suggested projects (e.g. for your MSc students)

---------------------
Context Free Grammars
---------------------

The ``nltk_lite.cfg`` module defines a set of classes that are used to
define context free grammars:

1. The ``cfg.Nonterminal`` class is used to represent nonterminals.
 
#. The ``cfg.Production`` class is used to represent CFG productions.

#. The ``cfg.Grammar`` class is used to represent CFGs.

------------
Nonterminals
------------


``Nonterminal`` is a simple class that's used to let NLTK distinguish
terminals from nonterminals.  Each ``Nonterminal`` is defined by a
**symbol**, which is represented by a case-sensitive string.  Typical
symbols for ``Nonterminals`` are ``"S"`` (for sentence) and ``"NP"``
(for noun phrases).  To construct a ``Nonterminal``, use the
``Nonterminal`` constructor::

  >>> from nltk_lite.parse import cfg
  >>> S = cfg.Nonterminal('S')
  >>> S
  <S>
  >>> NP = cfg.Nonterminal('NP')
  >>> NP
  <NP>

..  Mention the symbol() method? -->
..  Mention division of nonterminal? -->

If you are defining many nonterminals at once, you can use the
``nonterminals()`` function.  This function takes a string containing
a list of symbol names, and returns a list of ``Nonterminals``
constructed from those symbol names::

  >>> VP, Adj, V, N = cfg.nonterminals('VP, Adj, V, N')
  >>> VP, Adj, V, N
  (<VP>, <Adj>, <V>, <N>)

When using the ``nonterminals()`` function, you should be always be
careful to make sure that the order of the variables you define
matches the order of the nonterminals in the string.

-----------
Productions
-----------

CFG productions are represented with the ``cfg.Production`` class.
Each ``Production`` specifies that a nonterminal (the **left-hand
side**) can be expanded to a sequence of terminals and nonterminals
(the **right-hand side**).  Productions are created with the
``cfg.Producition`` constructor, which takes a nonterminal left-hand
side, and zero or more terminals and nonterminals for the right-hand
side::

  >>> prod1 = cfg.Production(S, [NP, VP])
  >>> prod1
  S -> NP VP
  >>> prod2 = cfg.Production(NP, ['the', Adj, N])
  >>> prod2
  NP -> 'the' Adj N

The right-hand side may contain any number of elements.  In
particular, for so-called **epsilon productions**, the right-hand side
is empty.  When parsing natural language, epsilon productions are
often used for **traces**, which mark the position from which a
constituent moved::

  >>> trace = cfg.Nonterminal('t')
  >>> trace
  <t>

  >>> prod3 = cfg.Production(trace, [])
  >>> prod3
  t ->

----
CFGs
----

Context free grammars are encoded by the ``Grammar`` class.  A ``Grammar``
consists of a special **start** nonterminal, and an ordered list of
productions.  ``Grammars`` are created with the constructor::

  >>> grammar = cfg.Grammar(S, [prod1, prod2, prod3])
  >>> grammar
  <Grammar with 3 productions>

A grammar's start state is returned by the ``start``method; and its list
of productions is returned by the ``productions`` method::

  >>> grammar.start()
  <S>
  >>> grammar.productions()
  (S -> NP VP, NP -> 'the' Adj N, t ->)

A complete example of a toy grammar is given below::

  >>> from nltk_lite.parse import cfg
  >>> S, VP, NP, PP = cfg.nonterminals('S, VP, NP, PP')
  >>> V, N, P, Name, Det = cfg.nonterminals('V, N, P, Name, Det')
  >>> productions = (
  ...     cfg.Production(S, [NP, VP]),
  ...     cfg.Production(NP, [Det, N]),
  ...     cfg.Production(VP, [V, NP]),
  ...     cfg.Production(VP, [V, NP, PP]),
  ...     cfg.Production(NP, [Det, N, PP]),
  ...     cfg.Production(PP, [P, NP]),
  ...     cfg.Production(NP, ['I']),   cfg.Production(Det, ['the']),
  ...     cfg.Production(Det, ['a']),  cfg.Production(N, ['man']),
  ...     cfg.Production(V, ['saw']),  cfg.Production(P, ['in']),
  ...     cfg.Production(P, ['with']), cfg.Production(N, ['park']),
  ...     cfg.Production(N, ['dog']),  cfg.Production(N, ['telescope'])
  ... )
  >>> grammar = cfg.Grammar(S, productions)


.. Note:: All of the grammars shown here use atomic non-terminal
   symbols, e.g. ``VP``.  For extensions that support feature-based
   grammars and parsers, please refer to the
   ``nltk_contrib.mit.rspeer.parser`` module.

------
 Trees 
------

The ``nltk_lite.tree`` module defines classes and functions for
working with trees.

.. Note:: The ``nltk_lite.tree`` module currently has no direct
   support for representing movement, traces, and co-indexing.  We plan
   to extend the class to support these features more fully in the
   future.

1.  The node value is a string containing
    the tree's constituent type (e.g., "NP" or "VP").
#.  The children encode the hierarchical contents of the tree.

.. Note:: Although the ``Tree`` class is usually used for encoding
   syntax trees, it can be used to encode *any* homogeneous hierarchical
   structure that spans a text (such as morphological structure or
   discourse structure).  In the general case, leaves and node values do
   not have to be strings.

 A ``Tree`` with node value
`n` and children
`c:subscript:`1``,
`c:subscript:`2``, ...
`c:subscript:`n`` is
written ``(``n``:
`c:subscript:`1``
`c:subscript:`2`` ...
`c:subscript:`n``)``.
``Tree`` s are created with the 
``Tree constructor``, which takes a
node value and zero or more children.
Here we see a simple tree with a single child node, itself a token::

  >>> from nltk_lite.parse.tree import Tree
  >>> tree1 = Tree('NP', ['John'])
  >>> tree1
  (NP: 'John')

Here is an example with two children:

  >>> tree2 = Tree('NP', ['the', 'man'])
  >>> tree2
  (NP: 'the' 'man')

Finally, here is a more complex example, where one of the
children is itself a tree:

  >>> tree3 = Tree('VP', ['saw', tree2])
  >>> tree3
  (VP: 'saw' (NP: 'the' 'man'))

A tree's node value is accessed with the ``node`` property:: 

  >>> tree1.node
  'NP'

A tree's children are accessed by indexing:: 

  >>> tree3[0]
  'saw'
  >>> tree3[1]
  (NP: 'the' 'man')
  >>> for child in tree3:
  ...     print child
  saw
  (NP: 'the' 'man')

The printed representation for complex trees can be difficult to read.
In these cases, the ``draw`` method can be very useful.  This method
opens a new window, containing a graphical representation of the tree::

  tree3.draw()

.. Note::
 This and the following graphical interface are broken in NLTK 1.4, but will
 be fixed in NLTK 1.5.

The tree display window allows you to zoom in and out;
to collapse and expand subtrees; and to print the graphical
representation to a postscript file. 

To compare multiple trees in a single window, use the ``draw_trees()``
method, from the ``nltk.draw.tree`` module::

  from nltk.draw.tree import draw_trees
  draw_trees(tree1, tree2, tree3)

The ``Tree`` class implements a number of other useful methods.  See
the ``Tree`` reference documentation for more information about these
methods.

  >>> tree3.leaves()
  ['saw', 'the', 'man']
  >>> tree3.height()
  3

..  Mention tree positions? -->

--------------------------------
 Initializing Trees from Strings 
--------------------------------

A convenient way to initialize a large tree is from a labelled
bracketing::

  >>> from nltk_lite.parse import bracket_parse
  >>> sent = '(VP (VBD saw) (NP (DT the) (NN man)))'
  >>> tree = bracket_parse(sent)
  >>> tree
  (VP: (VBD: 'saw') (NP: (DT: 'the') (NN: 'man')))

---------------------------------------
 Reading Trees from the Treebank Corpus 
---------------------------------------

The ``nltk_lite.corpora`` module defines the ``treebank`` corpus,
which contains a collection of hand-annotated parse trees for English
text.

..  Add a reference to LDC here?

  >>> from nltk_lite.corpora import treebank
  >>> from itertools import islice
  >>> print list(islice(treebank.parsed(), 1))[0]
  (S:
    (NP-SBJ:
      (NP: (NNP: 'Pierre') (NNP: 'Vinken'))
      (,: ',')
      (ADJP: (NP: (CD: '61') (NNS: 'years')) (JJ: 'old'))
      (,: ','))
    (VP:
      (MD: 'will')
      (VP:
        (VB: 'join')
        (NP: (DT: 'the') (NN: 'board'))
        (PP-CLR:
          (IN: 'as')
          (NP: (DT: 'a') (JJ: 'nonexecutive') (NN: 'director')))
        (NP-TMP: (NNP: 'Nov.') (CD: '29'))))
    (.: '.'))


--------------------
The Parser Interface 
--------------------

The ``parse`` module defines the ``ParseI`` interface, which defines
the set of methods which all parsers should support.  The ``ParseI``
interface defines two methods:

..  
 I say "n best" and "in descending order of quality" here,
 but so far we've only talked about grammars with no notion of
 parse quality.  Will that be confusing?

1. The ``parse`` method returns the single best parse for a given
   text.  The text is represented as a list of ``Tokens``.  If no
   parses are found for the given text, then ``parse`` returns
   ``None``.

#. The ``get_parse_list`` method returns a list of the parses for the
   given text.

For example, here is what the recursive descent parser generates for a
simple sentence and grammar::

  >>> from nltk_lite import tokenize
  >>> sent = list(tokenize.whitespace('I saw a man in the park'))
  >>> from nltk_lite import parse
  >>> rd_parser = parse.RecursiveDescent(grammar)

  >>> for p in rd_parser.get_parse_list(sent):
  ...     print p
  (S:
    (NP: 'I')
    (VP:
      (V: 'saw')
      (NP:
        (Det: 'a')
        (N: 'man')
        (PP: (P: 'in') (NP: (Det: 'the') (N: 'park'))))))
  (S:
    (NP: 'I')
    (VP:
      (V: 'saw')
      (NP: (Det: 'a') (N: 'man'))
      (PP: (P: 'in') (NP: (Det: 'the') (N: 'park')))))

-----------------------
The Shift Reduce Parser 
-----------------------

The ``nltk.parser`` module defines ``ShiftReduceParser``, a simple
non-backtracking implementation of a bottom-up shift-reduce parser.
Since this parser does not implement any backtracking, it is not
guaranteed to find a parse for a text, even if one exists.
Furthermore, it will always find at most one parse, even if more
parses exist.

Shift reduce parsers are created from ``Grammars`` by the
``ShiftReduceParser`` constructor.  The constructor takes an optional
argument ``trace``.  If ``trace`` is greater than zero, then the
parser will describe the steps that it takes as it parses a text.
Higher values of ``trace`` produce more verbose descriptions.  ::

  >>> sr_parse = parse.ShiftReduce(grammar)

The following example shows the trace output generated by
``sr_parser`` on a simple sentence: 

  >>> sent = list(tokenize.whitespace('I saw a man'))
  >>> sr_parse.trace()
  >>> sr_parse.parse(sent)
  Parsing 'I saw a man'
      [ * I saw a man]
    S [ 'I' * saw a man]
    R [ <NP> * saw a man]
    S [ <NP> 'saw' * a man]
    R [ <NP> <V> * a man]
    S [ <NP> <V> 'a' * man]
    R [ <NP> <V> <Det> * man]
    S [ <NP> <V> <Det> 'man' * ]
    R [ <NP> <V> <Det> <N> * ]
    R [ <NP> <V> <NP> * ]
    R [ <NP> <VP> * ]
    R [ <S> * ]
  (S: (NP: 'I') (VP: (V: 'saw') (NP: (Det: 'a') (N: 'man'))))

NLTK also defines a graphical demonstration tool for the
shift reduce parser::

 from nltk.draw.srparser import demo
 demo()

----------------------------
The Recursive Descent Parser 
----------------------------

The ``nltk_lite.parser`` module defines ``RecursiveDescent``, a
simple recursive implementation of a top-down parser.  Unlike the
shift-reduce parser, this parser is guaranteed to find all parses for
`a sentence.  But because it's a simple recursive top-down parser, it
can enter an infinite loop if the grammar contains a left-recursive
production.

Recursive descent parsers are created from ``CFGs`` by the
``RecursiveDescent`` constructor.  The constructor takes an
optional argument ``trace``.  As with the shift reduce parser, this
value specifies how verbosely the parser should describe the steps
that it takes as it parses a text::

  >>> from nltk_lite.parse import *
  >>> rd_parser = RecursiveDescent(grammar)

The following example shows the trace output generated by
``rd_parser`` on a simple sentence, where ``E`` = expand and ``M`` =
match.  Material to the left of the asterisk has been matched and
incorporated into the parse tree::

  >>> sent = list(tokenize.whitespace('I saw a man'))
  >>> rd_parser.trace()
  >>> rd_parser.get_parse_list(sent)
  Parsing 'I saw a man'
      [ * <S> ]
    E [ * <NP> <VP> ]
    E [ * <Det> <N> <VP> ]
    E [ * 'the' <N> <VP> ]
    E [ * 'a' <N> <VP> ]
    E [ * <Det> <N> <PP> <VP> ]
    E [ * 'the' <N> <PP> <VP> ]
    E [ * 'a' <N> <PP> <VP> ]
    E [ * 'I' <VP> ]
    M [ 'I' * <VP> ]
    E [ 'I' * <V> <NP> ]
    E [ 'I' * 'saw' <NP> ]
    M [ 'I' 'saw' * <NP> ]
    E [ 'I' 'saw' * <Det> <N> ]
    E [ 'I' 'saw' * 'the' <N> ]
    E [ 'I' 'saw' * 'a' <N> ]
    M [ 'I' 'saw' 'a' * <N> ]
    E [ 'I' 'saw' 'a' * 'man' ]
    M [ 'I' 'saw' 'a' 'man' ]
    + [ 'I' 'saw' 'a' 'man' ]
    E [ 'I' 'saw' 'a' * 'park' ]
    E [ 'I' 'saw' 'a' * 'dog' ]
    E [ 'I' 'saw' 'a' * 'telescope' ]
    E [ 'I' 'saw' * <Det> <N> <PP> ]
    E [ 'I' 'saw' * 'the' <N> <PP> ]
    E [ 'I' 'saw' * 'a' <N> <PP> ]
    M [ 'I' 'saw' 'a' * <N> <PP> ]
    E [ 'I' 'saw' 'a' * 'man' <PP> ]
    M [ 'I' 'saw' 'a' 'man' * <PP> ]
    E [ 'I' 'saw' 'a' 'man' * <P> <NP> ]
    E [ 'I' 'saw' 'a' 'man' * 'in' <NP> ]
    E [ 'I' 'saw' 'a' 'man' * 'with' <NP> ]
    E [ 'I' 'saw' 'a' * 'park' <PP> ]
    E [ 'I' 'saw' 'a' * 'dog' <PP> ]
    E [ 'I' 'saw' 'a' * 'telescope' <PP> ]
    E [ 'I' 'saw' * 'I' ]
    E [ 'I' * <V> <NP> <PP> ]
    E [ 'I' * 'saw' <NP> <PP> ]
    M [ 'I' 'saw' * <NP> <PP> ]
    E [ 'I' 'saw' * <Det> <N> <PP> ]
    E [ 'I' 'saw' * 'the' <N> <PP> ]
    E [ 'I' 'saw' * 'a' <N> <PP> ]
    M [ 'I' 'saw' 'a' * <N> <PP> ]
    E [ 'I' 'saw' 'a' * 'man' <PP> ]
    M [ 'I' 'saw' 'a' 'man' * <PP> ]
    E [ 'I' 'saw' 'a' 'man' * <P> <NP> ]
    E [ 'I' 'saw' 'a' 'man' * 'in' <NP> ]
    E [ 'I' 'saw' 'a' 'man' * 'with' <NP> ]
    E [ 'I' 'saw' 'a' * 'park' <PP> ]
    E [ 'I' 'saw' 'a' * 'dog' <PP> ]
    E [ 'I' 'saw' 'a' * 'telescope' <PP> ]
    E [ 'I' 'saw' * <Det> <N> <PP> <PP> ]
    E [ 'I' 'saw' * 'the' <N> <PP> <PP> ]
    E [ 'I' 'saw' * 'a' <N> <PP> <PP> ]
    M [ 'I' 'saw' 'a' * <N> <PP> <PP> ]
    E [ 'I' 'saw' 'a' * 'man' <PP> <PP> ]
    M [ 'I' 'saw' 'a' 'man' * <PP> <PP> ]
    E [ 'I' 'saw' 'a' 'man' * <P> <NP> <PP> ]
    E [ 'I' 'saw' 'a' 'man' * 'in' <NP> <PP> ]
    E [ 'I' 'saw' 'a' 'man' * 'with' <NP> <PP> ]
    E [ 'I' 'saw' 'a' * 'park' <PP> <PP> ]
    E [ 'I' 'saw' 'a' * 'dog' <PP> <PP> ]
    E [ 'I' 'saw' 'a' * 'telescope' <PP> <PP> ]
    E [ 'I' 'saw' * 'I' <PP> ]
  [(S: (NP: 'I') (VP: (V: 'saw') (NP: (Det: 'a') (N: 'man'))))]

NLTK also defines a graphical demonstration tool for the
recursive descent parser:: 

  from nltk.draw.rdparser import demo
  demo()


----

NLTK_

.. _NLTK: http://nltk.sourceforge.net/

