.. -*- mode: rst -*-
.. include:: ../definitions.txt

=======
Tagging
=======

:Authors: Steven Bird, Ewan Klein, Edward Loper
:Version: |version|
:Revision: $Revision$
:Date: $Date$
:Copyright: |copy| |copyrightinfo|
:License: |license|

.. Note:: This is a draft.  Please send any feedback to the authors.

------------
Introduction
------------

Parts of Speech
---------------

Many natural language expressions are ambiguous, and we need to draw
on other sources of information to aid interpretation.  For instance,
our preferred interpretation of ``fruit flies like a
banana`` depends on the presence of contextual cues that cause
us to expect ``flies`` to be a noun or a verb.  Before
we can even address such issues, we need to be able to represent the
required linguistic information.  Here is a possible representation:


=========  =========  ========  =====  ==========
``Fruit``  ``flies``  ``like``  ``a``  ``banana``
noun       verb       prep      det    noun
=========  =========  ========  =====  ==========

=========  =========  ========  =====  ==========
``Fruit``  ``flies``  ``like``  ``a``  ``banana``
noun       noun       verb      det    noun
=========  =========  ========  =====  ==========

Most language processing systems must recognize and interpret the
linguistic structures that exist in a sequence of words.  This task is
virtually impossible if all we know about each word is its text
representation.  To determine whether a given string of words has the
structure of, say, a noun phrase, it is infeasible to check through a
(possibly infinite) list of all strings which can be classed as noun
phrases.  Instead we want to be able to generalise over *classes* of
words. These word classes are commonly given labels such as
'determiner', 'adjective' and 'noun'.  Conversely, to interpret words
we need to be able to discriminate between different usages, such as
``deal`` as a noun or a verb.  The process of classifying words in
this way, and labelling them accordingly, is known as *part-of-speech
tagging*, *POS-tagging*, or simply *tagging*.  The collection of tags
used for a particular task is known as a *tag set*.

We earlier presented two interpretations of *Fruit flies like a
banana* as examples of how a string of word tokens can be augmented
with information about the word classes that the words belong to. In
effect, we carried out tagging for the string ``fruit flies like a
banana``. However, tags are more usually attached inline to the text
they are associated with. This is illustrated in the following
sentence from the Brown Corpus: `` The/at Pantheon's/np$ interior/nn
,/, still/rb in/in its/pp$ original/jj form/nn ,/, is/bez truly/ql
majestic/jj and/cc an/at architectural/jj triumph/nn ./.  ``

.. According to our table later on, ./. should actually be ./end

Here, the sequence ``The/at`` means that the word token ``The`` is
tagged ``at``, which is the Brown Corpus tag for article.
(The reader may be initially puzzled by strings like
``,/,``. This just means that the tag for a comma is '``,``'.)
	
Motivation for Tagging
----------------------

We can think of tagging as one way of *annotating* a text corpus.
Annotation is a way of adding information to a text -- indeed, we
might like to think of it as a way of making explicit information
which is already implicitly present in the text.

What is the value of annotating a text in this way? One illustration
is the use of tagged corpora to study patterns of word usage in
different genres (*stylistics*).  For example, we can use the tags to
identify all words of a certain class, such as modals, then tabulate
their frequency of occurrence in different genres, as shown below:

==================  ===  =====  ===  =====  ====  ====
Use of Modals in Brown Corpus, by Genre
------------------------------------------------------
Genre               can  could  may  might  must  will 
==================  ===  =====  ===  =====  ====  ====
skill and hobbies   273  59     130  22     83    259 
humor               17   33     8    8      9     13 
fiction: science    16   49     4    12     8     16 
press: reportage    94   86     66   36     50    387 
fiction: romance    79   195    11   51     46    43 
religion            84   59     79   12     54    64 
==================  ===  =====  ===  =====  ====  ====

Another motivation for tagging is that it helps us predict the
behaviour of a previously unseen word.  For example, if we encounter
the word ``blogging`` we might be able to infer that it is a verb, with
the root ``blog``, and likely to occur after forms of the auxiliary
``to be`` (e.g. ``he was blogging``), and unlikely to occur after adjectives.

Parts of speech are also used in speech synthesis and
recognition.  For example, ``wind/nn``, as in ``the wind blew``, is
pronounced with a short vowel, whereas ``wind/vb``, as in ``wind the
clock``, is pronounced with a long vowel.  Other examples can be
found where the stress pattern differs depending on whether the word
is a noun or a verb, e.g. ``contest``, ``insult``, ``present``, ``protest``,
``rebel``, ``suspect``.

Kinds of Tagging
----------------

This tutorial focuses on part-of-speech tagging, as an early step in
language processing which does not depend on deep linguistic analysis.
Readers should be aware that are many other kinds of tagging.  Words
can be tagged with directives to a speech synthesiser, indicating
which words should be emphasised.  Words can be tagged with sense
numbers, indicating which sense of the word was used.  Words can also
be tagged with morphological features.  Examples of each of these
kinds of tags are shown below.
Note that for space reasons, we only show the tag on a single
italicised word. Note also that the first two examples use XML-style
tags, where elements in angle brackets enclose the word that is
tagged.

1. *Speech Synthesis Markup Language (W3C SSML):*
   ``That is a <emphasis>*big*</emphasis> car!``
#. *SemCor: Brown Corpus tagged with WordNet senses:*
   ``Space in any <wf pos="NN" lemma="form" wnsn="4">*form*</wf>
   is completely measured by the three dimensions.``
   (Wordnet form/nn sense 4: "shape, form, configuration,
   contour, conformation")
#. *Morphological tagging, from the Turin University Italian Treebank:*
   ``E' italiano , come progetto e realizzazione , il
   *primo* (PRIMO ADJ ORDIN M SING) porto turistico dell' Albania .``

We have already noted that part-of-speech tags are closely related to
the notion of word class used in syntax.  The assumption in
theoretical linguistics is that every distinct word type will be
listed in a lexicon (or dictionary), with information about its
pronunciation, syntactic properties and meaning. A key component of
the word's syntactic properties will be its class. When we carry out a
syntactic analysis of our earlier example ``fruit flies like a
banana``, we will look up each word in the lexicon, determine its word
class, and then group it into a hierarchy of phrases, as illustrated
in the following parse tree.

.. figure:: ../images/syntax-tree.png

   Syntactic Parse Tree


Common Part-of-Speech Tags
--------------------------

In this tree, we have used standard syntactic abbreviations for word
classes. These are shown in the following table, together with their
counterparts from the Brown tag set.

================   =========     ============================
Word Class Labels and Brown Corpus Tags
-------------------------------------------------------------
Word Class Label   Brown Tag     Word Class 
================   =========     ============================
Det                at            Article 
N                  nn            Noun 
V                  vb            Verb 
Adj                jj            Adjective 
P                  in            Preposition 
Card               cd            Number 
-                  end           Sentence-ending punctuation 
================   =========     ============================

In syntactic analyses, there is often a close connection between the
class a word belongs to and the phrases it forms with neighbouring
words. So, for example, a noun phrase (labelled NP) will usually
consist of a noun (N) optional accompanied by an article (Det) and
some modifiers such as adjectives (Adj). We can express this
generalisation in terms of a production rule:

  NP |rarr| Det Adj* N

In such a case, we say that the noun is the *head* of the noun phrase;
roughly speaking, the head of a phrase is a required element which can
occur in any context that the phrase as a whole can occur in.
However, from a purely notational point of view, we are free to use
any labels we want for word classes, and these could just as well be
the labels provided by a tag set. For example, we could replace the
preceding rule with the following:

  NP |rarr| at jj* nn

This is useful since some practical tasks, we might use an automatic
tagger to label the words in our example with tags drawn from the
Brown tag set, and use that as a basis for building a syntactic parse
tree such as the one we saw above.

So far, we have only looked at tags as capturing information about
word class. However, common tag sets like those used in the Brown
Corpus also capture a certain amount of *morpho-syntactic*
information. Consider, for example, the selection of distinct forms of
the word ``be`` illustrated in the following sentences:

  Be still!
  Being still is hard.
  I am still.
  I have been still all day.
  I was still all day.

We say that these forms are morpho-syntactically distinct because they
exhibit different morphological inflections and different
co-occurrence restrictions with neighbouring words. For example,
``am`` cannot replace either of the first two examples:

  \*Am still!
  \*Am still is hard.

These differences between the forms are encoded in their Brown Corpus
tags: ``be/be, being/beg, am/bem, been/ben`` and ``was/bedz``. This
means that an automatic tagger which uses this tag set is in effect
carrying out a limited amount of morphological analysis.

Exercises
---------

1. **Ambiguity resolved by part-of-speech tags**:
   Scour the web for examples of spoof newspaper headlines, such as
   ``British Left Waffles on Falkland Islands``, and
   ``Juvenile Court to Try Shooting Defendant``.
   Manually tag these headlines to see if knowledge of the part-of-speech
   tags removes the ambiguity.

#. **Explorations with part-of-speech tagged corpora**:
   Tokenize the Brown Corpus and build one or more suitable data structures
   so that you can answer the following questions.

   a)  What is the most frequent tag? (This is the tag we would want to
       assign with ``tag.Default``.
   #)  Which word has the greatest number of distinct tags?
   #)  What proportion of word types are always assigned the same part-of-speech tag?
   #)  What is the ratio of masculine to feminine pronouns?
   #)  How many words are ambiguous, in the sense that they appear with at least two tags?
   #)  What percentage of word *occurrences* in the Brown Corpus involve
       these ambiguous words?
   #)  Which nouns are more common in their plural form, rather than their singular form?
       (Only consider regular plurals, formed with the ``-s`` suffix.)
   #)  Produce an alphabetically sorted list of the distinct words tagged as ``md``.
   #)  Identify words which can be plural nouns or third person singular verbs
       (e.g. ``deals``, ``flies``).
   #)  Identify three-word prepositional phrases of the form IN + DET + NN
       (eg. ``in the lab``).
   #)  There are 264 distinct words having exactly three possible tags.
       Print a table with the integers 1..10 in one column, and the
       number of distinct words in the corpus having 1..10 distinct tags.
   #)  For the word with the greatest number of distrinct tags, print
       out sentences from the corpus containing the word, and giving
       examples of the different tags.

.. Chapter Overview
.. ----------------

.. In the next section we take ...


--------------
Simple Taggers
--------------

In this section we consider three simple taggers.  They all process
the input tokens one by one, adding a tag to each token.  In each case
they begin with tokenized text.  We can easily create a sample of
tokenized text as follows::

  >>> from nltk_lite import tokenize
  >>> text = "John saw 3 polar bears ."
  >>> tokens = list(tokenize.whitespace(text))
  >>> print tokens
  ['John', 'saw', '3', 'polar', 'bears', '.']

.. Note:: The tokenizer is a *generator* over tokens.  We cannot print
   it directly, but we can convert it to a list for printing, as shown
   in the above program.  Note that we can only use a generator once,
   but if we save it as a list, the list can be used many times over.

The Default Tagger
------------------

The simplest possible tagger assigns the same tag to each token
regardless of the token's text.  The ``DefaultTagger`` class
implements this kind of tagger.  In the following program, we create a
tagger called ``my_tagger`` which tags everything as a noun.

  >>> from nltk_lite import tag
  >>> my_tagger = tag.Default('nn')
  >>> list(my_tagger.tag(tokens))
  [('John', 'nn'), ('saw', 'nn'), ('3', 'nn'), ('polar', 'nn'), ('bears', 'nn'), ('.', 'nn')]

This is a simple algorithm, and it performs poorly when used on its
own. On a typical corpus, it will tag only 20%-30% of the tokens
correctly. However, it is a very reasonable tagger to use as a
default, if a more advanced tagger fails to determine a token's
tag. When used in conjunction with other taggers, a ``DefaultTagger``
can significantly improve performance.

.. Important:: Default taggers assign their tag to every single word,
   even words that have never been encountered before.  Thus, they help
   to improve the robustness of a language processing system.  We will
   return to them later, in the context of our discussion of *backoff*.

The Regular Expression Tagger
-----------------------------

The regular expression tagger assigns tags to tokens on the basis of
matching patterns in the token's text.  For instance, the following
tagger assigns ``cd`` to cardinal numbers, and ``nn`` to everything
else::

  >>> patterns = [(r'^-?[0-9]+(.[0-9]+)?$', 'cd'), (r'.*', 'nn')]
  >>> nn_cd_tagger = tag.Regexp(patterns)
  >>> list(nn_cd_tagger.tag(tokens))
  [('John', 'nn'), ('saw', 'nn'), ('3', 'cd'), ('polar', 'nn'), ('bears', 'nn'), ('.', 'nn')]

We can generalise this method to guess the correct tag for words based
on the presence of certain prefix or suffix strings.  For instance,
English words beginning with ``un-`` are likely to be adjectives.

The Unigram Tagger
------------------

The ``UnigramTagger`` class implements a simple statistical tagging
algorithm: for each token, it assigns the tag that is most likely for
that token's text. For example, it will assign the tag ``jj`` to any
occurrence of the word ``frequent``, since ``frequent`` is used as an
adjective (e.g. ``a frequent word``) more often than it is used as a
verb (e.g. ``I frequent this cafe``).

Before a ``UnigramTagger`` can be used to tag data, it must be trained
on a *training corpus*. It uses this corpus to determine which tags
are most common for each word.  ``UnigramTaggers`` are trained using
the ``train()`` method, which takes a tagged corpus::

  >>> from nltk_lite.corpora import brown
  >>> from itertools import islice
  >>> train_sents = list(islice(brown.tagged(), 500))  # sents 0..499
  >>> unigram_tagger = tag.Unigram()
  >>> unigram_tagger.train(train_sents)

Once a ``UnigramTagger`` has been trained, the ``tag()`` method can be
used to tag new text::

  >>> text = "John saw the book on the table"
  >>> tokens = list(tokenize.whitespace(text))
  >>> list(unigram_tagger.tag(tokens))
  [('John', 'np'), ('saw', 'vbd'), ('the', 'at'), ('book', None), ('on', 'in'), ('the', 'at'), ('table', None)]

As we noted earlier, ``Unigram`` will assign the default tag
``None`` to any token that was not encountered in the training data.
We can instruct it to *backoff* to our default ``nn_cd_tagger`` when
it cannot assign a tag itself::

  >>> unigram_tagger = tag.Unigram(backoff=nn_cd_tagger)
  >>> unigram_tagger.train(train_sents)
  >>> list(unigram_tagger.tag(tokens))
  [('John', 'np'), ('saw', 'vbd'), ('the', 'at'), ('book', 'nn'), ('on', 'in'), ('the', 'at'), ('table', 'nn')]

Now all the words are guaranteed to be tagged.


Affix Taggers
-------------

Affix taggers are like unigram taggers, except they are trained on
word prefixes or suffixes of a specified length.  (NB. Here we use
*prefix* and *suffix* in the string sense, not the morphological
sense.)  For example, the following tagger will consider suffixes of
length 3 (e.g. *-ize*, *-ion*), for words having at least 5
characters.

  >>> affix_tagger = tag.Affix(-3, 5, backoff=unigram_tagger)

Exercises
---------

1. **Regular Expression Tagging**:
   We defined the ``nn_cd_tagger``, which can be used
   as a fall-back tagger for unknown words.  This tagger only checks for
   cardinal numbers.  By testing for particular prefix or suffix strings,
   it should be possible to guess other tags.  For example, 
   we could tag any word that ends with ``-s``
   as a plural noun.
   Define a regular expression tagger (using ``tag.Regexp``
   which tests for at least five other patterns in the spelling of words.
   (Use inline documentation to explain the rules.)
          
#. **Unigram Tagging**:
   Train a unigram tagger and run it on some new text.
   Observe that some words are not assigned a tag.  Why not?

#. **Affix Tagging**::
   Train an affix tagger ``tag.Affix()`` and run it on some new text.
   Experiment with different settings for the affix length
   and the minimum word length.  Can you find a setting which seems
   to perform better than the one described above?

------------------
Evaluating Taggers
------------------

As we experiment with different taggers, it is important to have an
objective performance measure.  Fortunately, we already have manually
verified training data (the original tagged corpus), so we can use
that to evaluate our taggers.

Consider the following sentence from the Brown Corpus.  The 'Gold
Standard' tags from the corpus are given in the second column, while
the tags assigned by a unigram tagger appear in the third column.  Two
mistakes made by the unigram tagger are italicised.

===============  =============  ==============
Evaluating Taggers
----------------------------------------------
Sentence         Gold Standard  Unigram Tagger
===============  =============  ==============
The              at             at
President        nn-tl          nn-tl
said             vbd            vbd
he               pps            pps
will             md             md
ask              vb             vb
Congress         np             np
to               to             to
increase         vb             *nn*
grants           nns            nns
to               in             *to*
states           nns            nns
for              in             in
vocational       jj             jj
rehabilitation   nn             nn
.                .              .
===============  =============  ==============

The tagger correctly tagged 14 out of 16 words, so it gets a score of
14/16, or 87.5%.  Of course, accuracy should be judged on the basis of
a larger sample of data.  NLTK provides a function called
``tag.accuracy`` to automate the task.  In the simplest case, we
can test the tagger using the same data it was trained on:

  >>> acc = tag.accuracy(unigram_tagger, train_sents)
  >>> print 'Accuracy = %4.1f%%' % (100 * acc)
  Accuracy = 84.7%

However, testing a language processing system using the same data it
was trained on is unwise.  A system which simply memorised the
training data would get a perfect score without doing any linguistic
modelling.  Instead, we would like to reward systems that make good
generalizations, so we should test against *unseen data*, and replace
``train_sents`` above with ``unseen_sents``.  We can then define the
two sets of data as follows:

  >>> train_sents  = list(brown.tagged('a'))[:500]
  >>> unseen_sents = list(brown.tagged('a'))[500:600] # sents 500-599

Now we train the tagger using ``train_sents`` and evaluate it using
``unseen_sents``, as follows:

  >>> unigram_tagger = tag.Unigram(backoff=nn_cd_tagger)
  >>> unigram_tagger.train(train_sents)
  >>> acc = tag.accuracy(unigram_tagger, unseen_sents)
  >>> print 'Accuracy = %4.1f%%' % (100 * acc)
  Accuracy = 74.7%

The accuracy scores produced by this evaluation method are lower, but
they give a more realistic picture of the performance of the tagger.
Note that the performance of any statistical tagger is highly
dependent on the quality of its training set. In particular, if the
training set is too small, it will not be able to reliably estimate
the most likely tag for each word. Performance will also suffer if the
training set is significantly different from the texts we wish to tag.

In the process of developing a tagger, we can use the accuracy score
as an objective measure of the improvements made to the system.
Initially, the accuracy score will go up quickly as we fix obvious
shortcomings of the tagger.  After a while, however, it becomes more
difficult and improvements are small.

While the accuracy score is certainly useful, it does not tell us how
to improve the tagger.  For this we need to undertake error analysis.
For instance, we could construct a *confusion matrix*, with a row and
a column for every possible tag, and entries that record how often a
word with tag *T* :subscript:`i` is incorrectly tagged as *T*
:subscript:`j`.  Another approach is to analyse the context of errors.

  >>> errors = {}
  >>> for i in range(len(unseen_sents)):
  ...     raw_sent = tag.untag(unseen_sents[i])
  ...     test_sent = list(unigram_tagger.tag(raw_sent))
  ...     unseen_sent = unseen_sents[i]
  ...     for j in range(len(test_sent)):
  ...         if test_sent[j][1] != unseen_sent[j][1]:
  ...             test_context = test_sent[j-1:j+1]
  ...             gold_context = unseen_sent[j-1:j+1]
  ...             if None not in test_context:
  ...                 pair = (tuple(test_context), tuple(gold_context))
  ...                 errors[pair] = errors.get(pair, 0) + 1

The above program catalogs all errors, along with the tag on the left
and their frequency of occurrence.  The ``errors`` dictionary has keys
of the form ``((t1,t2),(g1,g2))``, where ``(t1,t2)`` are the test
tags, and ``(g1,g2)`` are the gold-standard tags.  The values in the
``errors`` dictionary are simple counts of how often the error
occurred.  With some further processing, we construct the list
``counted_errors`` containing tuples consisting of counts and errors,
and then do a reverse sort to get the most significant errors first:

  >>> counted_errors = [(errors[k], k) for k in errors.keys()]
  >>> counted_errors.sort()
  >>> counted_errors.reverse()
  >>> for err in counted_errors[:5]:
  ...     print err
  (32, ((), ()))
  (5, ((('the', 'at'), ('Rev.', 'nn')), (('the', 'at'), ('Rev.', 'np'))))
  (5, ((('Assemblies', 'nn'), ('of', 'in')), (('Assemblies', 'nns-tl'), ('of', 'in-tl'))))
  (4, ((('of', 'in'), ('God', 'nn')), (('of', 'in-tl'), ('God', 'np-tl'))))
  (3, ((('to', 'to'), ('form', 'nn')), (('to', 'to'), ('form', 'vb'))))

The fifth line of output records the fact that there were 3 cases
where the unigram tagger mistakenly tagged a verb as a noun, following
the word ``to``.  (We encountered the inverse of this mistake for the word
``increase`` in the above evaluation table, where the unigram tagger tagged
``increase`` as a verb instead of a noun since it occurred more often
in the training data as a verb.)  Here, when ``form`` appears
after the word ``to``, it is invariably a verb.  Evidently, the performance
of the tagger would improve if it was modified to consider not just
the word being tagged, but also the tag of the word on the left.  Such
taggers are known as bigram taggers, and we consider them next.

Exercises
---------

1. **Evaluating a Regular Expression Tagger**:
   Consider the regular expression tagger developed in the exercises in
   the previous section.  Evaluate the tagger using ``tag.accuracy()``,
   and try to come up with ways to improve its performance.  Discuss your findings.
   How does objective evaluation help in the development process?

#. **Evaluating a Unigram Tagger**:
   Apply our evaluation methodology to the unigram tagger developed in
   the previous section.  Discuss your findings.
          
#. **Affix Tagging**::
   Write a program which calls ``tag.Affix()`` repeatedly, using
   different settings for the affix length and the minimum word length.
   What parameter values give the best overall performance?  Why do
   you think this is the case?

--------------------
Higher Order Taggers
--------------------

Earlier we encountered the ``UnigramTagger``, which assigns a tag to a
word based on the identity of that word.  In this section we will look
at taggers that exploit a larger amount of context when assigning a
tag.

Bigram Taggers
--------------

As their name suggests, <glossterm>bigram taggers</glossterm> use two
pieces of information for each tagging decision.  Usually this
information is the text of the current word together with the tag of
the previous word. These two pieces of information constitute the
<glossterm>context</glossterm> for the token to be tagged. Given the
context, the tagger assigns the most likely tag.  We can visualise
this process with the help of the following bigram table, a tiny
fragment of the internal data structure built by a bigram tagger.

====  ====  =========  ====   ========   ======   ====  ======
Fragment of Bigram Table
--------------------------------------------------------------
      ask   Congress   to     increase   grants   to    states
====  ====  =========  ====   ========   ======   ====  ======
at                            nn
tl                     to                         to 
bd                     to                nns      to
md    *vb*                     vb
vb          *np*        to                *nns*     to    nns 
np                     *to*                        to
to    vb               *vb*  
nn          np         to     nn         nns      to
nns                    to                         *to*
in          np         in                         in    *nns* 
jj                     to                nns      to    nns 
====  ====  =========  ====   ========   ======   ====  ======

.. source code for bigram table
   from nltk.tagger import *
   from nltk.corpus import brown
   train_tokens = []
   for item in brown.items()[:10]:
       train_tokens.append(brown.read(item))
   mytagger = tag.ngram(2)
   for tok in train_tokens: mytagger.train(tok)
   words = '''ask Congress to increase grants to states'''.split()
   tags = '''at nn-tl vbd md vb np to nn nns in jj'''.split()

   print "     ",
   for word in words:
       print " %s " % word,
   print
   for tag in tags:
       print "%5s" % tag,
       for word in words:
           guess = mytagger._freqdist[((tag,), word)].max()
           if not guess: guess=""
           print " %s " % guess,
       print
    
The best way to understand the table is to work through an example.
Suppose we have already processed the sentence ``The President will
ask Congress to increase grants to states for vocational
rehabilitation .`` as far as ``will/md``.  We can use the table to
simply read off the tags that should be assigned to the remainder of
the sentence.  When preceded by ``md``, the tagger guesses that the
word ``ask`` has the tag ``vb`` (italicised in the table).  Moving to
the next word, we know it is preceded by ``vb``, and looking across
this row we see that ``Congress`` is assigned the tag ``np``.  The
process continues through the rest of the sentence.  When we encounter
the word ``increase``, we correctly assign it the tag ``vb`` (unlike
the unigram tagger which assigned it ``nn``).  However, the bigram
tagger mistakenly assigns the infinitival tag to the word ``to``
immediately preceding ``states``, and not the preposition tag.  This
suggests that we may need to consider even more context in order to
get the correct tag.

N-Gram Taggers
--------------

As we have just seen, it may be desirable to look at more than just
the preceding word's tag when making a tagging decision.  An *n-gram
tagger* is a generalisation of a bigram tagger whose context is the
current token's text together with the part-of-speech tags of the *n*
preceding tokens, as shown in the following diagram. It then picks the
tag which is most likely for that context. The tag to be chosen, *t*
:subscript:`k`, is circled, and the context is shaded in grey. In this
example of an n-gram tagger, we have *n=3*; that is, we consider the
tags of the two preceding words in addition to the current word.

.. figure:: ../images/tag-context.png

   Tagger Context
 
.. note:: A 1-gram tagger is another term for a unigram tagger: i.e.,
   the context used to tag a token is just the text of the token itself.
   2-gram taggers are also called *bigram taggers*, and 3-gram taggers
   are called *trigram taggers*.  We have defined bigram and trigram
   taggers as special cases of n-gram taggers.

``tag.Ngram`` uses a tagged training corpus to determine which
part-of-speech tag is most likely for each context.  Here we see
a special case of n-gram tagger, namely a bigram tagger::

  >>> bigram_tagger = tag.Bigram()
  >>> bigram_tagger.train(brown.tagged(['a','b']))

Once a bigram tagger has been trained, it can be used to tag untagged
corpora:

  >>> text = "John saw the book on the table"
  >>> tokens = list(tokenize.whitespace(text))
  >>> list(bigram_tagger.tag(tokens))
  [('John', 'np'), ('saw', 'vbd'), ('the', 'at'), ('book', 'nn'), ('on', 'in'), ('the', 'at'), ('table', None)]

As with the other taggers considered earlier, n-gram taggers will
assign the default tag ``None`` to any token whose context was not
encountered in the training data.  Note that as *n* gets larger, the
specificity of the contexts increases; and with it, the chance that
the data we wish to tag will contain contexts that were not present in
the training data. This is sometimes referred to as a *sparse data*
problem. Thus, there is a trade-off between the accuracy and the
coverage of our results. This is a common type of trade-off in natural
language processing. It is closely related to the *precision/recall
trade-off* that we'll encounter later when we discuss information
retrieval.

.. Note:: n-gram taggers should not consider context that crosses a
   sentence boundary.  Accordingly, NLTK taggers are designed to work
   with lists of sentences, where each sentence is a list of words, a
   doubly-nested structure.

Combining Taggers
-----------------

One way to address the trade-off between accuracy and coverage is to
use the more accurate algorithms when we can, but to fall back on
algorithms with wider coverage when necessary. For example, we could
combine the results of a bigram tagger, a unigram tagger, and
a ``nn_cd_tagger``, as follows:

1. Try tagging the token with the bigram tagger.
2. If the bigram tagger is unable to find a tag for the token, try
   finding a tag with the unigram tagger.
3. If the unigram tagger is also unable to find a tag, use a
   default tagger to find a tag.

Each NLTK tagger permits a backoff-tagger to be specified.
The backoff-tagger may itself have a backoff tagger::

  >>> t0 = tag.Default('nn')
  >>> t1 = tag.Unigram(backoff=t0)
  >>> t2 = tag.Bigram(backoff=t1)
  >>> t1.train(brown.tagged('a'))
  >>> t2.train(brown.tagged('a'))

.. Note:: We specify the backoff tagger when the tagger is
   initialized, so that training can take advantage of the backing off.
   Thus, if the bigram tagger and its unigram backoff tagger would assign
   the same tag in a certain context, the training instance is discarded.
   This keeps the bigram tagger model as small as possible.  We can
   further specify that a tagger needs to see more than one instance of a
   context in order to retain it, e.g. ``Bigram(cutoff=2, backoff=t1)``
   will discard contexts which have only been seen once or twice.

Now we can test the taggers::

  >>> accuracy0 = tag.accuracy(t0, unseen_sents)
  >>> accuracy1 = tag.accuracy(t1, unseen_sents)
  >>> accuracy2 = tag.accuracy(t2, unseen_sents)

  >>> print 'Default Accuracy = %4.1f%%' % (100 * accuracy0)
  Default Accuracy = 13.1%
  >>> print 'Unigram Accuracy  = %4.1f%%' % (100 * accuracy1)
  Unigram Accuracy  = 87.9%
  >>> print 'Bigram Accuracy = %4.1f%%' % (100 * accuracy2)
  Bigram Accuracy = 85.2%


Exercises
---------

1. **Bigram Tagging**:
   Train a bigram tagger with no backoff tagger, and run it on some of the training
   data.  Next, run it on some new data.  What happens to the
   performance of the tagger?  Why?

#. **Combining taggers**:
   There is typically a trade-off between the accuracy and coverage
   for taggers: taggers that use more specific contexts usually
   produce more accurate results, when they have seen those contexts
   in the training data; but because the training data is limited,
   they are less likely to encounter each context. The backoff
   argument of the tagger initializer permits us to address this
   problem by trying taggers with more specific contexts first; and
   falling back to the more general taggers when necessary. Create a
   default tagger and various unigram and n-gram taggers,
   incorporating backoff, and train them on part of the Brown corpus.

   a) Create three different combinations of the taggers. Test the
      accuracy of each combined tagger. Which combination works best?

   #) Try varying the size of the training corpus. How does it affect
      your results?

#. **Tagger context** (advanced):

   N-gram taggers choose a tag for a token based on its text and the
   tags of the *n-1* preceding tokens. This is a common context to use
   for tagging, but certainly not the only possible context.
   Construct a new tagger, subclassed from ``SequentialTagger``, that
   uses a different context. If your tagger's context contains
   multiple elements, then you should combine them in a
   ``tuple``. Some possibilities for elements to include are: (i) the
   text of the current token, or of a previous token; (ii) the length
   of the current token's text, or of a previous token's text. (iii)
   the first letter of the current token's text, or of a previous
   token's text. (iv) the tag of a previous token.  Try to choose
   context elements that you believe will help the tagger decide which
   tag is appropriate. Keep in mind the trade-off between more
   specific taggers with accurate results; and more general taggers
   with broader coverage.  Combine your tagger with other taggers
   using the backoff method.

   a) How does the combined tagger's accuracy compare to the basic tagger? 

   #) How does the combined tagger's accuracy compare to the combined taggers you
      created in the previous exercise? 

#. **Reverse sequential taggers** (advanced):
   Since sequential taggers tag tokens in order, one at a time, they
   can only use the predicted tags to the *left* of the current token
   to decide what tag to assign to a token. But in some cases, the
   *right* context can provide more information about what tag should
   be used. A reverse sequential tagger is a tagger that: (i) assigns
   tags to one token at a time, starting with the last token of the
   text, and proceeding in right-to-left order. (ii) decides which tag
   to assign a token on the basis of that token, the tokens that
   follow it, and the predicted tags for the tokens that follow it.

   There is no need to create new classes to perform reverse
   sequential tagging.  By reversing texts at appropriate times, we
   can use sequential tagging classes to perform reverse sequential
   tagging. In particular, we should reverse the training text before
   we train the tagger; and reverse the text that we wish to tag both
   before and after we use the sequential tagger. Use this technique
   to create a bigram reverse sequential tagger.

   a) Measure its accuracy on a tagged section of the Brown corpus. Be
      sure to use a different section of the corpus for testing than you
      used for training.

   b) How does its accuracy compare to a first order sequential
   tagger, using the same training data and test data?
        
#. **Alternatives to backoff**:
   Create a new kind of tagger that combines several taggers using
   a new mechanism other than backoff (e.g. voting).  For robustness
   in the face of unknown words, include a regexp tagger, a unigram
   tagger that removes a small number of prefix or suffix characters
   until it recognises a word, or an n-gram tagger that does not
   consider the text of the token being tagged.
        
----------------
The Brill Tagger
----------------

A potential issue with n-gram taggers is their size.  If tagging is
to be employed in a variety of language technologies deployed on
mobile computing devices, it is important to find ways to reduce the
size of models without overly compromising performance.  An n-gram
tagger with backoff may store trigram and bigram tables, large sparse
arrays which may have hundreds of millions of entries.  As we saw in
the above bigram table, n-gram models only consider the tags of words
in the context.  A consequence of the size of the models is that it is
simply impractical for n-gram models to be conditioned on the
identities of words in the context.  In this section we will examine
Brill tagging, a statistical tagging method which performs very well
using models that are only a tiny fraction of the size of n-gram
taggers.

Brill tagging is a kind of *transformation-based learning*.  The
general idea is very simple: guess the tag of each word, then go back
and fix the mistakes.  In this way, a Brill tagger successively
transforms a bad tagging of a text into a good one.  As with n-gram
tagging this is a *supervised learning* method, since we need
annotated training data.  However, unlike n-gram tagging, it does
not count observations but compiles a list of transformational
correction rules.

The process of Brill tagging is usually explained by analogy with
painting.  Suppose we were painting a tree, with all its details of
boughs, branches, twigs and leaves, against a uniform sky-blue
background.  Instead of painting the tree first then trying to paint
blue in the gaps, it is simpler to paint the whole canvas blue, then
"correct" the tree section by overpainting the blue background.  In
the same fashion we might paint the trunk a uniform brown before going
back to overpaint further details with a fine brush.  Brill tagging
uses the same idea: get the bulk of the painting right with broad
brush strokes, then fix up the details.  As time goes on, successively
finer brushes are used, and the scale of the changes becomes
arbitrarily small.  The decision of when to stop is somewhat
arbitrary.  The following table illustrates this process, first
tagging with the unigram tagger, then fixing the errors.

==============  =======  ==========  =================================  =============================
Steps in Brill Tagging
-----------------------------------------------------------------------------------------------------
Sentence:       Gold:    Unigram:    Replace ``nn`` with ``vb``         Replace ``to`` with ``in``
                                     when the previous word is ``to``   when the next tag is ``nns`` 
The             at       at
President       nn-tl    nn-tl
said            vbd      vbd
he              pps      pps
will            md       md
ask             vb       vb
Congress        np       np
to              to       to
increase        vb       *nn*         *vb*
grants          nns      nns
to              in       *to*         *to*                                *in*
states          nns      nns
for             in       in
vocational      jj       jj
rehabilitation  nn       nn
==============  =======  ==========  =================================  =============================

In this table we see two rules.  All such rules are generated from a
template of the following form: form "replace *T* :subscript:`1` with *T*
:subscript:`2` in the context *C*".  Typical contexts are the identity
or the tag of the preceding or following word, or the appearance of a
tag within 2-3 words of of the current word.  During its training
phase, the tagger guesses values for *T* :subscript:`1`, *T*
:subscript:`2` and *C*, to create thousands of candidate rules.  Each
rule is then assigned a score based on its net benefit: the number of
incorrect tags that it corrects, less the number of correct tags it
incorrectly modifies.  This process is best illustrated by a listing
of the output from the NLTK Brill tagger (here run on tagged Wall
Street Journal text from the Penn Treebank). [#]_

.. [#] We are grateful to Christopher Maloof for developing this Brill tagger
       for NLTK.

::

  Loading tagged data...
  Training unigram tagger: [accuracy: 0.820940]
  Training Brill tagger on 37168 tokens...
 
  Iteration 1: 1482 errors; ranking 23989 rules;
    Found: "Replace POS with VBZ if the preceding word is tagged PRP"
    Apply: [changed 39 tags: 39 correct; 0 incorrect]
 
  Iteration 2: 1443 errors; ranking 23662 rules;
    Found: "Replace VBP with VB if one of the 3 preceding words is tagged MD"
    Apply: [changed 36 tags: 36 correct; 0 incorrect]
 
  Iteration 3: 1407 errors; ranking 23308 rules;
    Found: "Replace VBP with VB if the preceding word is tagged TO"
    Apply: [changed 24 tags: 23 correct; 1 incorrect]
 
  Iteration 4: 1384 errors; ranking 23057 rules;
    Found: "Replace NN with VB if the preceding word is to"
    Apply: [changed 67 tags: 22 correct; 45 incorrect]
  ...
  Iteration 20: 1138 errors; ranking 20717 rules;
    Found: "Replace RBR with JJR if one of the 2 following words is tagged NNS"
    Apply: [changed 14 tags: 10 correct; 4 incorrect]
 
  Iteration 21: 1128 errors; ranking 20569 rules;
    Found: "Replace VBD with VBN if the preceding word is tagged VBD"
  [insufficient improvement; stopping]
 
  Brill accuracy: 0.835145


Brill taggers have another interesting property: the rules are
linguistically interpretable.  Compare this with the n-gram taggers,
which employ a potentially massive table of n-grams.  It is not clear
what we can learn from inspecting such a table.

Exercises
---------

1. Try the Brill tagger demonstration, as follows:

  >>> from nltk_lite.tag import brill
  >>> brill.demo()

#. Consult the documentation for the demo function, using ``help(brill.demo)``.
   Experiment with the tagger by setting different values for the parameters.
   Is there any trade-off between training time (corpus size) and performance?

#. (Advanced) Inspect the diagnostic files created by the tagger ``rules.out`` and
   ``errors.out``.  Obtain the demonstration code (``nltk_lite/tag/brill.py``)
   and create your own version of the Brill tagger.

   a) Delete some of the rule templates, based on what you learned
      from inspecting ``rules.out``.

   b) Add some new rule templates which employ contexts that might help
      to correct the errors you saw in ``errors.out``.

----------
Conclusion
----------

This chapter has introduced the language processing task known as
tagging, with an emphasis on part-of-speech tagging.  English word
classes and their corresponding tags were introduced.  We showed how
tagged tokens and tagged corpora can be represented, then discussed a
variety of taggers: default tagger, regular expression tagger, unigram
tagger, n-gram taggers, and the Brill tagger.  We also described
some objective evaluation methods.  In the process, the reader has
been introduced to two important paradigms in language processing,
namely *language modelling* and *transformation-based learning*.  The
former is extremely general, and we will encounter it again later.
The latter had to be specially tailored to the tagging task, but
resulted in smaller, linguistically-interpretable models.

.. Discussion of how statistical methods have been used to
   reach a linguistically interpretable, symbolic result.
   Contrast between n-gram and Brill tagger about whether
   we can learn anything from inspecting the model itself
   (n-gram data vs transformational rules).

There are several other important approaches to tagging involving
*Hidden Markov Models* (see ``nltk_lite.tag.hmm``) and *Finite State
Transducers*, though a discussion of these approaches falls outside
the scope of this chapter.  Later we will see a generalization of
tagging called *chunking* in which a contiguous sequence of words is
assigned a single tag.

Tagging exhibits several properties that are characteristic of natural
language processing.  First, tagging involves *classification*: words have
properties; many words share the same property (e.g. ``cat`` and ``dog``
are both nouns), while some words can have multiple such properties
(e.g. ``wind`` is a noun and a verb).  Second, in tagging, disambiguation
occurs via *representation*: we augment the representation of tokens with
part-of-speech tags.  Third, training a tagger involves *sequence learning
from annotated corpora*.  Finally, tagging uses *simple, general, methods*
such as conditional frequency distributions and transformation-based learning.

---------------
Further Reading
---------------

Tagging: Jurafsky and Martin, Chapter 8

Brill tagging: Manning and Schutze 361ff; Jurafsky and Martin 307ff

HMM tagging: Manning and Schutze 345ff

Wikipedia:
``http://en.wikipedia.org/wiki/Part-of-speech_tagging``

List of available taggers:
``http://www-nlp.stanford.edu/links/statnlp.html``

-----------------
Further Exercises
-----------------

1. **Comparing n-gram taggers and Brill taggers**:
   Investigate the relative performance of n-gram taggers with backoff
   and Brill taggers as the size of the training data is increased.
   Consider the training time, running time, memory usage, and accuracy,
   for a range of different parameterizations of each technique.
        
#. **Application to other languages**:
   Obtain some tagged data for another language, and train and
   evaluate a variety of taggers on it.  If the language is
   morphologically complex, or if there are any orthographic clues
   (e.g. capitalization) to word classes, consider developing a
   regular expression tagger for it (ordered after the unigram
   tagger, and before the default tagger).  How does the accuracy of
   your tagger(s) compare with the same taggers run on English data?
   Discuss any issues you encounter in applying these methods to the
   language.
        
#. **HMM taggers**:
   Explore the Hidden Markov Model tagger included with NLTK,
   ``nltk_lite.tag.hmm``.

----

NLTK_

.. _NLTK: http://nltk.sourceforge.net/

