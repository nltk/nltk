.. -*- mode: rst -*-
.. include:: ../definitions.txt

=======================
Projects with NLTK-Lite
=======================

:Authors: Steven Bird
:Contact: sb@csse.unimelb.edu.au
:Version: |version|
:Revision: $Revision$
:Date: $Date$
:Copyright: |copy| 2001-2005 University of Pennsylvania
:License: Creative Commons Attribution-NonCommercial-ShareAlike License

------------
Introduction 
------------

This document describes a variety of possible natural language
processing projects that can be undertaken using NLTK-Lite.

The NLTK team welcomes contributions of good student projects, and
some past projects (e.g. the Brill and HMM taggers) have been
incorporated into the toolkit.

--------------
Project Topics
--------------

Linguistically Oriented
------------------------

1. Develop a concordance system for the Brown Corpus, supporting
   searches that include part-of-speech tags, replicating some of the
   functionality of commercial software (e.g. MonoConc_).
   Investigate indexing for more efficient searches.

.. _MonoConc: http://www.athel.com/mono.html

#. Develop a morphological analyser for a language of your choice.

#. Write a soundex function that is appropriate for a language you are
   interested in.  If the language has clusters (consonants or
   vowels), consider how reliably people can discriminate the second
   and subsequent member of a cluster.  If these are highly
   confusible, ignore them in the signature.  If the *order* of
   segments in a cluster leads to confusion, normalise this in the
   signature (e.g. sort each cluster alphabetically, so that a word
   like ``treatments`` would be normalised to ``rtaemtenst``, before
   the code is computed).  (NB. See field.html for more details.)

#. Develop a text classification system which efficiently classifies documents
   in two or three closely related languages. Consider the discriminating features
   between languages despite their apparent similarity. Implementation should
   be evaluated using unseen data.

#. Explore the phonotactic system of a language you ar interested in.
   Compare your findings to a published phonological or grammatical
   description of the same language.

#. Implement a structured text rendering module which takes linguistic
   data from a source such as Shoebox and generates XML based lexicon or interlinear
   text based on user preferences for field exports.

#. Develop a grammatical paradigm generation function which takes some
   form of tagged text as input and generates paradigm representations of 
   related linguistic features.

Computationally Oriented
------------------------

1. Build a language-guesser, to classify text documents by language,
   replicating some of the functionality of TextCat_.
   Create an evaluation dataset, possibly using the Opus_Corpus_.

.. _TextCat: http://odur.let.rug.nl/~vannoord/TextCat/
.. _Opus_Corpus: http://logos.uio.no/opus/

2. Develop a semantic similarity module based on WordNet_ and two of
   Pedersen's Wordnet_Similarity_ algorithms.  Develop a lexical-chain
   based WSD system, and evaluate it using the SEMCOR corpus (corpus reader
   provided in NLTK).

.. _WordNet: http://wordnet.princeton.edu/
.. _WordNet_Similarity: http://wn-similarity.sourceforge.net/

3. Build a text compression system combining a dictionary-based
   compression algorithm (such as the ones described in
   Managing_Gigabytes_) along with information provided by a
   part-of-speech tagger which should lower the conditional entropy of
   the following word.

.. _Managing_Gigabytes: http://www.cs.mu.oz.au/mg/

4. Implement a classifier which detects emotion in text, using one of
   the emotional text databases from the HUMAINE_Portal_ for training and testing.

.. _HUMAINE_Portal: http://emotion-research.net/

5. Re-implement any NLTK-Lite functionality for a language other than
   English (tokenizer, tagger, chunker, parser, etc).  You will
   probably need to collect suitable corpora, and develop corpus
   readers.

6. Implement an LPath_ tree query interpreter.

.. _LPath: http://www.ldc.upenn.edu/Projects/QLDB/

7. Create a database of named entities, categorised as: person,
   location, organisation, cardinal, duration, measure, date.
   Train a named-entity tagger using the NIST IEER data
   (included with NLTK) and use it to tag more text and collect
   an expanded set of named entities.
   
8. Port the NLTK text classification system to NLTK-Lite.

9. Implement a system for cascaded_chunking_.

.. _cascaded_chunking: http://www.vinartus.net/spa/97a.pdf

10. Implement a chat-bot that incorporates a more sophisticated
    dialogue model than ``nltk_lite.chat.eliza``.

11. Implement a feature-based grammar and parser in NLTK-Lite
    (incorporate nltk.contrib.mit.rspeer)

12. Implement a categorial grammar parser, including semantic
    representations.

13. Develop a prepositional phrase attachment classifier, using
    the ``ppattach`` corpus for training and testing.

14. Develop a program for unsupervised learning of phonological rules,
    using the method described by Goldwater and Johnson:
    http://acl.ldc.upenn.edu/acl2004/sigphon/pdf/goldwater.pdf

15. Use WordNet to infer lexical semantic relationships on the entries
    of a Shoebox lexicon for some arbitrary language.

----------
Assessment
----------

This section describes the project assessment requirements for
*433-460 Human Language Technology* at the University of Melbourne.
Project assessment has three components: an oral presentation (5%),
a written report (10%), and an implementation (20%).

Oral Presentation
-----------------

Students will give a 10-minute oral presentation to the rest of the
class in the second-last week of semester.  This will be evaluated for
the quality of content and presentation:

* presentation (clarity, presentation materials, organization)
* content (defining the task, motivation, data, results, outstanding issues)

Written Report
--------------

Students should submit a ~5-page written report, with approximately
one page covering each of the following points:

* introduction (define the task, motivation)
* method (any algorithms, data)
* implementation (description, how to run it)
* results (e.g. show some output and discuss)
* evaluation (your critical discussion of the work) 

This should be prepared using the Python ``docutils`` and ``doctest``
packages.  These are easily learnt, and ideally suited for creating
reports with embedded program code, and they have been used for all
NLTK-Lite documentation.  For a detailed example, see the text
source for the NLTK-Lite tagging tutorial (text_, html_).

.. _text: http://nltk.sourceforge.net/lite/doc/en/tag.txt
.. _html: http://nltk.sourceforge.net/lite/doc/en/tag.html

* Docutils_: an open-source text processing system for processing
  plaintext documentation into useful formats, such as HTML or
  LaTeX. It includes reStructuredText, the easy to read, easy to use,
  what-you-see-is-what-you-get plaintext markup language.

.. _Docutils: http://docutils.sourceforge.net/

* Doctest*: a standard Python module that searches for pieces of text
  that look like interactive Python sessions, and then executes those
  sessions to verify that they work exactly as shown.

.. _Doctest: http://docs.python.org/lib/module-doctest.html

Implementation
--------------

Marks will be be awarded for the basic implementation and for various
kinds of complexity, as described below:

* Basic implementation (10%)

 - we are able to run the system
 - we can easily test the system (interface is usable, output is appropriately detailed and clearly formatted)
 - we can easily work out how the system is implemented (understandable code, inline documentation; you can assume we read the report first)
 - the system implements NLP algorithms (i.e. relevant to the subject, re-using existing NLP algorithms wherever possible instead of reinventing the wheel)
 - the NLP algorithms are correctly implemented 

* Complexity (10%)

 - the system implements a non-trivial problem
 - the system combines multiple HLT components as appropriate
 - appropriate training data is used (effort in obtaining and preparing the data will be considered)
 - the system permits exploration of the problem domain and the algorithms (e.g. through appropriate parameterization)
 - a range of system configurations/modifications are explored (e.g. classifiers trained and tested using different parameters) 



----

NLTK_

.. _NLTK: http://nltk.sourceforge.net/
