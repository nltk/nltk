.. -*- mode: rst -*-
.. include:: ../definitions.txt

=============================================
Accessing and Analyzing Linguistic Field Data
=============================================

:Authors: Steven Bird
:Contact: sb@csse.unimelb.edu.au
:Version: |version|
:Revision: $Revision$
:Date: $Date$
:Copyright: |copy| 2001-2005 University of Pennsylvania
:License: Creative Commons Attribution-NonCommercial-ShareAlike License

.. Note:: This is a draft.  Please send any feedback to the authors.

------------
Introduction 
------------

Linguistic fieldwork deals with essentially three kinds of data:
lexicons, paradigms and texts. A lexicon is a database of words,
minimally containing part of speech information and glosses. A
paradigm (broadly construed) is any kind of rational tabulation of
words or phrases to illustrate contrasts and systematic variation.  A
text is essentially any larger unit such as a narrative or a
conversation.  In addition to these three kinds of data, linguistic
fieldwork deals with three main kinds of description: field notes,
grammars and analytical papers.

These various kinds of data and description enter into a complex web
of relations. For example, the discovery of a new word in a text may
require an update to the lexicon and the construction of a new
paradigm (e.g. to correctly classify the word). Such updates may
occasion the creation of some field notes, the extension of a grammar
and possibly even the revision of the manuscript for an analytical
paper. Progress on description and analysis gives fresh insights about
how to organise existing data and it informs the quest for new
data.

Whether one is sorting data, or generating tabulations, or gathering
statistics, or searching for a (counter-)example, or verifying the
transcriptions used in a manuscript, the principal challenge is
computational.  In the following we will consider various methods
for manipulating linguistic field data, focussing on data stored in
the popular *Shoebox* format.

-----------------------------------------------------------------
Tools and technologies for language documentation and description
-----------------------------------------------------------------

Language documentation projects are increasing in their reliance on
new digital technologies and software tools.  Bird and Simons (2003)
identified and categorized a wide variety of these tools.  We briefly
review these here, and mention various ways that our own programs can
interface with them.

General purpose tools
---------------------

Conventional office software is widely used in computer-based language
documentation work, given its familiarity and ready availability.
This includes word processors and spreadsheets.

**Word Processors.** These are often used in creating dictionaries and
interlinear texts.  However, it is rather time-consuming to maintain
the consistency of the content and format.  Consider a dictionary in
which each entry has a part-of-speech field, drawn from a set of 20
possibilities, displayed after the pronunciation field, and rendered
in 11-point bold.  No convential word processor has search or macro
functions capable of verifying that all part-of-speech fields have
been correctly entered and displayed.  This task requires exhaustive
manual checking.  If the word processor permits the document to be
saved in a non-proprietary format, such as RTF, HTML, or XML, it may
be possible to write programs to do this checking automatically.

Consider the following fragment of a lexical entry:
"sleep [sli:p] **vi** *condition of body and mind...*\ ".
We can enter this in MSWord, then "Save as Web Page",
then inspect the resulting HTML::

    <p class=MsoNormal>sleep <span style='mso-spacerun:yes'> </span>[<span
    class=SpellE>sli:p</span>]<span style='mso-spacerun:yes'>  </span><b><span
    style='font-size:11.0pt'>vi</span></b><span style='mso-spacerun:yes'>  </span><i>a
    condition of body and mind�o:p></o:p></i></p>

Observe that the entry is represented as an HTML paragraph, using the
``<p>`` element, and that the part of speech appears inside a ``<span
style='font-size:11.0pt'>`` element.  The following program defines
the set of legal parts-of-speech ``legal_pos``.  Then it extracts all
11-point content from the ``dict.htm`` file and stores it in the set
``used_pos``.  Observe that the search pattern contains a
parenthesized sub-expression; only the material that matches this
sub-expression is returned by ``re.findall``.  Finally, the program
constructs the set of illegal parts-of-speech as ``used_pos -
legal_pos``:

    >>> import re
    >>> legal_pos = set(['n', 'v.t.', 'v.i.', 'adj', 'det'])
    >>> pattern = re.compile(r"'font-size:11.0pt'>([a-z.]+)<")
    >>> document = open("dict.htm").read()
    >>> used_pos = set(re.findall(pattern, document))
    >>> illegal_pos = used_pos.difference(legal_pos)
    >>> print list(illegal_pos)
    ['v.intr', 'v.i', 'intrans']

This simple program represents the tip of the iceberg.  We can develop
sophisticated tools to check the consistency of word processor files,
and report errors so that the maintainer of the dictionary can correct
the original file *using the original word processor*.  We can write
other programs to *convert* the data into a different format.  For
example, the following program extracts the words and their
pronunciations and generates output in "comma-separated value" (CSV) format:

    >>> import re
    >>> document = open("dict.htm").read()
    >>> document = re.sub("[\r\n]", "", document)
    >>> word_pattern = re.compile(r">([\w]+)")
    >>> pron_pattern = re.compile(r"\[.*>([a-z:]+)<.*\]")
    >>> for entry in document.split("<p"):
    ...     word_match = word_pattern.search(entry)
    ...     pron_match = pron_pattern.search(entry)
    ...     if word_match and pron_match:
    ...         lex = word_match.group(1)
    ...         pos = pron_match.group(1)
    ...         print "'%s','%s'" % (lex, pos)
    'sleep','sli:p'
    'walk','wo:k'
    'wake','weik'

**Spreadsheets.** These are often used for wordlists or paradigms.  A
comparative wordlist may be stored in a spreadsheet, with a row for
each cognate set, and a column for each language.  Examples are
available from ``www.rosettaproject.org``.  Programs such as Excel can
export spreadsheets in the CSV format, and we can write programs to
manipulate them, with the help of Python's ``csv`` module.  For
example, we may want to print out cognates having an edit-distance of
at least three from each other (i.e. 3 insertions, deletions, or
substitutions).

**Databases.** Sometimes lexicons are stored in a full-fledged
relational database.  When properly normalized, these databases can
implement many well-formedness constraints.  For example, we can
require that all parts-of-speech come from a specified vocabulary by
declaring that the part-of-speech field is an *enumerated type*.
However, the relational model is often too restrictive for linguistic
data, which typically has many optional and repeatable fields (e.g.
dictionary sense definitions and example sentences).
Query languages such as SQL cannot express many
linguistically-motivated queries, e.g. *find all words that appear
in example sentences for which no dictionary entry is provided*.
Now supposing that the database supports exporting data to CSV format,
we can express this query in the following program:

    >>> import csv
    >>> lexemes = []
    >>> defn_words = []
    >>> for row in csv.reader(open("dict.csv")):
    ...     lexeme, pron, pos, defn = row
    ...     lexemes.append(lexeme)
    ...     defn_words += defn.split()
    >>> undefined = list(set(defn_words).difference(set(lexemes)))
    >>> undefined.sort()
    >>> print undefined
    ['...', 'a', 'and', 'body', 'by', 'cease', 'condition', 'down', 'each', 'foot', 'lifting', 'mind', 'of', 'progress', 'setting', 'to']

-------------------------
Processing "Shoebox" Data
-------------------------

The most popular tool for managing linguistic field data is *Shoebox*.
Together with its more recent incarnation (*Toolbox*), Shoebox uses a
simple file format which we can easily read and write, permitting us
to apply computational methods to linguistic field data.  NLTK_Lite
includes a Shoebox dictionary of Rotokas, an East Papuan language
spoken on the island of Bougainville (provided by Stuart Robinson)::

  \lx kaa
  \ps N.M
  \cl isi
  \ge cooking banana
  \gp banana bilong kukim
  \sf FLORA
  \dt 12/Feb/2005
  \ex Taeavi iria kaa isi kovopaueva kaparapasia.
  \xp Taeavi i bin planim gaden banana bilong kukim tasol long paia.
  \xe Taeavi planted banana in order to cook it.

Each field consists of a field name (e.g. ``lx``, for lexeme), and a
value (e.g. ``kaa``).  Other fields are:
``ps`` part-of-speech;
``cl`` classifier;
``ge`` English gloss;
``gp`` Pidgin English gloss;
``sf`` Semantic field;
``dt`` Date last edited;
``ex`` Example sentence;
``xp`` Pidgin translation of example;
``xe`` English translation of example.

We can use the ``shoebox.raw()`` method to access a Shoebox file to
perform various operations that involve single-pass scanning of a
lexicon.  For example, here we compute the average number of fields
for each entry:

  >>> from nltk_lite.corpora import shoebox
  >>> sum_size = num_entries = 0
  >>> for entry in shoebox.raw('rotokas'):
  ...     num_entries += 1
  ...     sum_size += len(entry)
  >>> print sum_size/num_entries
  10

As we will see below, we can also use the ``raw()`` method in
functions that add or remove fields, or to examine sequences of fields.

The raw method reads each field into a list, preserving the order of fields.
Another version of the method reads an entry into a Python dictionary.

  >>> entries = list(shoebox.dictionary('rotokas'))
  >>> from pprint import pprint
  >>> pprint(entries[3])
  {'cl': 'isi',
   'dt': '12/Feb/2005',
   'ex': 'Taeavi iria kaa isi kovopaueva kaparapasia.',
   'ge': 'cooking banana',
   'gp': 'banana bilong kukim',
   'lx': 'kaa',
   'ps': 'N.M',
   'sf': 'FLORA',
   'xe': 'Taeavi planted banana in order to cook it.',
   'xp': 'Taeavi i bin planim gaden banana bilong kukim tasol long paia.'}

We can use this method to print a formatted version of a lexicon.
It allows us to request specific fields without needing to be
concerned with their relative ordering in an entry.

  >>> for entry in entries[70:80]:
  ...     lex = entry['lx']
  ...     pos = entry['ps']
  ...     dfn = entry['ge']
  ...     if entry.has_key('eng'):
  ...         dfn = entry['eng']
  ...     print "%s (%s) '%s'" % (lex, pos, dfn)
  kakapikoto (N.N2) 'newborn baby'
  kakapu (V.B) 'place in sling for purpose of carrying'
  kakapua (N.N) 'sling for lifting'
  kakara (N.N) 'bracelet'
  Kakarapaia (N.PN) 'village name'
  kakarau (N.F) 'stingray'
  Kakarera (N.PN) 'name'
  Kakareraia (N.???) 'name'
  kakata (N.F) 'cockatoo'
  kakate (N.F) 'bamboo tube for water'

Simple Entry Processing
-----------------------

The simplest approach to processing a Shoebox file is to scan it, one
entry at a time.  As we have seen, the ``shoebox.raw()`` method
returns entries, where each entry is a sequence of fields.

For example, we can extract all the values of a particular field into
a list, as follows:

  >>> lexemes = []
  >>> for entry in shoebox.raw('rotokas'):
  ...     for field in entry:
  ...         if field[0] == 'lx':
  ...             normalised_lexeme = field[1].lower()
  ...             lexemes.append(normalised_lexeme)

However, we can achieve the same behaviour more economically using
Python's *list comprehension* syntax, as follows:

  >>> lexemes = [field[1].lower() for entry in shoebox.raw('rotokas')
  ...	             for field in entry if field[0] == 'lx']

Note that we do not need to store the entire lexicon in memory.
Instead, we simply scan through it, and extract the information
we need.

Adding New Fields
~~~~~~~~~~~~~~~~~

It is often convenient to add new fields that are derived from
existing ones.  Such fields often facilitate analysis.

For example, let us define a function which maps a string of
consonants and vowels to the corresponding CV sequence,
e.g. ``kakapua`` would map to ``CVCVCVV``.

  >>> import re
  >>> def cv(s):
  ...     s = s.lower()
  ...     s = re.sub(r'[^a-z]',     r'-', s)
  ...     s = re.sub(r'[aeiou]',    r'V', s)
  ...     s = re.sub(r'[^V\-]',     r'C', s)
  ...     return (s)

This mapping has four steps.  First, the string is converted to lowercase,
then we replace any non-alphabetic characters ``[^a-z]`` with a dash.
Next, we replace all vowels with ``V``.  Finally, anything that is not
a ``V`` or a dash, must be a consonant, so we replace it with a ``C``.

Now, we can scan the lexicon and add a new cv field after every lx
field.  Here we will do it for a single entry only:

  >>> entries = list(shoebox.raw('rotokas'))
  >>> for field in entries[50]:
  ...     print "\\%s %s" % field
  ...	  if field[0] == "lx":
  ...	      print "\\cv %s" % cv(field[1])
  \lx kaeviro
  \cv CVVCVCV
  \ps V.A
  \ge lift off
  \ge take off
  \gp go antap
  \nt used to describe action of plane
  \dt 12/Feb/2005
  \ex Pita kaeviroroe kepa kekesia oa vuripierevo kiuvu.
  \xp Pita i go antap na lukim haus win i bagarapim.
  \xe Peter went to look at the house that the wind destroyed.


Removing Fields
~~~~~~~~~~~~~~~

Finally, we can also use this technique to remove fields, as follows:

  >>> retain = ('lx', 'ps')
  >>> entries = list(shoebox.raw('rotokas'))
  >>> for entry in entries[50:55]:
  ...     for field in entry:
  ...         if field[0] in retain:
  ...             print "\\%s %s" % field
  ...     print
  \lx kaeviro
  \ps V.A
  <BLANKLINE>
  \lx kagave
  \ps N.F
  <BLANKLINE>
  \lx kaie
  \ps V.A
  <BLANKLINE>
  \lx kaiea
  \ps N.N
  <BLANKLINE>
  \lx kaikaio
  \ps N.N
  <BLANKLINE>


Analysis
--------

In this section we consider a selection of analysis tasks.

First, we will develop a program to find reduplicated words.
For each lexeme ``lex``, we will check if the lexicon contains the
reduplicated form ``lex+lex``.  In order to do this we need to store
all lexemes in memory.  We will also keep track of the English
glosses, so they can be displayed alongside the wordforms.
Accordingly, we create a Python dictionary ``lexgloss`` which maps
lexemes to their English glosses:

  >>> lexgloss = {}
  >>> for entry in shoebox.dictionary('rotokas'):
  ...     if entry.has_key('lx') and entry['ps'][0] == 'V':
  ...         lexgloss[entry['lx']] = entry['ge']

Next, for each lexeme, check if the reduplicated form is present, and
if so, report the forms and their glosses.

  >>> for lex in lexgloss:
  ...     if lex+lex in lexgloss:
  ...         print "%s (%s); %s (%s)" % (lex, lexgloss[lex], lex+lex, lexgloss[lex+lex])
  kuvu (fill.up); kuvukuvu (stamp the ground)
  kitu (save); kitukitu (scrub clothes)
  kopa (ingest); kopakopa (gulp.down)
  kasi (burn); kasikasi (angry)
  koi (high pitched sound); koikoi (groan with pain)
  kee (chip); keekee (shattered)
  kauo (jump); kauokauo (jump up and down)
  kea (deceived); keakea (lie)
  kove (drop); kovekove (drip repeatedly)
  kape (unable to meet); kapekape (grip with arms not meeting)
  kapo (fasten.cover.strip); kapokapo (fasten.cover.strips)
  koa (skin); koakoa (remove the skin)
  kipu (paint); kipukipu (rub.on)
  koe (spoon out a solid); koekoe (spoon out)
  kovo (work); kovokovo (surround)
  kiru (have sore near mouth); kirukiru (crisp)
  kotu (bite); kotukotu (grind teeth together)
  kavo (collect); kavokavo (work black magic)
  kuri (scrape); kurikuri (scratch repeatedly)
  karu (unhook); karukaru (open)
  kare (return); karekare (return)
  kari (break); karikari (shred)
  kiro (write); kirokiro (write)
  kae (carry); kaekae (tempt)
  koru (make return); korukoru (obstruct)
  ku (finished with); kuku (spoonfeed)
  kosi (exit); kosikosi (exit)

Print trisyllabic verbs ending in a long vowel.  Make use of the fact
that syllable onsets are obligatory, and only consist of a single
consonant.  We use the ``cv`` function to get the CV template of the
lexeme, then count the number of Cs it contains.

  >>> for entry in shoebox.dictionary('rotokas'):
  ...     if entry.has_key('lx'):
  ...         lex = entry['lx']
  ...         cns = list(cv(lex)).count('C')
  ...         pos = entry['ps']
  ...         if lex[-1] in 'aeiou' and lex[-2] in 'aeiou' \
  ...           and cns == 3 and pos[0] == 'V':
  ...             dfn = entry['ge']
  ...             print "%s (%s) '%s'" % (lex, pos, dfn)
  kaetupie (V.B) 'tighten'
  kakupie (V.B) 'yodel'
  kapatau (V.B) 'add to'
  kapuapie (V.B) 'wound'
  kapupie (V.B) 'close tight'
  kapuupie (V.B) 'close'
  karepie (V.B) 'return'
  karivai (V.A) 'have an appetite'
  kasipie (V.B) 'care for'
  kaukaupie (V.B) 'intense sunlight'
  kavorou (V.A) 'intercept'
  kavupie (V.B) 'leave.behind'
  kekepie (V.B) 'show'
  keruria (V.A) 'determined'
  ketoopie (V.B) 'make sprout from seed'
  koatapie (V.B) 'accept'
  koetapie (V.B) 'satisfy curiosity'
  kokovae (V.A) 'sing'
  kokovua (V.B) 'shave the hair line'
  kopiipie (V.B) 'kill'
  korupie (V.B) 'take outside'
  kosipie (V.B) 'make exit'
  kovopie (V.B) 'use to make work'
  kukuvai (V.B) 'cover the head from rain or sun'
  kuvaupie (V.B) 'leave alone'
  kuverea (V.A) 'not all right'

Let's count up the various kinds of CV syllables.

  >>> from nltk_lite.probability import FreqDist
  >>> fd = FreqDist()
  >>> from nltk_lite.tokenize import regexp
  >>> for lex in lexemes:
  ...     for syl in regexp(lex, pattern=r'[^aeiou][aeiou]+'):
  ...         if len(syl) == 2:
  ...             fd.inc(syl)

Rather than just printing the syllables and their frequency counts, we
will tabulate them:

  >>> for vowel in 'aeiou':
  ...     for cons in 'ptkvsr':
  ...          print '%s%s:%4d ' % (cons, vowel, fd.count(cons+vowel)),
  ...     print
  pa:  70  ta:  31  ka: 298  va:  64  sa:   0  ra: 156 
  pe:  24  te:   7  ke: 108  ve:  17  se:   1  re:  47 
  pi:  42  ti:   0  ki:  69  vi:  95  si:  79  ri:  68 
  po:  25  to: 133  ko: 306  vo:  36  so:   3  ro:  73 
  pu:  30  tu:  29  ku: 129  vu:  35  su:   0  ru:  58 

Consider the ``t`` and ``s`` columns, and observe that ``ti`` is not
attested, while ``si`` is frequent.  This suggests that a phonological
process of palatalisation is operating in the language.  We would then
want to consider the other syllables involving ``s``.


Finding Minimal Pairs
~~~~~~~~~~~~~~~~~~~~~

First, select the length of the words of interest, the position
of the target segment in the word, and the minimum number of forms
we would like to see:

  >>> length = 4
  >>> position = 1
  >>> min = 3

Now, we iterate over all the lexemes, checking if they have the
required length:

  >>> from nltk_lite.utilities import MinimalSet
  >>> ms = MinimalSet()
  >>> for lex in lexemes:
  ...     if len(lex) == length:
  ...         context = lex[:position] + '_' + lex[position+1:]
  ...         target = lex[position]
  ...         ms.add(context, target, lex)

Finally, we print the table of minimal sets.  We specify that
each context was seen at least the minimum number of required times.

  >>> for context in ms.contexts(3):
  ...     for target in ms.targets():
  ...         print "%-4s" % ms.display(context, target, "-"),
  ...     print
  kasi -    kesi kusi kosi
  kava -    -    kuva kova
  karu kiru keru kuru koru
  kapu kipu -    -    kopu
  karo kiro -    -    koro
  kari kiri keri kuri kori
  kapa -    kepa -    kopa
  kara kira kera -    kora
  kaku -    -    kuku koku
  kaki kiki -    -    koki

Observe in the above example that the context, target, and displayed
material were all based on the lexeme field.  However, the idea of
minimal sets is much more general.  For instance, suppose we wanted to
get a list of wordforms having more than one possible part-of-speech.
Then the target will be part-of-speech field, and the context will be
the lexeme field.  We will also display the English gloss field.

  >>> ms = MinimalSet()
  >>> for entry in shoebox.dictionary('rotokas'):
  ...     if entry.has_key('lx'):
  ...         display = "%s (%s)" % (entry['ps'], entry['ge'])
  ...         ms.add(entry['lx'], entry['ps'], display)
  >>> for context in ms.contexts()[:10]:
  ...     print "%s:" % context, "; ".join(ms.display_all(context))
  kokovara: N.N (unripe coconut); V.A (unripe)
  kapua: N.N (sore); V.A (have sores)
  kosikosi: V.B (cut off sago palm leaves); V.A (exit)
  koie: V.??? (get pig to eat); N.??? (pig)
  kovo: N.N (work); V.B (work); CLASS (garden)
  kavori: V.B (collect crayfish or lobster); N.??? (lobster)
  kororo: N.N (sphere); N.??? (bird)
  korita: V.B (cut up meat); N.??? (cutlet?)
  keru: N.N (bone); V.A (harden like bone)
  kirokiro: V.B (write); N.F (bush used for sorcery)


Analysing Texts
~~~~~~~~~~~~~~~

Use the shoebox lexicon to tag a text with part-of-speech information,
then do POS-sensitive concordance-style searches.


Example Applications: Improving Access to Lexical Resources
-----------------------------------------------------------

A lexicon constructed as part of field-based research is a potential
*language resource* for speakers of a language.  Even when the
language in question has a standard writing system, many speakers
will not be literate in the language.  They may be able to attempt
an approximate spelling for a word, or they may prefer to access the
dictionary via an index which uses the language of wider
communication.  In this section we deal with the first of these.
The second is left to the reader as an exercise.  We will also
generate a wordfinder puzzle which can be used to test knowledge
of lexical items.

Fuzzy Spelling
~~~~~~~~~~~~~~

Confusible sets:

  >>> group = {
  ...     ' ':0,                     # blank (for short words)
  ...     'p':1,  'b':1,  'v':1,     # labials
  ...     't':2,  'd':2,  's':2,     # alveolars
  ...     'l':3,  'r':3,             # sonorant consonants
  ...     'i':4,  'e':4,             # high front vowels
  ...     'u':5,  'o':5,             # high back vowels
  ...     'a':6                      # low vowels
  ... }

Soundex: idea of a signature.  Words with the same signature
considered confusible.  Consider first letter of a word to be so
cognitively salient that people will not get it wrong.

  >>> def soundex(word):
  ...     if len(word) == 0: return word  # sanity check
  ...     word += '    '                  # ensure word long enough
  ...     c0 = word[0].upper()
  ...     c1 = group[word[1]]
  ...     cons = filter(lambda x: x in 'pbvtdslr ', word[2:])
  ...     c2 = group[cons[0]]
  ...     c3 = group[cons[1]]
  ...     return "%s%d%d%d" % (c0, c1, c2, c3)
  >>> print soundex('kalosavi')
  K632
  >>> print soundex('ti')
  T400
  
Now we can build a soundex index of the lexicon:

  >>> soundex_idx = {}
  >>> for lex in lexemes:
  ...     code = soundex(lex)
  ...     if code not in soundex_idx:
  ...         soundex_idx[code] = set()
  ...     soundex_idx[code].add(lex)

We should sort these candidates by proximity with the target word.

  >>> from nltk_lite.utilities import edit_dist
  >>> def fuzzy_spell(target):
  ...     scored_candidates = []
  ...     code = soundex(target)
  ...     for word in soundex_idx[code]:
  ...         dist = edit_dist(word, target)
  ...         scored_candidates.append((dist, word))
  ...     scored_candidates.sort()
  ...     return [w for (d,w) in scored_candidates[:10]]

Finally, we can look up a word to get approximate matches:

  >>> fuzzy_spell('kokopouto')
  ['kokopeoto', 'kokopuoto', 'kokepato', 'koovoto', 'koepato', 'kooupato', 'kopato', 'kopiito', 'kovuto', 'koavaato']
  >>> fuzzy_spell('kogou')
  ['kogo', 'koou', 'kokeu', 'koko', 'kokoa', 'kokoi', 'kokoo', 'koku', 'kooe', 'kooku']


Wordfinder Puzzle
~~~~~~~~~~~~~~~~~

Here we will generate a grid of letters, containing words found in the
dictionary.  First we remove any duplicates and disregard the order in
which the lexemes appeared in the dictionary.  We do this by converting
it to a set, then back to a list.  Then we select the first 200 words,
and then only keep those words having a reasonable length.

  >>> words = list(set(lexemes))
  >>> words = words[:200]
  >>> words = [w for w in words if 3 <= len(w) <= 12]

Now we generate the wordfinder grid, and print it out.

  >>> from nltk_lite.misc.wordfinder import wordfinder
  >>> grid, used = wordfinder(words)
  >>> for i in range(len(grid)):
  ...     for j in range(len(grid[i])):
  ...         print grid[i][j],
  ...     print
  O G H K U U V U V K U O R O V A K U N C
  K Z O T O I S E K S N A I E R E P A K C
  I A R A A K I O Y O V R S K A W J K U Y
  L R N H N K R G V U K G I A U D J K V N
  I I Y E A U N O K O O U K T R K Z A E L
  A V U K O X V K E R V T I A A E R K R K
  A U I U G O K U T X U I K N V V L I E O
  R R K O K N U A J Z T K A K O O S U T R
  I A U A U A S P V F O R O O K I C A O U
  V K R R T U I V A O A U K V V S L P E K
  A I O A I A K R S V K U S A A I X I K O
  P S V I K R O E O A R E R S E T R O J X
  O I I S U A G K R O R E R I T A I Y O A
  R R R A T O O K O I K I W A K E A A R O
  O E A K I K V O P I K H V O K K G I K T
  K K L A K A A R M U G E P A U A V Q A I
  O O O U K N X O G K G A R E A A P O O R
  K V V P U J E T Z P K B E I E T K U R A
  N E O A V A E O R U K B V K S Q A V U E
  C E K K U K I K I R A E K O J I Q K K K

Finally we generate the words which need to be found.

  >>> for i in range(len(used)):
  ...     print "%-12s" % used[i],
  ...     if float(i+1)%5 == 0: print
  KASIARAO     KERIKERISI   KARUVIRA     KOGO         KOROTO      
  KOVASI       KOKOVARA     KUVU         KAPOKARI     KOKOROPAVIRA
  KEAVIRA      KUROVIRA     KUVERETO     KATA         KUVA        
  KOKARAPATO   KAREKOVA     KAVOROU      KUSI         KIKISI      
  KOARAUA      KOVOVO       KOKORUUTO    KAPUA        KORUKO      
  KERISI       KERAVISI     KITUKITU     KAPUU        KOVUTO      
  KOIE         KOGOVA       KAROTO       KUUVU        KAVOVOA     
  KESIOTO      KAARE        KAARA        KUROEA       KORUOTO     
  KITU         KUROO        KUROA        KOKEE        KUI         
  KUE          KEOPA        KOVE         KOVO         KIUVE       
  KAUO         KILIA        KAAKASI      KEROROI      KOSI        
  KAKUA        KIRIOTO      KAITA

Generating Reports
------------------


Discovering Entry Patterns
~~~~~~~~~~~~~~~~~~~~~~~~~~

Print most frequent fields

  >>> fd = FreqDist()
  >>> for entry in shoebox.raw('rotokas'):
  ...     for field in entry:
  ...	      fd.inc(field[0])
  >>> fd.sorted_samples()[:10]
  ['ge', 'ex', 'xe', 'xp', 'gp', 'lx', 'ps', 'dt', 'rt', 'eng']


Discovering patterns of fields:

  >>> fd = FreqDist()
  >>> for entry in shoebox.raw('rotokas'):
  ...     marker_list = [field[0] for field in entry]
  ...     markers = ':'.join(marker_list)
  ...	  fd.inc(markers)
  >>> top_ten = fd.sorted_samples()[:10]
  >>> print '\n'.join(top_ten)
  lx:rt:ps:ge:gp:dt:ex:xp:xe
  lx:ps:ge:gp:dt:ex:xp:xe
  lx:ps:ge:gp:dt:ex:xp:xe:ex:xp:xe
  lx:rt:ps:ge:gp:dt:ex:xp:xe:ex:xp:xe
  lx:ps:ge:gp:nt:dt:ex:xp:xe
  lx:ps:ge:gp:dt
  lx:ps:ge:ge:gp:dt:ex:xp:xe:ex:xp:xe
  lx:rt:ps:ge:ge:gp:dt:ex:xp:xe:ex:xp:xe
  lx:ps:ge:ge:gp:dt:ex:xp:xe
  lx:rt:ps:ge:ge:gp:dt:ex:xp:xe


.. Parsing Entries?
   Some shoebox entries have nested structure.  Thus they correspond to a
   tree over the fields.

Looking at Timestamps
~~~~~~~~~~~~~~~~~~~~~

  >>> fd = FreqDist()
  >>> from string import split
  >>> for entry in shoebox.dictionary('rotokas'):
  ...     if 'dt' in entry:
  ...         (day, month, year) = split(entry['dt'], '/')
  ...         fd.inc((month, year))
  >>> for time in fd.sorted_samples():
  ...     print time[0], '/', time[1], ':', fd.count(time)
  Feb / 2005 : 307
  Dec / 2004 : 151
  Jan / 2005 : 123
  Feb / 2004 : 64
  Sep / 2004 : 49
  May / 2005 : 46
  Mar / 2005 : 37
  Apr / 2005 : 29
  Jul / 2004 : 14
  Nov / 2004 : 5
  Oct / 2004 : 5
  Aug / 2004 : 4
  May / 2003 : 2
  Jan / 2004 : 1
  May / 2004 : 1

To put these in time order, we need to set up a special comparison function.
Otherwise, if we just sort the months, we'll get them in alphabetical order.

  >>> month_index = {
  ...     "Jan" : 1, "Feb" : 2,  "Mar" : 3,  "Apr" : 4,
  ...     "May" : 5, "Jun" : 6,  "Jul" : 7,  "Aug" : 8,
  ...     "Sep" : 9, "Oct" : 10, "Nov" : 11, "Dec" : 12
  ... }
  >>> def time_cmp(a, b):
  ...     a2 = a[1], month_index[a[0]]
  ...     b2 = b[1], month_index[b[0]]
  ...     return cmp(a2, b2)

The comparison function says that we compare two times of the
form ``('Mar', '2004')`` by reversing the order of the month and
year, and converting the month into a number to get ``('2004', '3')``,
then using Python's built-in ``cmp`` function to compare them.

Now we can get the times found in the Shoebox entries, sort them
according to our ``time_cmp`` comparison function, and then print them
in order.  This time we print bars to indicate frequency:

  >>> times = fd.samples()
  >>> times.sort(cmp=time_cmp)
  >>> for time in times:
  ...     print time[0], '/', time[1], ':', '#' * (1 + fd.count(time)/10)
  May / 2003 : #
  Jan / 2004 : #
  Feb / 2004 : #######
  May / 2004 : #
  Jul / 2004 : ##
  Aug / 2004 : #
  Sep / 2004 : #####
  Oct / 2004 : #
  Nov / 2004 : #
  Dec / 2004 : ################
  Jan / 2005 : #############
  Feb / 2005 : ###############################
  Mar / 2005 : ####
  Apr / 2005 : ###
  May / 2005 : #####


-----------------
Language Archives
-----------------

*This section will describe OLAC, the Open Language Archives Community.*


Managing Metadata for Language Resources
----------------------------------------

*This section will discuss how to manipulate OLAC data*



---------
Exercises
---------

1. Write a program that scans an HTML dictionary file to find entries
   having an illegal part-of-speech field, and reports the *headword* for
   each entry.

#. Obtain a comparative wordlist in CSV format, and write a program
   that prints those cognates having an edit-distance of at least three
   from each other.

#. Write a program to filter out just the date field (``dt``) without
   having to list the fields we wanted to retain.

#. Print an index of a lexicon.  For each lexical entry, construct a
   tuple of the form ``(gloss, lexeme)``, then sort and print them all.

#. Write a program to find any parts of speech (``ps`` field) that
   occurred less than ten times.  Perhaps these are typing mistakes?

#. We saw a method for discovering cases of whole-word reduplication.
   Write a function to find words that may contain partial
   reduplication.  Use the ``re.search()`` method, and the following
   regular expression: ``(..+)\1``

#. What is the frequency of each consonant and vowel contained in
   lexeme fields?

#. We saw a method for adding a ``cv`` field.  There is an interesting
   issue with keeping this up-to-date when someone modifies the content
   of the ``lx`` field on which it is based.  Write a version of this
   program to add a ``cv`` field, replacing any existing ``cv`` field.

#. Build an index of those lexemes which appear in example sentences.
   Suppose the lexeme for a given entry is *w*.
   Then add a single cross-reference field ``xrf`` to this entry, referencing
   the headwords of other entries having example sentences containing
   $w$.  Do this for all entries and save the result as a shoebox-format file.
  
#. Write a program to add a new field ``syl`` which gives a count of
   the number of syllables in the word.

#. Write a function which displays the complete entry for a lexeme.
   When the lexeme is incorrectly spelled it should display the entry
   for the most similarly spelled lexeme.
 
#. How many entries were last modified in 2004?



----

NLTK_

.. _NLTK: http://nltk.sourceforge.net/


.. To add: paradigms
