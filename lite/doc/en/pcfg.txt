.. -*- mode: rst -*-
.. include:: ../definitions.txt

=====================
Probabilistic Parsing
=====================

:Authors: Edward Loper, Steven Bird
:Contact: sb@csse.unimelb.edu.au
:Version: |version|
:Revision: $Revision$
:Date: $Date$
:Copyright: |copy| 2001-2005 University of Pennsylvania
:License: Creative Commons Attribution-NonCommercial-ShareAlike License

.. Note:: This is a draft.  Please send any feedback to the authors.


------------
Introduction
------------

Parsing allows us to find tree structures representing the internal
organization of a text.  These trees are useful for a wide variety of
tasks, including semantic interpretation, information retrieval, and
machine translation.  Unfortunately, most texts have a large number of
possible structures.  This causes two problems for the symbolic
approaches discussed in the earlier chapter on parsing.

1. **Ambiguity**: There is no way to decide which of the tree
   structures are likely to correctly represent the text's internal
   organization.

2. **Efficiency**: Parsing a text requires searching a very large
   space of possible tree structures.  With no information about which
   subtrees are more likely to be included in a complete parse, it can
   take a long time to find even a single parse.

Probabilistic techniques provide tools to address both of these
problems.  We can assign probabilities to parses, and use them to
decide which structures are more likely to represent a text's internal
organization; and we can use probabilities to guide our search of the
space of possible tree structures.

-----------------------------------
Probabilistic Context Free Grammars
-----------------------------------
 
A *probabilistic context free grammar* (or *PCFG*) is a context free
grammar that associates a probability with each of its productions.
It generates the same set of parses for a text that the corresponding
context free grammar does, and assigns a probability to each parse.
The probability of a parse generated by a PCFG is simply the product
of the probabilities of the productions used to generate it.

Probabilistic context free grammars are implemented by the
``nltk_lite.parse.pcfg.Grammar`` class.  Like CFGs, each PCFG consists
of a start state and a list of productions.  But the productions are
represented by ``pcfg.Production``, a subclass of ``cfg.Production``
that associates a probability with a context free grammar
production.

PCFG Productions
----------------
      
Each PCFG production specifies that a nonterminal (the *left-hand
side*) can be expanded to a sequence of terminals and nonterminals
(the *right-hand side*).  In addition, each production has a
probability associated with it.  Productions are created using the
``nltk_lite.parse.pcfg.Production`` constructor, which takes a
probability, a nonterminal left-hand side, and zero or more terminals
and nonterminals for the right-hand side.

  >>> from nltk_lite.parse import cfg
  >>> S, VP, V, NP = cfg.nonterminals('S, VP, V, NP')

  >>> from nltk_lite.parse import pcfg
  >>> prod1 = pcfg.Production(VP, [V, NP], prob=0.23)
  >>> prod1
  VP -> V NP (p=0.23)

  >>> prod2 = PCFGProduction(V, ['saw'], prob=0.12)
  >> prod2
  V -> 'saw' (p=0.12)

  >>> prod3 = PCFGProduction(NP, ['cookie'], prob=0.04)
  >>> prod3
  NP -> 'cookie' (p=0.04)

The probability associated with a production is returned by the
``prob`` method:

  >>> print prod1.prob(), prod2.prob(), prod3.prob() 
  0.23 0.12 0.04

As with CFG productions, the left-hand side of a PCFG production is
returned by the ``lhs`` method; and the right-hand side is returned by
the ``rhs`` method:

  >>> prod1.lhs() 
  <VP>
  >>> prod1.rhs() 
  (<V>, <NP>)


PCFGs
-----

PCFGs are created using the ``pcfg.Grammar`` constructor, which takes
a start symbol and a list of productions:

  >>> prods = [pcfg.Production(S, [VP, NP], prob=1.0),
  ...          pcfg.Production(VP, ['saw', NP], prob=0.4),
  ...          pcfg.Production(VP, ['ate'], prob=0.3),
  ...          pcfg.Production(VP, ['gave', NP, NP], prob=0.3),
  ...          pcfg.Production(NP, ['the', 'cookie'], prob=0.8),
  ...          pcfg.Production(NP, ['Jack'], prob=0.2)]

  >>> grammar = pcfg.Grammar(S, prods) 
  >>> print grammar
  CFG with 6 productions (start state = S)
    S -> VP NP (p=1)
    VP -> 'saw' NP (p=0.4)
    VP -> 'ate' (p=0.3)
    VP -> 'gave' NP NP (p=0.3)
    NP -> 'the' 'cookie' (p=0.8)
    NP -> 'Jack' (p=0.2)

In order to ensure that the trees generated by the grammar form a
proper probability distribution, PCFG grammars impose the constraint
that all productions with a given left-hand side must have
probabilities that sum to one:

  for all *lhs*:
    SIGMA :subscript:rhs P(*lhs* |rarr| *rhs*) = 1

The example grammar given above obeys this constraint: for ``S``,
there is only one production, with a probability of 1.0; for ``VP``,
0.4+0.3+0.3=1.0; and for ``NP``, 0.8+0.2=1.0.

As with CFGs, the start state of a PCFG is returned by the ``start``
method; and the productions are returned by the ``productions``
method:

  >>> grammar.start()
  <S>
  >>> grammar.productions()
  [[Production: S -> VP NP (p=1)], 
   [Production: VP -> 'saw' NP (p=0.4)],
   [Production: VP -> 'ate' (p=0.4)],
   [Production: VP -> VP PP (p=0.2)],
   [Production: NP -> 'the' 'boy' (p=0.8)],
   [Production: NP -> 'Jack' (p=0.2)],
   [Production: PP -> 'under' NP (p=1.0)]]

---------------------
Probabilistic Parsers
---------------------

The Probabilistic Parser Interface
----------------------------------

``ProbabilisticParserI`` defines a standard interface for
probabilistic parsers.  It extends the ``ParseI`` interface in two
ways.  First, the parse trees returned by ``parse`` and ``parse_n``
must define the ``PROB`` property, specifying each tree's probability.

  >>> from nltk_lite.parse import ViterbiParse
  >>> from nltk_lite.tokenize import whitespace
  >>> parser = ViterbiParser(grammar)
  >>> sent = tokenize.whitespace('John baked some cookies')
  >>> tree = parser.get_parse(sent)
  (S: (NP: <John>)
      (VP: (V: <baked>) (NP: (Det: <some>) (N: <cookies>))
  >>> print tree.prob()
  0.000282

Second, ``ProbabilisticParser``s are required to implement the
``parse_dist`` method, which returns a ``ProbDist`` with ``TreeToken``
samples.

  >>> pdist = pcfg_parser.parse_dist(words)
  <ProbDist with 1 sample>
  >>> pdist.prob(pdist.samples()[0])
  (S: (NP: <John>)
      (VP: (V: <baked>) (NP: (Det: <some>) (N: <cookies>))
  >>> pdist.prob(pdist.samples()[0]['PROB'])
  0.000282

Note that the probabilities in the distribution returned by
``parse_dist`` are <emphasis>not</emphasis> required to sum to 1.
      
Probabilistic Parser Implementations
------------------------------------

The next two sections introduce two probabilistic parsing algorithms
for PCFGs.  The first is a Viterbi-style algorithm that uses dynamic
programming to find the single most likely parse for a given text.
Whenever it finds multiple possible parses for a subtree, it discards
all but the most likely parse.  The second is a bottom-up chart parser
that maintains a queue of edges, and adds them to the chart one at a
time.  The ordering of this queue is based on the probabilities
associated with the edges, allowing the parser to expand more likely
edges before less likely ones.  Different queue orderings are used to
implement a variety of different search strategies.  These
algorithms are implemented in the ``nltk_lite.parse.viterbi`` and
``nltk_lite.parse.pchart`` modules.

A Viterbi-Style PCFG Parser
---------------------------

The ``ViterbiParse`` PCFG parser is a bottom-up parser that uses
dynamic programming to find the single most likely parse for a text.
It parses texts by iteratively filling in a *most likely constituents
table*.  This table records the most likely tree structure for each
span and node value.  In particular, it has an entry for every start
index, end index, and node value, recording the most likely subtree
that spans from the start index to the end index, and has the given
node value.  For example, after parsing the sentence "I saw John with
my cookie" with a simple grammar, the most likely constituents table
might be as follows:

===== ==== ==================================================================  =======
Most Likely Constituents Table
--------------------------------------------------------------------------------------
Span  Node Tree                                                                Prob
===== ==== ==================================================================  =======
[0:1] NP   (NP: I)                                                             0.3
[2:3] NP   (NP: John)                                                          0.3
[4:6] NP   (NP: my cookie)                                                     0.2
[3:6] PP   (PP: with (NP: my cookie))                                          0.1
[2:6] NP   (NP: (NP: John) (PP: with (NP: my cookie)))                         0.01
[1:3] VP   (VP: saw (NP: John)))                                               0.03
[1:6] VP   (VP: saw (NP: (NP: John) (PP: with (NP: my cookie))))               0.001
[0:6] S    (S: (NP: I) (VP: saw (NP: (NP: John) (PP: with (NP: my cookie)))))  0.0001
===== ==== ==================================================================  =======

Once the table has been completely filled in, the parser simply returns the entry for
the most likely constituent that spans the entire text, and whose node value is the
start symbol.  For this example, it would return the entry with a span of [0:6] and a
node value of "S".

Note that we only record the *most likely* constituent for any given span and node
value.  For example, in the table above, there are actually two possible constituents
that cover the span [1:6] and have "VP" node values.
    
1. "saw John, who has my cookie"::

  (VP: saw
     (NP: (NP: *John*) 
          (PP: *with* 
          (NP: *my* *cookie*))))

2. "used my cookie to see John"::

  (VP: (VP: *saw* 
           (NP: (NP: *John*))
           (PP: *with* 
              (NP: *my* *cookie*))))

Since the grammar we are using to parse the text indicates that the first of these
tree structures has a higher probability, the parser discards the second one.

Filling in the Most Likely Constituents Table
*********************************************

Because the grammar used by ``ViterbiParse`` is a PCFG, the probability of each
constituent can be calculated from the probabilities of its children.  Since a
constituent's children can never cover a larger span than the constituent itself,
each entry of the most likely constituents table depends only on entries for
constituents with *shorter* spans (or equal spans, in the case of unary and epsilon
productions).

``ViterbiParse`` takes advantage of this fact, and fills in the most likely
constituent table incrementally.  It starts by filling in the entries for all
constituents that span a single element of text.  After it has filled in all the
table entries for constituents that span one element of text, it fills in the entries
for constituents that span two elements of text.  It continues filling in the entries
for constituents spanning larger and larger portions of the text, until the entire
table has been filled.

To find the most likely constituent with a given span and node value,
``ViterbiParse`` considers all productions that could produce that node value.  For
each production, it checks the most likely constituents table for sequences of
children that collectively cover the span and that have the node values specified by
the production's right hand side.  If the tree formed by applying the production to
the children has a higher probability than the current table entry, then it updates
the most likely constituents table with the new tree.

Handling Unary Productions and Epsilon Productions
**************************************************

A minor difficulty is introduced by unary productions and epsilon productions: an
entry of the most likely constituents table might depend on another entry with the
same span.  For example, if the grammar contains the production ``V`` |rarr| ``VP``,
then the table entries for ``VP`` depend on the entries for ``V`` with the same span.
This can be a problem if the constituents are checked in the wrong order.  For
example, if the parser tries to find the most likely constituent for a ``VP``
spanning [1:3] before it finds the most likely constituents for ``V`` spanning [1:3],
then it can't apply the ``V`` |rarr| VP`` production.
      
To solve this problem, ``ViterbiParse`` repeatedly checks each span until it finds no
new table entries.  Note that cyclic grammar productions (e.g. ``V`` |rarr| ``V``)
will *not* cause this procedure to enter an infinite loop.  Since all production
probabilities are less than or equal to 1, any constituent generated by a cycle in
the grammar will have a probability that is less than or equal to the original
constituent; so ``ViterbiParse`` will discard it.

Using ``ViterbiParse``
----------------------

Viterbi parsers are created using the ``ViterbiParse`` constructor:

  >>> from nltk_lite.parse.viterbi import *
  >>> ViterbiParse(grammar)
  <ViterbiParse for <CFG with 16 productions>>

``ViterbiParse`` implements all of the methods defined by the
``ProbabilisticParserI`` interface.  Note, however, that since ``ViterbiParse`` only
finds the single most likely parse, that ``parse_n`` and ``parse_dist`` will never
return more than one parse.

  >>> viterbi = ViterbiParse(grammar)
  >>> sent1 = list(tokenize.whitespace('the dog ate my cookie'))
  >>> viterbi.parse(sent1)
  (S:
    (NP: (Det: <the>) (N: <dog>))
    (VP: (V: <ate>) (NP: (Det: <my>) 
                               (N: <cookie>)))) 0.00175
      
  >>> sent2 = list(tokenize.whitespace('I saw John with my cookie'))
  >>> viterbi.parse_n(sent2)
  5.2040625e-05      
  (S:
    (NP: <I>)
    (VP:
      (V: <saw>)
      (NP:
        (NP: <John>)
        (PP:
          (P: <with>)
          (NP: (Det: <my>) (N: <cookie>))))))
  3.82192874e-08
  (S:
    (NP: <I>)
    (VP:
      (VP:
        (V: <saw>)
        (NP: <John>))
      (PP:
        (P: <with>)
        (NP: (Det: <my>) (N: <cookie>)))))

The ``trace`` method can be used to set the level of tracing output that is generated
when parsing a text.  Trace output displays the constituents that are considered, and
indicates which ones are added to the most likely constituent table.  It also
indicates the likelihood for each constituent.

  >>> viterbi.trace(3)
  >>> viterbi.parse(sent2)

  Inserting tokens into the most likely constituents table...
     Insert: |[=] . . . . .| I
     Insert: |. [=] . . . .| saw
     Insert: |. . [=] . . .| John
     Insert: |. . . [=] . .| with
     Insert: |. . . . [=] .| my
     Insert: |. . . . . [=]| cookie
  Finding the most likely constituents spanning 1 text elements...
     Insert: |[=] . . . . .| NP -> 'I' (p=0.15)       0.1500000
     Insert: |. [=] . . . .| V -> 'saw' (p=0.65)      0.6500000
     Insert: |. [=] . . . .| VP -> V (p=0.1)          0.0650000
     Insert: |. . [=] . . .| NP -> 'John' (p=0.1)     0.1000000
     Insert: |. . . [=] . .| P -> 'with' (p=0.61)     0.6100000
     Insert: |. . . . [=] .| Det -> 'my' (p=0.2)      0.2000000
     Insert: |. . . . . [=]| N -> 'cookie' (p=0.5)    0.5000000
  Finding the most likely constituents spanning 2 text elements...
     Insert: |[=|=] . . . .| S -> NP VP (p=1.0)       0.0097500
     Insert: |. [=|=] . . .| VP -> V NP (p=0.5)       0.0325000
     Insert: |. . . . [=|=]| NP -> Det N (p=0.5)      0.0500000
  Finding the most likely constituents spanning 3 text elements...
     Insert: |[=|===] . . .| S -> NP VP (p=1.0)       0.0048750
     Insert: |. . . [=|===]| PP -> P NP (p=1.0)       0.0305000
  Finding the most likely constituents spanning 4 text elements...
     Insert: |. . [=|=====]| NP -> NP PP (p=0.25)     0.0007625
  Finding the most likely constituents spanning 5 text elements...
     Insert: |. [===|=====]| VP -> VP PP (p=0.4)      0.0003965
    Discard: |. [=|=======]| VP -> V NP (p=0.5)       0.0002478
    Discard: |. [=|=======]| VP -> V NP (p=0.5)       0.0002478
  Finding the most likely constituents spanning 6 text elements...
     Insert: |[=|=========]| S -> NP VP (p=1.0)       0.0000594

The level of tracing output can also be set with an optional argument to
the ``ViterbiParse`` constructor.  By default, no tracing output is generated.
Tracing output can be turned off by calling ``trace`` with a value of ``0``.

-----------------------------
A Bottom-Up PCFG Chart Parser
-----------------------------

Introduction
------------

The Viterbi-style algorithm described in the previous section finds the single most
likely parse for a given text.  But for many applications, it is useful to produce
several alternative parses.  This is often the case when probabilistic parsers are
combined with other probabilistic systems.  In particular, the most probable parse
may be assigned a low probability by other systems; and a parse that is given a low
probability by the parser might have a better overall probability.

For example, a probabilistic parser might decide that the most likely parse for "I
saw John with the cookie" is is the structure with the interpretation "I used my
cookie to see John"; but that parse would be assigned a low probability by a semantic
system.  Combining the probability estimates from the parser and the semantic system,
the parse with the interpretation "I saw John, who had my cookie" would be given a
higher overall probability.

This section describes ``BottomUpChartParser``, a parser for PCFGs that can find
multiple parses for a text.  It assumes that you have already read the chart parsing
tutorial, and are familiar with the data structures and productions used for chart
parsing.

The Basic Algorithm
-------------------

``BottomUpChartParser`` is a bottom-up parser for ``PCFG``s that uses a ``Chart`` to
record partial results.  It maintains a queue of ``Edge``\ s, and adds them to the
chart one at a time.  The ordering of this queue is based on the probabilities
associated with the edges, allowing the parser to insert more likely edges before
exploring less likely ones.  For each edge that the parser adds to the chart, it may
become possible to insert new edges into the chart; these are added to the queue.
``BottomUpChartParser`` continues adding the edges in the queue to the chart
until enough complete parses have been found, or until the edge queue is empty.

Probabilistic Edges
-------------------

An ``Edge`` associates a dotted production and a location with a (partial) parse
tree.  A *probabilistic edge* can be formed by using a
``ProbabilisticTree`` to encode an edge's parse tree.  The probability of
this tree is the product of the probability of the
production that generated it and the probabilities of its children.  For example, the
probability associated with an edge ``[Edge: S`` |rarr| ``NP`` |dot| ``VP]@[0:2]`` is the
probability of its NP child times the probability of the PCFG production
``S`` |rarr| ``NP VP``.  Note that an edge's tree token only includes
children for elements to the left of the edge's dot.  Thus, the edge's probability
does *not* include any probabilities for the elements to the right of the edge's dot.

The Edge Queue
--------------

The edge queue is a sorted list of edges that can be added to the chart.  It is
initialized with a single edge for each token in the text.  These *token edges* have
the form [Edge: token |rarr| |dot|] where *token* is the word.

As each edge from the queue is added to the chart, it may become possible to insert
new edges into the chart; these new edges are added to the queue.  There are two ways
that it can become possible to insert new edges into the chart:

1. The *bottom-up initialization production* can be used to add a self-loop edge
   whenever an edge whose dot is in position 0 is added to the chart.

2. The *fundamental production* can be used to combine a new edge with edges already
   present in the chart.

The edge queue is implemented using a ``list``.  For efficiency reasons,
``BottomUpChartParser`` uses ``pop`` to remove edges from the queue.  Thus, the
front of the queue is the *end* of the list.  This needs to be kept in mind when
implementing sorting orders for the queue: edges that should be tried first should be
placed at the end of the list.

Sorting The Edge Queue
----------------------

By changing the sorting order used by the queue, we can control the strategy that the
parser uses to search for parses of a text.  Since there are a wide variety of
reasonable search strategies, ``BottomUpChartParser`` does not define the sorting
order for the queue.  Instead, ``BottomUpPCFGChartParser`` is defined as an abstract
class; and subclasses are used to implement a variety of different queue orderings.
Each subclass is required to define the ``sort_queue`` method, which sorts a given
queue.  The remainder of this section describes four different subclasses of
``BottomUpChartParser`` that are defined in the ``nltk_lite.parse.pchart`` module.

InsidePCFGParse
***************

.. We should either explain "inside probabilities" or rename this parser (to
        ``LowestCostFirstPCFGParser``?). 

The simplest way to order the queue is to sort the edges by the probabilities of
their tree tokens.  This ordering concentrates the efforts of the parser on edges
that are more likely to be correct descriptions of the texts that they span.  This
approach is implemented by the ``InsidePCFGParse`` class.

The probability of an edge's tree token provides an upper bound on the probability of
any parse produced using that edge.  The probabilistic "cost" of using an edge to
form a parse is one minus its tree token's probability.  Thus, inserting the edges
with the most likely tree tokens first results in a *lowest-cost-first* search
strategy.  Lowest-cost-first search is an *optimal* search strategy: the first
solution it finds is guaranteed to be the best solution.

However, lowest-cost-first search can be rather inefficient.  Since a tree's
probability is the product of the probabilities of all the productions used to
generate it, smaller trees tend to have higher probabilities than larger ones.  Thus,
lowest-cost-first search tends to insert edges with small tree tokens before moving
on to edges with larger ones.  But any complete parse of the text will necessarily
have a large tree token; so complete parses will tend to be inserted after nearly all
other edges.

The basic problem with lowest-cost-first search is that it ignores the probability
that an edge's tree is part of a complete parse.  It will try parses that are locally
coherent, even if they are unlikely to form part of a complete parse.  Unfortunately,
it can be quite difficult to calculate the probability that a tree is part of a
complete parse.  However, we can use a variety of techniques to approximate that
probability.

Since ``InsideParse`` is a subclass of ``BottomUpChartParse``, it only
needs to define a ``sort_queue`` method.  Thus, the implementation of
``InsideParse`` class is quite simple::

  class InsidePCFGParser(BottomUpChartParse):
    def sort_queue(self, queue, chart):
      # Sort the edges by the probabilities of their tree tokens.
      queue.sort(lambda e1,e2:cmp(e1.tree().prob(), e2.tree().prob()))

LongestParse
************

``LongestParse`` sorts its queue in descending order of the edges' lengths.  These
lengths (properly normalized) provide a crude approximations to the probabilities
that trees are part of complete parses.  Thus, ``LongestParse`` employs a
*best-first* search strategy, where it inserts the edges that are closest to
producing complete parses before trying any other edges.  Best-first search is *not*
an optimal search strategy: the first solution it finds is not guaranteed to be the
best solution.  However, it will usually find a complete parse much more quickly than
lowest-cost-first search.

Since ``LongestParse`` is a subclass of ``BottomUpChartParse``, its implementation
simply defines a ``sort_queue`` method::

  class LongestPCFGParser(BottomUpPCFGChartParser):
    def sort_queue(self, queue, chart):
      # Sort the edges by the lengths of their tree tokens.
      queue.sort(lambda e1,e2: cmp(len(e1.loc()), len(e2.loc())))

BeamPCFGParse
*************

When large grammars are used to parse a text, the edge queue can grow quite long.
The edges at the end of a large well-sorted queue are unlikely to be used.
Therefore, it is reasonable to remove (or *prune*) these edges from the queue.

``BeamPCFGParser`` provides a simple implementation of a pruning PCFG parser.  It
uses the same sorting order as ``InsidePCFGParser``.  But whenever the edge queue
grows beyond a pre-defined maximum length, ``BeamPCFGParser`` truncates it.  The
resulting search strategy, lowest-cost-first search with pruning, is a type of beam
search.  (A *beam search* is a search strategy that only keeps the best partial
results.)  The queue's predefined maximum length is called the *beam size* (or simply
the *beam*).  The parser's beam size is set by the first argument to its constructor.

Beam search reduces the space requirements for lowest-cost-first search, by
discarding edges that are not likely to be used.  But beam search also loses many of
lowest-cost-first search's more useful properties.  Beam search is not optimal: it is
not guaranteed to find the best parse first.  In fact, since it might prune a
necessary edge, beam search is not even *complete*: it is not guaranteed to return a
parse if one exists.

The implementation for ``BeamPCFGParse`` defines two methods.  First, it overrides
the constructor, since it needs to record the beam size.  And second, it defines the
``sort_queue`` method, which sorts the queue and discards any excess edges::
        
  class BeamPCFGParser(BottomUpPCFGChartParser):
    def __init__(self, beam_size, grammar, trace=0):
      BottomUpPCFGChartParser.__init__(self, grammar, trace)
      self._beam_size = beam_size

    def sort_queue(self, queue, chart):
      # Sort the queue.
      queue.sort(lambda e1,e2:cmp(e1.tree().prob(), e2.tree().prob()))
      # Truncate the queue, if necessary.
      if len(queue) > self._beam_size:
        queue[:] = queue[len(queue)-self._beam_size:]

Note that when truncating the queue, ``sort_queue`` uses the expression ``queue[:]``
to change the *contents* of the ``queue`` variable.  In particular, compare it to the
following code, which reassigns the local variable ``queue``, but does not modify the
contents of the given list::

  # WRONG: This does not change the contents of the edge queue. 
  if len(queue) > self._beam_size:
    queue = queue[len(queue)-self._beam_size:]

  # WRONG: The sort method returns None.
  return queue.sort(lambda e1,e2:cmp(e1.tree().prob(), e2.tree().prob()))

      
.. We plan to add an inside/outside parser; when we
   do, we'll add a description of it to this section. 

Using ``BottomUpChartParser``
-----------------------------

These parsers are created using the ``BottomUpPCFGChartParser`` subclasses's
constructors.  These include: ``InsideParse``, ``LongestParse``, ``BeamParser``, and
``RandomParse``.

See the reference documentation for the ``BottomUpChartParse`` module for a complete
list of subclasses.  Unless a subclass overrides the constructor, it takes a single
PCFG:

  >>> inside_parser = InsidePCFGParser(grammar)
  >>> longest_parser = LongestPCFGParser(grammar)
  >>> beam_parser = BeamPCFGParser(20, grammar)

.. Warning:: ``BottomUpChartParse`` is an abstract class; you should not directly
   instantiate it.  If you try to use it to parse a text, it will raise an exception,
   since ``sort_queue`` will be undefined.

  >>> inside_parser.parse(sent1)
  (S:
    (NP: (Det: <the>) (N: <dog>))
    (VP: (V: <ate>) (NP: (Det: <my>) 
                               (N: <cookie>)))) 0.00175
      
  >>> inside_parser.parse_n(sent2)
  5.2040625e-05      
  (S:
    (NP: <I>)
    (VP:
      (V: <saw>)
      (NP:
        (NP: <John>)
        (PP:
          (P: <with>)
          (NP: (Det: <my>) (N: <cookie>))))))
  3.82192874e-06
  (S:
    (NP: <I>)
    (VP:
      (VP:
        (V: <saw>)
        (NP: <John>))
      (PP:
        (P: <with>)
        (NP: (Det: <my>) (N: <cookie>)))))

The ``trace`` method can be used to set the level of tracing output that is generated
when parsing a text.  Trace output displays edges as they are added to the chart, and
shows the probability for each edges' tree token.

  >>> inside_parser.trace(3)
  >>> inside_parser.parse(text2)

  Initializing the queue with token edges...
  Processing the edge queue...
    |[=] . . . . .| 'I' -> *          1.0000000
    |. [=] . . . .| 'saw' -> *        1.0000000
    |. . [=] . . .| 'John' -> *       1.0000000
    |. . . [=] . .| 'with' -> *       1.0000000
    |. . . . [=] .| 'my' -> *         1.0000000
    |. . . . . [=]| 'cookie' -> *     1.0000000
    |. > . . . . .| V -> * 'saw'      0.6500000
    |. > . . . . .| VP -> * V NP      0.7000000
    |. [=] . . . .| V -> 'saw' *      0.6500000
    |. . . > . . .| P -> * 'with'     0.6100000
           .               .               .
           .               .               .
           .               .               .
    |. . [=======>| NP -> NP * PP     0.0001906
    |. [=========]| VP -> VP PP *     0.0001387
    |[===========]| S -> NP VP *      0.0000520
    |. [=========>| VP -> VP * PP     0.0000346
    |[===========]| S -> NP VP *      0.0000208
  Found 2 parses with 54 edges

----

NLTK_

.. _NLTK: http://nltk.sourceforge.net/
