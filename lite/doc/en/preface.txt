.. -*- mode: rst -*-
.. include:: ../definitions.txt

==============================================
Learning NLP with the Natural Language Toolkit
==============================================

:Authors: Steven Bird, Ewan Klein, Edward Loper
:Version: |version|
:Revision: $Revision$
:Date: $Date$
:Copyright: |copy| 2001-2005 University of Pennsylvania
:License: Creative Commons Attribution-NonCommercial-ShareAlike License

.. Note:: This is a draft.  Please send any feedback to the authors.

------------
Introduction
------------

The *Natural Language Toolkit (NLTK)* was originally created as part
of a computational linguistics course in the Department of Computer
and Information Science at the University of Pennsylvania in 2001.
Since then it has been developed and expanded with the help of dozens
of contributors.  It has now been adopted in courses in dozens of
universities, and serves as the basis of many research projects.  In
this section we will discuss some of the benefits of learning (and
teaching) NLP using NLTK.

Recently, the NLTK developers have been creating a lightweight version
NLTK, called NLTK-Lite.  NLTK-Lite is simpler and faster than NLTK.
Once it is complete, NLTK-Lite will provide all the same functionality
as NLTK.  However, unlike NLTK, NLTK-Lite does not impose such a heavy
burden on the programmer.  Wherever possible, standard Python objects
are used instead of custom NLP versions, so that students learning to
program for the first time will be learning to program in Python with
some useful libraries, rather than learning to program in NLTK.

NLP is often taught within the confines of a single-semester course,
either at advanced undergraduate level, or at postgraduate level.
Unfortunately, it turns out to be rather difficult to cover both the
theoretical and practical sides of the subject in such a short span of
time.  Some courses focus on theory to the exclusion of practical
exercises, and deprive students of the challenge and excitement of
writing programs to automatically process natural language.  Other
courses are simply designed to teach programming for linguists, and do
not get past the mechanics of programming to cover significant NLP.
NLTK was developed to address this very problem, making it feasible to
cover a substantial amount of theory and practice within a
single-semester course.
    
A significant fraction of any NLP course is made up of fundamental
data structures and algorithms.  These are usually taught with the
help of formal notations and complex diagrams.  Large trees and charts
are copied onto the board and edited in tedious slow motion, or
laboriously prepared for presentation slides.  A more effective method
is to use live demonstrations in which those diagrams are generated
and updated automatically.  NLTK provides interactive graphical user
interfaces, making it possible to view program state and to study
program execution step-by-step.  Most NLTK components have a
demonstration mode, and will perform an interesting task without
requiring any special input from the user.  It is even possible to
make minor modifications to programs in response to "what if"
questions.  In this way, students learn the mechanics of NLP quickly,
gain deeper insights into the data structures and algorithms, and
acquire new problem-solving skills.
    
NLTK supports assignments of varying difficulty and scope.  In the
simplest assignments, students experiment with existing components to
perform a wide variety of NLP tasks.  This may involve no programming
at all, in the case of the existing demonstrations, or simply changing
a line or two of program code.  As students become more familiar with
the toolkit they can be asked to modify existing components or to
create complete systems out of existing components.  NLTK also
provides students with a flexible framework for advanced projects,
such as developing a multi-component system, by integrating and
extending NLTK components, and adding on entirely new components.
Here NLTK helps by providing standard implementations of all the basic
data structures and algorithms, interfaces to standard corpora,
substantial corpus samples, and a flexible and extensible
architecture.  Thus, as we have seen, NLTK offers a fresh approach to
NLP pedagogy, in which theoretical content is tightly integrated with
application.
    
------------------
The Design of NLTK
------------------

NLTK was designed with six requirements in mind.  First, NLTK is *easy
to use*.  The primary purpose of the toolkit is to allow students to
concentrate on building natural language processing systems.  The more
time students must spend learning to use the toolkit, the less useful
it is.  Second, we have made a significant effort to ensure that all
the data structures and interfaces are *consistent*, making it easy to
carry out a variety of tasks using a uniform framework.  Third, the
toolkit is *extensible*, easily accommodating new components, whether
those components replicate or extend the toolkit's existing
functionality.  Moreover, the toolkit is organized so that it is
usually obvious where extensions would fit into the toolkit's
infrastructure.  Fourth, the toolkit is designed to be *simple*,
providing an intuitive and appealing framework along with substantial
building blocks, for students to gain a practical knowledge of NLP
without having to write mountains of code.  Fifth, the toolkit is
*modular*, so that the interaction between different components of the
toolkit is minimized, and uses simple, well-defined interfaces.  In
particular, it should be possible to complete individual projects
using small parts of the toolkit, without needing to understand how
they interact with the rest of the toolkit.  This allows students to
learn how to use the toolkit incrementally throughout a course.
Modularity also makes it easier to change and extend the toolkit.
Finally, the toolkit is *well documented*, including nomenclature,
data structures, and implementations.
    
Contrasting with these requirements are three non-requirements.
First, while the toolkit provides a wide range of functions, it is not
intended to be encyclopedic.  There should be a wide variety of ways
in which students can extend the toolkit.  Second, while the toolkit
should be efficient enough that students can use their NLP systems to
perform meaningful tasks, it does not need to be highly optimized for
runtime performance.  Such optimizations often involve more complex
algorithms, and sometimes require the use of C or C++, making the
toolkit harder to install.  Third, we have avoided clever programming
tricks, since clear implementations are far preferable to ingenious
yet indecipherable ones.
    
NLTK is organized into a collection of task-specific components.  Each
module is a combination of data structures for representing a
particular kind of information such as trees, and implementations of
standard algorithms involving those structures such as parsers.  This
approach is a standard feature of *object-oriented design*, in which
components encapsulate both the resources and methods needed to
accomplish a particular task.
    
The most fundamental NLTK components are for identifying and
manipulating individual words of text.  These include: ``tokenize``,
for breaking up strings of characters into word tokens; ``corpora``,
for reading various corpora; ``tag``, for adding part-of-speech tags,
including regular-expression taggers, n-gram taggers and Brill
taggers.
    
The second kind of module is for creating and manipulating structured
linguistic information.  These components include: ``tree``, for
representing and processing parse trees; ``featurestructure``, for
building and unifying nested feature structures (or attribute-value
matrices); ``cfg``, for specifying free grammars; and ``parse``, for
creating parse trees over input text, including chart parsers, chunk
parsers and probabilistic parsers.

Several utility components are provided to facilitate processing and
visualization.  These include: ``draw``, to visualize NLP structures
and processes; ``probability``, to count and collate events, and
perform statistical estimation; and ``corpus``, to access tagged
linguistic corpora.
    
.. Finally, several advanced components are provided, mostly
   demonstrating NLP applications of machine learning techniques.  These
   include: ``clusterer``, for discovering groups of similar items within
   a large collection, including k-means and expectation maximisation;
   ``classifier``, for categorising text into different types, including
   naive Bayes and maximum entropy; and ``hmm``, for Hidden Markov
   Models, useful for a variety of sequence classification tasks.
    
A further group of components is not part of NLTK proper.  These
are a wide selection of third-party contributions, often developed as
student projects at various institutions where NLTK is used, and
distributed in a separate package called *NLTK Contrib*.  Several of
these student contributions, such as the Brill tagger and the HMM
module, have now been incorporated into NLTK.  Although these
components are not maintained, they may serve as a useful starting
point for future student projects.  In general, they do not work with
the current version of NLTK.
    
In addition to software and documentation, NLTK provides substantial
corpus samples, listed below.  Many of these can be accessed using the
``corpora`` module, avoiding the need to write specialized file
parsing code before you can do NLP tasks.
    
=========================  =====================  ============================================================
Corpora and Corpus Samples Distributed with NLTK (starred items with NLTK-Lite)
--------------------------------------------------------------------------------------------------------------
Corpus                     Compiler               Contents
=========================  =====================  ============================================================
20 Newsgroups (sel)        Lang, Rennie           3 newsgroups, 4000 posts, 780k words
\*Brown Corpus             Francis, Kucera        15 genres, 1.15M words, tagged
CoNLL 2000 Chunking Data   Tjong Kim Sang         270k words, tagged and chunked
\*Genesis Corpus           Misc web sources       6 texts, 200k words, 6 languages
Project Gutenberg (sel)    Hart, Newby, et al     14 texts, 1.7M words
NIST 1999 Info Extr (sel)  Garofolo               63k words, newswire and named-entity SGML markup
Levin Verb Index           Beth Levin             3k verbs with Levin classes
Lexicon Corpus                                    Words, tags and frequencies from Brown Corpus and WSJ
Names Corpus               Kantrowitz, Ross       8k male and female names
PP Attachment Corpus       Ratnaparkhi            28k prepositional phrases, tagged as noun or verb modifiers
Roget's Thesaurus          Project Gutenberg      200k words, formatted text
SEMCOR                     Rus, Mihalcea          880k words, part-of-speech and sense tagged
SENSEVAL 2 Corpus          Ted Pedersen           600k words, part-of-speech and sense tagged
Stopwords Corpus           Porter et al           2,400 stopwords for 11 languages
\*Penn Treebank (sel       LDC                    40k words, tagged and parsed
Wordlist Corpus            OpenOffice.org et al   960k words and 20k affixes for 8 languages
=========================  =====================  ============================================================

----

NLTK_

.. _NLTK: http://nltk.sourceforge.net/

