% for slides
\documentclass{beamer}

% for handout
% \documentclass[handout]{beamer}
% \usepackage{pgfpages}
% \pgfpagesuselayout{4 on 1}[a4paper,landscape,scale=0.9]

\mode<handout>
{
  \usetheme{default}
  \usepackage{fullpage}
  \usepackage{pgf}
  \usepackage{hyperref}
  \setjobnamebeamerversion{field.beamer}
}

\mode<article>
{
  \usepackage{fullpage}
  \usepackage{pgf}
  \usepackage{hyperref}
  \setjobnamebeamerversion{field.beamer}
}

\mode<presentation>
{
  \usetheme{Warsaw}
  \setbeamercovered{transparent}
  % If you wish to uncover everything in a step-wise fashion, uncomment
  % the following command: 
  \beamerdefaultoverlayspecification{<+->}

}


\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{times}
\usepackage[T1]{fontenc}
\usepackage{alltt}

\title{Accessing and Analyzing Linguistic Field Data}

\author{Steven Bird}
\institute{
  University of Melbourne, AUSTRALIA
}

\date{\today}

\subject{Natural Language Toolkit}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
  \frametitle{Introduction}
  \begin{itemize}
  \item variety of data types, e.g.
  \item \textbf{lexicon:} database of words, minimally containing part
    of speech information and glosses
  \item \textbf{paradigm:} any kind of rational
    tabulation of words or phrases to illustrate contrasts and systematic
    variation
  \item \textbf{text:} any larger unit such as a narrative or a
    conversation
  \item descriptions: field notes, grammars and analytical papers
  \item complex inter-relations (new word; lexicon update, paradigm;
    add to field notes; extend grammar, ...)
  \item tasks: sorting, tabulating, garnering statistics, searching,
  verifying transcriptions
  \item principle challenge is computational
  \end{itemize}

\end{frame}

\begin{frame}
\frametitle{Tools and Technologies}
\begin{itemize}
\item General-purpose tools
  \begin{itemize}
  \item word processors
  \item spreadsheets
  \item databases
  \end{itemize}
\item Special prupose tools
  \begin{itemize}
  \item Shoebox
  \end{itemize}
\item broader question: data management in support of best practices
\item Bird, Steven and Gary Simons (2003).  Seven Dimensions of Portability
    for Language Documentation and Description, \textit{Language} 79: 557-582.
\end{itemize}
\end{frame}

\section{General Purpose Tools}

\subsection{Word Processors}

\begin{frame}
\frametitle{General Purpose Tools: Word Processors}

\begin{itemize}
\item office software widely used in computer-based language
  documentation work
\item familiar, ready availability (though may be too expensive?)
\item Word processors for managing dictionaries:
  \begin{itemize}
  \item consistency of content and format --- time consuming
  \item how would we check the following constraint: \\
    \textit{each entry must have a part-of-speech field, drawn from a set of 20
     possibilities, displayed after the pronunciation field, and rendered
     in 11-point bold}
  \item generally a bad idea!
  \end{itemize}
\item save to a non-proprietary format (e.g. RTF, HTML, or XML)
\item write programs to do this checking automatically
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Example: Microsoft Word}
\begin{itemize}
\item sleep \verb|[sli:p]| \textbf{vi} \textit{condition of body and mind...}
\item We can enter this in MSWord, then "Save as Web Page"
\item resulting HTML:

\begin{verbatim}
<p class=MsoNormal>sleep <span style='mso-spacerun:yes'> </span>[<span
class=SpellE>sli:p</span>]<span style='mso-spacerun:yes'>  </span><b><span
style='font-size:11.0pt'>vi</span></b><span style='mso-spacerun:yes'>  </span><i>a
condition of body and mind ...o:p></o:p></i></p>
\end{verbatim}

\item Observations:
  \begin{itemize}
  \item entry: HTML paragraph using the \verb|<p>| element
  \item pos: inside a \verb|<span style='font-size:11.0pt'>| element
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Example: Validating Dictionaries stored in MSWord}

\begin{verbatim}
>>> import re
>>> legal_pos = set(['n', 'v.t.', 'v.i.', 'adj', 'det'])
>>> pattern = re.compile(r"'font-size:11.0pt'>([a-z.]+)<")
>>> document = open("dict.htm").read()
>>> used_pos = set(re.findall(pattern, document))
>>> illegal_pos = used_pos.difference(legal_pos)
>>> print list(illegal_pos)
['v.intr', 'v.i', 'intrans']
\end{verbatim}
\end{frame}

\subsection{Spreadsheets}

\begin{frame}[fragile]
\frametitle{Example: Converting MSWord dictionary to Excel}
\small

\begin{verbatim}
>>> import re
>>> document = open("dict.htm").read()
>>> document = re.sub("[\r\n]", "", document)
>>> word_pattern = re.compile(r">([\w]+)")
>>> pron_pattern = re.compile(r"\[.*>([a-z:]+)<.*\]")
>>> for entry in document.split("<p"):
...     word_match = word_pattern.search(entry)
...     pron_match = pron_pattern.search(entry)
...     if word_match and pron_match:
...         lex = word_match.group(1)
...         pos = pron_match.group(1)
...         print '"%s","%s"' % (lex, pos)
"sleep","sli:p"
"walk","wo:k"
"wake","weik"
\end{verbatim}
\end{frame}

\begin{frame}
\frametitle{Spreadsheets}
\begin{itemize}
\item wordlists, paradigms
\item e.g. comparative wordlist (\url{http://www.rosettaproject.org/}):
  \begin{itemize}
  \item row for each cognate set
  \item column for each language
  \end{itemize}
\item export spreadsheets in \textit{CSV format}
\item read them using Python's \texttt{csv} module
\item e.g. find cognates having an edit-distance of at least three from each other
\end{itemize}
\end{frame}

\subsection{Databases}

\begin{frame}
\frametitle{Databases}
\begin{itemize}
  \item lexicons are stored in a full-fledged relational databases
  \item databases can implement many well-formedness constraints\\
    e.g. ensure that all parts-of-speech come from an
    \textit{enumerated type}
  \item However, the relational model is often too restrictive
  \item optional, repeatable fields (e.g.
    dictionary sense definitions and example sentences).
  \item SQL cannot express many linguistically-motivated queries\\
    \textit{Find all words that appear in example sentences for which no dictionary entry is provided}
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Database Example}  

\begin{verbatim}
>>> import csv
>>> lexemes = []
>>> defn_words = []
>>> for row in csv.reader(open("dict.csv")):
...     lexeme, pron, pos, defn = row
...     lexemes.append(lexeme)
...     defn_words += defn.split()
>>> undefined = list(set(defn_words).difference(set(lexemes)))
>>> undefined.sort()
>>> print undefined
['...', 'a', 'and', 'body', 'by', 'cease', 'condition', 'down', 'each', 'foot', 'lifting', 'mind', 'of', 'progress', 'setting', 'to']
\end{verbatim}
\end{frame}

\section{Processing Shoebox Data}

\begin{frame}
\frametitle{Processing Shoebox Data}

\begin{itemize}
\item single most popular tool for managing linguistic field data
\item many kinds of validation and formatting not supported by Shoebox software
\item simple file format
\item each file is a collection of \textit{entries} (aka
  \textit{records})
\item each entry is made up of one or more \textit{fields}
\item challenges:
  \begin{enumerate}
  \item different entry types, implications for which fields are present
  \item field order only sometimes significant
  \end{enumerate}
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Shoebox Example}

\begin{verbatim}
\lx kaa
\ps N.M
\cl isi
\ge cooking banana
\gp banana bilong kukim
\sf FLORA
\dt 12/Feb/2005
\ex Taeavi iria kaa isi kovopaueva kaparapasia.
\xp Taeavi i bin planim gaden banana bilong kukim tasol long paia.
\xe Taeavi planted banana in order to cook it.
\end{verbatim}
\end{frame}

\subsection{Accessing Shoebox Data}

\begin{frame}[fragile]
\frametitle{Accessing Shoebox Data: raw() method}

\begin{itemize}
\item scan the file, processing entries one at a time
\item preserves order of fields
\end{itemize}

\begin{verbatim}
>>> from nltk_lite.corpora import shoebox
>>> sum_size = num_entries = 0
>>> for entry in shoebox.raw('rotokas'):
...     num_entries += 1
...     sum_size += len(entry)
>>> print sum_size/num_entries
10
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Accessing Shoebox Data: dictionary() method}

\begin{itemize}
\item read each entry into a Python dictionary
\item assumes field names are unique
\end{itemize}

\scriptsize
\begin{verbatim}
>>> entries = list(shoebox.dictionary('rotokas'))
>>> from pprint import pprint
>>> pprint(entries[3])
{'cl': 'isi',
 'dt': '12/Feb/2005',
 'ex': 'Taeavi iria kaa isi kovopaueva kaparapasia.',
 'ge': 'cooking banana',
 'gp': 'banana bilong kukim',
 'lx': 'kaa',
 'ps': 'N.M',
 'sf': 'FLORA',
 'xe': 'Taeavi planted banana in order to cook it.',
 'xp': 'Taeavi i bin planim gaden banana bilong kukim tasol long paia.'}
\end{verbatim}
\end{frame}

\subsection{Simple Entry Processing}

\begin{frame}[fragile]
\frametitle{Simple Entry Processing}
\scriptsize
\begin{enumerate}
\item scan through each entry
\item for each entry, scan through each field
\end{enumerate}

\begin{verbatim}
>>> lexemes = []
>>> for entry in shoebox.raw('rotokas'):
...     for field in entry:
...         if field[0] == 'lx':
...             normalised_lexeme = field[1].lower()
...             lexemes.append(normalised_lexeme)
\end{verbatim}

\textbf{Alternative:}

\begin{verbatim}
>>> lexemes = [field[1].lower()
...              for entry in shoebox.raw('rotokas')
...	               for field in entry if field[0] == 'lx']
\end{verbatim}
\end{frame}


\begin{frame}[fragile]
\frametitle{Adding New Fields}
\small

\begin{itemize}
\item Example: add CV field
\item Aside: utility function to do CV template
\end{itemize}

\begin{verbatim}
>>> import re
>>> def cv(s):
...     s = s.lower()
...     s = re.sub(r'[^a-z]',     r'_', s)
...     s = re.sub(r'[aeiou]',    r'V', s)
...     s = re.sub(r'[^V_]',      r'C', s)
...     return (s)
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Adding New Fields (cont)}
\scriptsize

\begin{verbatim}
>>> raw_entries = list(shoebox.raw('rotokas'))
>>> for field in raw_entries[50]:
...     print "\\%s %s" % field
...	    if field[0] == "lx":
...	        print "\\cv %s" % cv(field[1])
\lx kaeviro
\cv CVVCVCV
\ps V.A
\ge lift off
\ge take off
\gp go antap
\nt used to describe action of plane
\dt 12/Feb/2005
\ex Pita kaeviroroe kepa kekesia oa vuripierevo kiuvu.
\xp Pita i go antap na lukim haus win i bagarapim.
\xe Peter went to look at the house that the wind destroyed.
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Removing Fields}
\scriptsize

\begin{itemize}
\item sanitise our data before giving it to others
\end{itemize}

\begin{verbatim}
>>> retain = ('lx', 'ps')
>>> raw_entries = list(shoebox.raw('rotokas'))
>>> for entry in raw_entries[50:55]:
...     for field in entry:
...         if field[0] in retain:
...             print "\\%s %s" % field
...     print
\lx kaeviro
\ps V.A

\lx kagave
\ps N.F

\lx kaie
\ps V.A

\lx kaiea
\ps N.N

\lx kaikaio
\ps N.N
\end{verbatim}
\end{frame}

\subsection{Formatting}

\begin{frame}[fragile]
\frametitle{Formatting Entries: dictionary() method}

\begin{itemize}
\item request specific fields without concern for their relative
  ordering
\end{itemize}
\scriptsize

\begin{verbatim}
>>> entries = list(shoebox.dictionary('rotokas'))
>>> for entry in entries[70:80]:
...     lex = entry['lx']
...     pos = entry['ps']
...     dfn = entry['ge']
...     if 'eng' in entry:
...         dfn = entry['eng']
...     print "%s (%s) '%s'" % (lex, pos, dfn)
kakapikoto (N.N2) 'newborn baby'
kakapu (V.B) 'place in sling for purpose of carrying'
kakapua (N.N) 'sling for lifting'
kakara (N.N) 'bracelet'
Kakarapaia (N.PN) 'village name'
kakarau (N.F) 'stingray'
Kakarera (N.PN) 'name'
Kakareraia (N.???) 'name'
kakata (N.F) 'cockatoo'
kakate (N.F) 'bamboo tube for water'
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Generating HTML Tables}

\begin{itemize}
\item Publish shoebox lexicon on the web
\end{itemize}
\scriptsize

\begin{verbatim}
>>> html = "<table>\n"
>>> for entry in entries[70:80]:
...     lex = entry['lx']
...     pos = entry['ps']
...     dfn = entry['ge']
...     if 'eng' in entry:
...         dfn = entry['eng']
...     html += "  <tr><td>%s</td><td>%s</td><td>%s</td></tr>\n" % (lex, pos, dfn)
>>> html += "</table>"
>>> print html
<table>
  <tr><td>kakapikoto</td><td>N.N2</td><td>newborn baby</td></tr>
  <tr><td>kakapu</td><td>V.B</td><td>place in sling for purpose of carrying</td></tr>
  <tr><td>kakapua</td><td>N.N</td><td>sling for lifting</td></tr>
  <tr><td>kakara</td><td>N.N</td><td>bracelet</td></tr>
  <tr><td>Kakarapaia</td><td>N.PN</td><td>village name</td></tr>
  <tr><td>kakarau</td><td>N.F</td><td>stingray</td></tr>
  <tr><td>Kakarera</td><td>N.PN</td><td>name</td></tr>
  <tr><td>Kakareraia</td><td>N.???</td><td>name</td></tr>
  <tr><td>kakata</td><td>N.F</td><td>cockatoo</td></tr>
  <tr><td>kakate</td><td>N.F</td><td>bamboo tube for water</td></tr>
</table>
\end{verbatim}
\end{frame}

\section{Linguistic Exploration}

\subsection{Reduplication}

\begin{frame}[fragile]
\frametitle{Reduplication}
\scriptsize
\begin{itemize}
\item create a table of lexemes and their glosses

\begin{verbatim}
>>> lexgloss = {}
>>> for entry in shoebox.dictionary('rotokas'):
...     if 'lx' in entry and entry['ps'][0] == 'V':
...         lexgloss[entry['lx']] = entry['ge']
\end{verbatim}

\item For each lexeme, check if the lexicon contains the reduplicated form:

\begin{verbatim}
>>> for lex in lexgloss:
...     if lex+lex in lexgloss:
...         print "%s (%s); %s (%s)" % (lex, lexgloss[lex], lex+lex, lexgloss[lex+lex])
kuvu (fill.up); kuvukuvu (stamp the ground)
kitu (save); kitukitu (scrub clothes)
kopa (ingest); kopakopa (gulp.down)
kasi (burn); kasikasi (angry)
koi (high pitched sound); koikoi (groan with pain)
kee (chip); keekee (shattered)
kauo (jump); kauokauo (jump up and down)
kea (deceived); keakea (lie)
kove (drop); kovekove (drip repeatedly)
kape (unable to meet); kapekape (grip with arms not meeting)
kapo (fasten.cover.strip); kapokapo (fasten.cover.strips)
koa (skin); koakoa (remove the skin)
kipu (paint); kipukipu (rub.on)
koe (spoon out a solid); koekoe (spoon out)
kovo (work); kovokovo (surround)
kiru (have sore near mouth); kirukiru (crisp)
kotu (bite); kotukotu (grind teeth together)
kavo (collect); kavokavo (work black magic)
kuri (scrape); kurikuri (scratch repeatedly)
karu (unhook); karukaru (open)
kare (return); karekare (return)
kari (break); karikari (shred)
kiro (write); kirokiro (write)
kae (carry); kaekae (tempt)
koru (make return); korukoru (obstruct)
ku (finished with); kuku (spoonfeed)
kosi (exit); kosikosi (exit)
\end{verbatim}
\end{itemize}
\end{frame}

\subsection{Complex Search Criteria}

\begin{frame}[fragile]
\frametitle{Complex Search Criteria}
\begin{itemize}
\item for phonological description, identify segments, alternations,
  syllable canon...
\item what syllable types occur in lexemes (MSC, conspiracies)?
\small

\begin{verbatim}
>>> from nltk_lite.tokenize import regexp
>>> from nltk_lite.probability import FreqDist
>>> fd = FreqDist()
>>> for lex in lexemes:
...     for syl in regexp(lex, pattern=r'[^aeiou][aeiou]'):
...         fd.inc(syl)
\end{verbatim}
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Complex Search Criteria (cont)}

\begin{itemize}
\item Tabulate the results:

{\small
\begin{verbatim}
>>> for vowel in 'aeiou':
...     for cons in 'ptkvsr':
...          print '%s%s:%4d ' % (cons, vowel, fd.count(cons+vowel)),
...     print
pa:  84  ta:  43  ka: 414  va:  87  sa:   0  ra: 185 
pe:  32  te:   8  ke: 139  ve:  25  se:   1  re:  62 
pi:  97  ti:   0  ki:  88  vi:  96  si:  95  ri:  83 
po:  31  to: 140  ko: 403  vo:  42  so:   3  ro:  86 
pu:  49  tu:  35  ku: 169  vu:  44  su:   1  ru:  72 
\end{verbatim}}
\item NB \texttt{t} and \texttt{s} columns
\item \texttt{ti} not attested, while \texttt{si} is frequent: palatalization?
\item which lexeme contains \texttt{su}?  \textit{kasuari}
\end{itemize}
\end{frame}


\begin{frame}[fragile]
\frametitle{Prosodically-motivated search}
\small
\begin{itemize}
\item segmental and prosodic constraints
\item well-formed morphemes and lexemes
\item highly-specific query (e.g. find things predicted not to exist
  by our theoretical model?)
\item \textit{Find all trisyllabic lexemes ending in a long vowel}
\item bottom-up design: need to count syllables, and test for vowels
\end{itemize}

\begin{verbatim}
>>> def num_syls(word):
...     template = cv(word)
...     num_cons = template.count('C')
...     return num_cons
>>> def is_vowel(char):
...     return (char in 'aeiou')
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Prosodically-motivated search (cont)}
\scriptsize

\begin{verbatim}
>>> for entry in shoebox.dictionary('rotokas'):
...     if 'lx' in entry:
...         lex = entry['lx']
...         pos = entry['ps']
...         if num_syls(lex) == 3 and is_vowel(lex[-1]) and is_vowel(lex[-2]) and pos[0] == 'V':
...             dfn = entry['ge']
...             print "%s (%s) '%s'" % (lex, pos, dfn)
kaetupie (V.B) 'tighten'
kakupie (V.B) 'yodel'
kapatau (V.B) 'add to'
kapuapie (V.B) 'wound'
kapupie (V.B) 'close tight'
kapuupie (V.B) 'close'
karepie (V.B) 'return'
karivai (V.A) 'have an appetite'
kasipie (V.B) 'care for'
kaukaupie (V.B) 'intense sunlight'
kavorou (V.A) 'intercept'
kavupie (V.B) 'leave.behind'
kekepie (V.B) 'show'
keruria (V.A) 'determined'
ketoopie (V.B) 'make sprout from seed'
koatapie (V.B) 'accept'
koetapie (V.B) 'satisfy curiosity'
kokovae (V.A) 'sing'
kokovua (V.B) 'shave the hair line'
kopiipie (V.B) 'kill'
korupie (V.B) 'take outside'
kosipie (V.B) 'make exit'
kovopie (V.B) 'use to make work'
kukuvai (V.B) 'cover the head from rain or sun'
kuvaupie (V.B) 'leave alone'
kuverea (V.A) 'not all right'
\end{verbatim}
\end{frame}

\subsection{Minimal Sets}

\begin{frame}
\frametitle{Finding Minimal Sets}
\begin{itemize}
\item E.g. mace vs maze, face vs faze
\item minimal set parameters: context, target, display
\small

\begin{tabular}{l|lll}
\textbf{Minimal Set} &     \textbf{Context} &  \textbf{Target} & \textbf{Display} \\ \hline
\textit{bib, bid, big} &   first two letters&  third letter    & word\\
\textit{deal (N), deal (V)} & whole word    &  pos             & word (pos)
\end{tabular}
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Finding Minimal Sets: Example 1}
\small

\begin{verbatim}
>>> lexemes = [entry['lx'] for entry in shoebox.dictionary('rotokas')
...              if 'lx' in entry]
>>> position = 1
>>> parameters = [(lex[:position] + '_' + lex[position+1:],
...                lex[position],
...                lex)
...               for lex in lexemes if len(lex) == 4]
\end{verbatim}

\begin{verbatim}
>>> from nltk_lite.utilities import MinimalSet
>>> def build_min_set(parameters):
...     min_set = MinimalSet()
...     for context, target, display in parameters:
...         min_set.add(context, target, display)
...     return min_set
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Finding Minimal Sets: Example 1 (cont)}
\small

\begin{verbatim}
>>> ms = build_min_set(parameters)
>>> for context in ms.contexts(3):
...     print context + ':',
...     for target in ms.targets():
...         print "%-4s" % ms.display(context, target, "-"),
...     print
k_si: kasi -    kesi -    kosi
k_ru: karu kiru keru kuru koru
k_pu: kapu kipu -    -    kopu
k_ro: karo kiro -    -    koro
k_ri: kari kiri keri kuri kori
k_pa: kapa -    kepa -    kopa
k_ra: kara kira kera -    kora
k_ku: kaku -    -    kuku koku
k_ki: kaki kiki -    -    koki
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Finding Minimal Sets: Example 2}
\scriptsize

\begin{verbatim}
>>> parameters = [(entry['lx'], entry['ps'][0], "%s (%s)" % (entry['ps'][0], entry['ge']))
...               for entry in shoebox.dictionary('rotokas') if 'lx' in entry]
>>> ms = build_min_set(parameters)
>>> for context in ms.contexts()[:10]:
...     print "%10s:" % context, "; ".join(ms.display_all(context))
  kokovara: N (unripe coconut); V (unripe)
     kapua: N (sore); V (have sores)
      koie: N (pig); V (get pig to eat)
      kovo: C (garden); N (work); V (work)
    kavori: N (lobster); V (collect crayfish or lobster)
    korita: N (cutlet?); V (cut up meat)
      keru: N (bone); V (harden like bone)
  kirokiro: N (bush used for sorcery); V (write)
    kaapie: N (fishhook); V (capture)
       kou: C (heap); V (defecate)
\end{verbatim}
\end{frame}

\subsection{Example Application}

\begin{frame}
\frametitle{Example Application: Improving Access to Lexical Resources}
\begin{itemize}
\item lexicon as a \textit{language resource}
\item existence of standardized writing system?  literacy?
\item lookup by approximate spelling
\item soundex
\item edit distance
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Computing Soundex}
\small

\begin{verbatim}
>>> group = {
...     ' ':0,                     # blank (for short words)
...     'p':1,  'b':1,  'v':1,     # labials
...     't':2,  'd':2,  's':2,     # alveolars
...     'l':3,  'r':3,             # sonorant consonants
...     'i':4,  'e':4,             # high front vowels
...     'u':5,  'o':5,             # high back vowels
...     'a':6                      # low vowels
... }
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Computing Soundex (cont)}
\small

\begin{verbatim}
>>> def soundex(word):
...     if len(word) == 0: return word  # sanity check
...     word += '    '                  # ensure word long enough
...     c0 = word[0].upper()
...     c1 = group[word[1]]
...     cons = filter(lambda x: x in 'pbvtdslr ', word[2:])
...     c2 = group[cons[0]]
...     c3 = group[cons[1]]
...     return "%s%d%d%d" % (c0, c1, c2, c3)
>>> print soundex('kalosavi')
K632
>>> print soundex('ti')
T400
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Soundex Index}
\begin{verbatim}
>>> soundex_idx = {}
>>> for lex in lexemes:
...     code = soundex(lex)
...     if code not in soundex_idx:
...         soundex_idx[code] = set()
...     soundex_idx[code].add(lex)
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Sorting and Lookup}
\scriptsize
\begin{verbatim}
>>> from nltk_lite.utilities import edit_dist
>>> def fuzzy_spell(target):
...     scored_candidates = []
...     code = soundex(target)
...     for word in soundex_idx[code]:
...         dist = edit_dist(word, target)
...         scored_candidates.append((dist, word))
...     scored_candidates.sort()
...     return [w for (d,w) in scored_candidates[:10]]
\end{verbatim}

\begin{verbatim}
>>> fuzzy_spell('kokopouto')
['kokopeoto', 'kokopuoto', 'kokepato', 'koovoto', 'koepato', 'kooupato', 'kopato', 'kopiito', 'kovuto', 'koavaato']
>>> fuzzy_spell('kogou')
['kogo', 'koou', 'kokeu', 'koko', 'kokoa', 'kokoi', 'kokoo', 'koku', 'kooe', 'kooku']
\end{verbatim}
\end{frame}

\section{Generating Reports}

\subsection{Summaries}

\begin{frame}[fragile]
\frametitle{Generating Summary Reports}
\begin{itemize}
\item overall picture of the quality and organisation of the data
\item E.g. entry patterns
\end{itemize}

\begin{verbatim}
>>> fd = FreqDist()
>>> for entry in shoebox.raw('rotokas'):
...     for field in entry:
...	        fd.inc(field[0])
>>> fd.sorted_samples()[:10]
['ge', 'ex', 'xe', 'xp', 'gp', 'lx', 'ps', 'dt', 'rt', 'eng']
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Generating Summary Reports (cont)}

\begin{verbatim}
>>> fd = FreqDist()
>>> for entry in shoebox.raw('rotokas'):
...     marker_list = [field[0] for field in entry]
...     markers = ':'.join(marker_list)
...	    fd.inc(markers)
>>> top_ten = fd.sorted_samples()[:10]
>>> print '\n'.join(top_ten)
lx:rt:ps:ge:gp:dt:ex:xp:xe
lx:ps:ge:gp:dt:ex:xp:xe
lx:ps:ge:gp:dt:ex:xp:xe:ex:xp:xe
lx:rt:ps:ge:gp:dt:ex:xp:xe:ex:xp:xe
lx:ps:ge:gp:nt:dt:ex:xp:xe
lx:ps:ge:gp:dt
lx:ps:ge:ge:gp:dt:ex:xp:xe:ex:xp:xe
lx:rt:ps:ge:ge:gp:dt:ex:xp:xe:ex:xp:xe
lx:ps:ge:ge:gp:dt:ex:xp:xe
lx:rt:ps:ge:ge:gp:dt:ex:xp:xe
\end{verbatim}
\end{frame}

\subsection{Timestamps}

\begin{frame}[fragile]
\frametitle{Looking at Timestamps}
\small
\begin{verbatim}
>>> fd = FreqDist()
>>> from string import split
>>> for entry in shoebox.dictionary('rotokas'):
...     if 'dt' in entry:
...         (day, month, year) = split(entry['dt'], '/')
...         fd.inc((month, year))
>>> for time in fd.sorted_samples():
...     print time[0], '/', time[1], ':', fd.count(time)
Feb / 2005 : 307
Dec / 2004 : 151
Jan / 2005 : 123
Feb / 2004 : 64
Sep / 2004 : 49
May / 2005 : 46
Mar / 2005 : 37
Apr / 2005 : 29
Jul / 2004 : 14
Nov / 2004 : 5
Oct / 2004 : 5
Aug / 2004 : 4
May / 2003 : 2
Jan / 2004 : 1
May / 2004 : 1
\end{verbatim}
\end{frame}

\end{document}
