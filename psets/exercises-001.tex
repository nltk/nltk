% Natural Language Toolkit: Pseudo Problem Set 1
%
% Copyright (C) 2001 University of Pennsylvania
% Author: Edward Loper <edloper@gradient.cis.upenn.edu>
% URL: <http://nltk.sf.net>
% For license information, see LICENSE.TXT
%
% $Id$

\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{boxedminipage}

\begin{document}
\title{Psuedo-Problem Set 1}
\author{Edward Loper}
\maketitle

\section{Installation}

Download nltk.tgz, and untar it in your working directory:

\noindent\begin{tabbing}
\tt\textbf{gradient\%} cd \textit{my-working-directory} \\
\tt\textbf{gradient\%} tar -xvzf ntlk.tgz
\end{tabbing}

\noindent
This will create a new subdirectory, called \texttt{nltk}, conaining
the toolkit.  In order to use the toolkit, you will need to import the
apropriate symbols.  For this problem set, the following will suffice:

\noindent\begin{tabbing}
\tt\textbf{>>>} from nltk.token import * \\
\tt\textbf{>>>} from nltk.tagger import *
\end{tabbing}

\noindent
This problem set also uses a few data files (\texttt{words.txt} and
\texttt{tagged.txt}).  You should copy these files to your working
directory.  \textit{(For now, just supply your own files.
\texttt{words.txt} should be raw text, and \texttt{tagged.txt} should
have space-delimited symbols like \texttt{cow/NN}.)}

\section{Tokens}

Tokens are used to represent individual occurances of elements of a
text, such as words or numbers.  In this problem, you will learn how
to use tokens.  When working on this problem, and when processing
tokens in general, it is useful to keep clear the distinction between
tokens and token types:

\begin{description}
  \item[Token Type] An individual element of text, such as a word or a
       punctuation mark.
  \item[Token] An occurance of a token type in a text.
\end{description}

\noindent
In the toolkit, a token consists of a token type and a location.  Try
creating a few \texttt{TokenType}s, \texttt{Location}s, and
\texttt{Token}s:  

\noindent\begin{tabbing}
\tt\textbf{>>>} type1 = SimpleTokenType('cat') \\
\tt\textbf{>>>} type2 = SimpleTokenType('dog') \\
\tt\textbf{>>>} loc = SimpleLocation(0) \\
\tt\textbf{>>>} token1 = Token(type1, loc) \\
\tt\textbf{>>>} token2 = Token(type2) \\
\tt\textbf{>>>} print type1, type2, loc, token1, token2 \\
\tt\textbf{>>>} print token1.type(), token1.loc() \\
\tt\textbf{>>>} print type1.word() \\
\tt\textbf{>>>} print loc.index() \\
\end{tabbing}

\subsection{Tokenizing Text}

``Tokenizing'' is the task of separating a text file into tokens.  The 
toolkit defines a number of tokenizers, which divide up text files in
different ways.  Most of these tokenizers can be used to tokenize a
file as follows:

\noindent\begin{tabbing}
\tt\textbf{>>>} text = open(\textrm{\textit{filename}}).read() \\
\tt\textbf{>>>} tokens = \textrm{\textit{Tokenizer}}().tokenize(text)
\end{tabbing}

\noindent
Where \textit{Tokenizer} is a specific tokenzier, and
\textit{filename} is the name of a text file. Tokenize the file
\texttt{words.txt} using \texttt{SimpleTokenizer}. 

\noindent\begin{tabbing}
\tt\textbf{>>>} text = open("words.txt").read() \\
\tt\textbf{>>>} tokens = SimpleTokenizer().tokenize(text)
\end{tabbing}

\noindent
This will produce \texttt{Token}s with \texttt{SimpleTokenType}s.

Read about \texttt{Token}, \texttt{SimpleTokenType},
\texttt{IndexLocation}, and \texttt{SimpleTokenizer} in the reference
documentation.  Try experimenting with some of the tokens in
\texttt{tokens}.

\subsection{Tagged Tokens}

Token are often ``tagged'' with additional information, such as their
part of speech.  The \texttt{TaggedTokenType} class is used to
associate tags with words.  Try creating a few tagged tokens:

\noindent\begin{tabbing}
\tt\textbf{>>>} walk\_nn = TaggedTokenType('walk', 'nn') \\
\tt\textbf{>>>} walk\_vb = TaggedTokenType('walk', 'vb') \\
\tt\textbf{>>>} loc = SimpleLocation(0) \\
\tt\textbf{>>>} walk\_vb\_token = Token(walk\_vb, loc)
\end{tabbing}

The file \texttt{tagged.txt} contains text that has been tagged with
part of speech information.  We can separate it into tagged tokens
using a \texttt{TaggedTokenizer}:

\noindent\begin{tabbing}
\tt\textbf{>>>} text = open("tagged.txt").read() \\
\tt\textbf{>>>} tagged\_tokens = TaggedTokenizer().tokenize(text)
\end{tabbing}

Try experimenting with some of the tokens in \texttt{tagged\_tokens}. 

\subsection{Advanced Problem: Tokenizers}

Create a new tokenizer that normalizes all tokens to lower case.  You
can use an existing tokenizer, or start from scratch.  Your tokenizer
should be have \texttt{TokenizerI} as one of its bases.

\subsection{Advanced Problem: Tokenizers}

Create a new tokenizer that separates punctuation into its own tokens,
when appropriate.  You can use an existing tokenizer, or start from
scratch.  Your tokenizer should be have \texttt{TokenizerI} as one of
its bases.

\section{Frequency Distributions}

The toolkit provides special classes to encapsulate frequency
distributions.

\subsection{Coin Toss}

Frequency distributions can be used to model a wide variety of
phenomena.  To begin with, we will frequency distributions to examine
a simple coin toss.  First, you will need to import \texttt{randint},
which we will use to simulate coin tosses:

\noindent\begin{tabbing}
\tt\textbf{>>>} from random import randint \\
\tt\textbf{>>>} from nltk.tagger import *
\end{tabbing}

Construct a new frequency distribution, and use it to model 100 coin
tosses: 

\noindent\begin{tabbing}
\tt\textbf{>>>} fdist = SimpleFreqDist()
    \qquad\qquad\qquad
    \= \textrm{\# Create a new frequency distribution} \\
\tt\textbf{>>>} for i in range(100):\\
\tt\textbf{>>>} \qquad x = randint(1, 6)
    \>\textrm{\# Get a random number from 1 to 6} \\
\tt\textbf{>>>} \qquad fdist.inc(x)
    \>\textrm{\# Increment the count for the sample \textit{x}} \\
\end{tabbing}

\noindent
Now, we can examine a number of properties of the frequency
distribution: 

\noindent\begin{tabbing}
\tt\textbf{>>>} print fdist.N()
    \qquad\qquad\qquad\qquad
    \=\textrm{\# The number of die rolls} \\
\tt\textbf{>>>} print fdist.count(2)
    \>\textrm{\# The number of die rolls that came up 2} \\
\tt\textbf{>>>} print fdist.freq(2)
    \>\textrm{\# The percentage of die rolls that came up 2} \\
\tt\textbf{>>>} print fdist.max()
    \>\textrm{\# The most frequent die roll} \\
\end{tabbing}

\noindent
We can use events to ask more complex questions about the frequency
distribution: 

{\small
\noindent\begin{tabbing}
\tt\textbf{>>>} die\_lt\_4 = SetEvent(1, 2, 3)
    \qquad\qquad\qquad
    \=\textrm{\# The event that a die came up less than 4} \\
\tt\textbf{>>>} die\_even = SetEvent(2, 4, 6)
    \>\textrm{\# The event that a die came up with an even number} \\
\tt\textbf{>>>} print fdist.freq(die\_even)
    \>\textrm{\# The percentage of die rolls that came up even} \\
\tt\textbf{>>>} print fdist.cond\_max(die\_lt\_4)
    \>\textrm{\# The most frequent die roll, of those rolls less than 4} \\
\tt\textbf{>>>} print fdist.cond\_freq(die\_even, die\_lt\_4)
    \>\textrm{\# The conditional frequency with which a die came up}\\
    \>\textrm{\# \qquad even, given that it came up less than 4}
\end{tabbing}}

\noindent
Read about \texttt{SimpleFreqDist} and \texttt{SetEvent} in the
reference documentation.

\subsection{Words}

In this problem, we will use a frequency distribution to examine the
words in \texttt{words.txt}.  First, create a new frequency
distribution, and count up the frequency of every word's token type:

\noindent\begin{tabbing}
\tt\textbf{>>>} word\_fdist = SimpleFreqDist()
    \qquad\qquad\qquad
    \= \textrm{\# Create a new frequency distribution} \\
\tt\textbf{>>>} for token in tokens:\\
\tt\textbf{>>>} \qquad word\_fdist.inc(token.type())
    \>\textrm{\# Increment the count for this token's type}
\end{tabbing}

\noindent
Note that we must be careful to add \emph{token types}, and not
\emph{tokens}, to the frequency distribution.

Use this frequency distribution to answer the following questions: 
\begin{itemize}
  \item What is the most frequent token type?
  \item How many tokens are in \texttt{words.txt}
  \item How many token types are in \texttt{words.txt} \textit{(n.b.,
        you can't currently answer this -- FreqDist will be extended
        later.)}
  \item How many times does the token type ``you'' appear?
  \item What percentage of the tokens are the pronouns ``I'', ``you'',
        ``he'', ``she'', or ``it''?
  \item What percentage of the tokens are more than 4 letters long?
  \item Of the tokens that are more than 6 letters long, what
        percentage contain the letter ``a''?
\end{itemize}

\noindent
To answer the last two questions, you will need to construct special
events.  For now, don't worry about how these work.

\noindent\begin{tabbing}
\tt\textbf{>>>} more\_than\_4\_letters = FuncEvent(lambda x:len(x.word())>4)\\
\tt\textbf{>>>} more\_than\_6\_letters = FuncEvent(lambda x:len(x.word())>6)\\
\tt\textbf{>>>} contains\_a = FuncEvent(lambda x:'a' in x.word())
\end{tabbing}

\noindent
Experiment with determining other properties using frequency
distributions.  \textit{(In the future, you'll be able to examine
zipfs law here.  But FreqDists aren't advanced enough yet)}

\subsection{CFSample}

The \texttt{CFSample} class defines a sample that consists of two
parts: a context, and a feature.  Modeling features this way is useful
for a number of problems.  Experiment with the \texttt{CFSample}
class.  Try building a frequency distribution from
\texttt{tagged\_tokens}, consisting of \texttt{CFSample}s whose
context is the \texttt{TokenType}'s word; and whose feature is the
\texttt{TokenType}'s tag.

\section{Tagging}

Use a frequency distribution to construct a zeroth order tagger.  The
frequency distribution should have tagged tokens as its samples.  To
decide which token to assign, use \texttt{cond\_max}.  This will be
easiest to do if you use the CFSample class, which defines samples
that consist of a context and a feature.  In this case, the context
will be the token type's word; and the feature will be its tag.

\end{document}

