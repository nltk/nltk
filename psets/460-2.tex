\documentclass{460}
\assignment{2}{Tagging \& Chunking}{11am Monday 1 September 2003}
\usepackage{url}

\begin{document}
\maketitle

\section{Rule-Based Tagging}

Please read the tagging tutorial.
NLTK defines the \texttt{NN\_CD\_Tagger}, which is typically used as a
``fall-back'' tagger for unknown words.  It tags tokens as either
numbers (``CD'') or nouns (``NN''), by checking whether their type
matches a simple regular expression.  Although \texttt{NN\_CD\_Tagger}
has decent performance, there is a lot of room for improvement.  For
example, we could tag any word that ends with ``-s'' as a plural noun.
Propose four new rules that could be used to tag unknown words, based
on the contents of their type.
Write a new tagger named \texttt{MyTagger} which implements these new
rules.  Your code should have the form:

\begin{verbatim}
class MyTagger(TaggerI):
    def __init__(self):
        ...
    def tag(self, tokens):
        ...
\end{verbatim}

You can use \texttt{nltk.tagger.accuracy()}
to test the performance of your new tagger.  Since it is generally
advisable to test a system on different data than you train it on,
set aside the first 100 tokens of your training text
for testing.  Remember to set your
\texttt{NLTK\_CORPORA} environment variable to
\url{/home/subjects/460/local/nltk/data} before using the
\texttt{nltk.corpus} module.
To test your tagger use a function like this:

\begin{verbatim}
def p1():
    tokens = []
    for text in brown.items('fiction: romance'): # or some other group from Brown
        tokens += brown.tokenize(text)
        # add some sentence-boundary tags?
    tagger = MyTagger()
    tagger.train(tokens[100:])
    result = tagger.tag(untag(tokens[:100]))
    print 'Accuracy', accuracy(tokens[:100], result)
\end{verbatim}

Use inline documentation to describe your method and list its advantages
and disadvantages.

\section{Probabilistic Tagging}

NLTK defines the \texttt{NthOrderTagger}, which uses a token's base
type and the predicted tags of the previous $n$ tokens to help predict
each new tag.  In other words, it uses the following context to
predict the tag for token $t_i$:
$\langle t_i.basetype, t_{i-1}.tag, \ldots, t_{i-n}.tag\rangle$.
Using this context, \texttt{NthOrderTagger} can tag texts with
relatively high accuracy.  However, it is possible to define taggers
that use many other contexts.  Some examples of alternative contexts
which might yield good results are:
$\langle t_i.basetype, t_{i-1}.basetype\rangle$,
$\langle t_i.basetype, t_{i+1}.basetype\rangle$,
$\langle t_i.basetype, t_{i-1}.basetype, t_{i-1}.tag\rangle$,
$\langle Set(t_{i-2}.basetype, \ldots, t_{i+2}.basetype)\rangle$.

Choose one of these contexts, or come up with your own, and use it to
write a probabilistic tagger named \texttt{MyPrTagger}, including
\texttt{train()} and \texttt{tag()} methods.
Write a function \texttt{p2()} to run your tagger and
report its performance.  Use inline documentation to describe
your method and list its advantages and disadvantages.

\section{Backoff Taggers}

Experiment with combining your tagger with other taggers, using
\textit{BackoffTagger}.  Try to find a combination of taggers that
maximizes accuracy.  Write a function \texttt{p3()} to report
accuracy, and use inline documentation to discuss your work.

\section{Chunking}

Please review the chunking tutorial.
Develop a noun phrase chunker for the chunking corpus
(\url{nltk/data/chunking}) using the regular-expression
chunk parser \texttt{REChunkParser}.  Use any
combination of rules (\texttt{ChunkRule}, \texttt{ChinkRule},
\texttt{UnChunkRule}, \texttt{MergeRule}, \texttt{SplitRule}, and
\texttt{REChunkParserRule}).
Define a function \texttt{p4()}, which reports the precision,
recall and F-measure for your chunker applied to 100 sentences,
and use inline documentation to discuss your work.

Load a sample of the CoNNL data using the following code:

\begin{verbatim}
from nltk.corpus import chunking
from nltk.tree import TreeToken
samplesize = 10 # small for development
sentences = chunking.tokenize('train.txt')  # NB also test.txt
chunk_tokenizer = ConllChunkedTokenizer(['NP'])
c_sents = [TreeToken('S', *chunk_tokenizer.tokenize(s.type()))
           for s in sentences[:samplesize]]
\end{verbatim}

This produces a sample of 10 chunked sentences \texttt{c\_sents}.  We will
extract the word tokens, input them to the chunker, then compare the resulting
chunked sentences with the originals, as follows:

\begin{verbatim}
from nltk.parser.chunk import *

chunkscore = ChunkScore()

# simple rule, to be replaced by a cascade: rule1, rule2, ...
rule1 = ChunkRule('<PRP|DT|POS|JJ|CD|N.*>+', "Chunk items that often occur in NPs")
chunkparser = REChunkParser([rule1], 'NP', 'S')

for c_sent in c_sents:                   # for each chunked sentence
    tokens = c_sent.leaves()             # extract word tokens
    guess = chunkparser.parse(tokens)    # chunk them
    chunkscore.score(c_sent, guess)      # compare with original

print chunkscore
\end{verbatim}

The \texttt{nltk.parser.chunk} module supports the development of
chunkers in two ways.  First, you can use the \texttt{trace=1}
optional argument of the \texttt{REChunkParser.parse()} method to
show the chunks after each rule has been applied.  Second, you can use
the \texttt{chunkscore.incorrect()} and \texttt{chunkscore.missed()}
methods to report any false positive and false negative chunks
(see the chunking tutorial for more details).

\section*{Submission}

Submission will be in the form of a single Python program and submit
it using \texttt{submit 460 2 username.py}.
Points will be awarded for well-structured and
well-documented code, and insightful discussion.  Points will be
deducted for late submissions (20\% per day or part thereof).
Assignments must be strictly original work.

\end{document}