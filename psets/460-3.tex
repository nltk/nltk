\documentclass{460}
\assignment{3}{Parsing}{11am Monday 15 September 2003}

\begin{document}
\maketitle

\section{Left-Corner Parser}

A left-corner parser is a top-down parser with bottom-up filtering
(J\&M \S 10).  Develop a new parser class \texttt{LeftCornerParser}
(inheriting from
\texttt{ParserI}) which implements a left-corner parser.  Write a
function \texttt{p1a()} to apply your parser to three sentences.

Next, compare the performance of this parser with
\texttt{ShiftReduceParser()} and \texttt{RecursiveDescentParser()},
for the same sentences.  Log the time taken by each parser on
each sentence using the \texttt{time} module, e.g.:

\begin{verbatim}
import time
start = time.time()
parser.parse(sentence)
end = time.time()
processing_time = end - start
\end{verbatim}

Write a function \texttt{p1b()} which runs all three parsers on all
three sentences, and prints a 3x3 grid of times, as well as row and
column totals.  Use inline comments to discuss your findings.

\section{Chart Parser}

The \texttt{draw/chart} module has a graphical interface which can be
accessed by running \texttt{python} \url{/home/subjects/460/local/nltk/draw/chart.py}.
Use this interface to experiment with rule invocation
strategies.  Design your own strategy and execute it
manually using the graphical interface.  List the steps in your
strategy, and report any efficiency improvements it has (e.g.~in terms
of the size of the resulting chart).  Do these improvements depend on
the structure of the grammar?  What do you think of the prospects for
significant performance boosts from cleverer strategies?  Report your
work using inline comments (marked clearly as question 2).

\pagebreak

\section{Grammar Development}

Pick some of the syntactic constructions described in J\&M \S 10
and create a set of 30 sentences.  Ten sentences should be unambiguous (have
a unique parse), ten should be ambiguous, and a further ten should be
ungrammatical.  Develop a grammar which accounts for this data,
providing trees for the grammatical ones and rejecting the ungrammatical ones,
refining your test set as needed to test the grammar.  Write
a function \texttt{p3()} to demonstrate your grammar on three sentences:
(i) a sentence having exactly one parse;
(ii) a sentence having more than one parse;
(iii) a sentence having no parses.

Next, create a list \texttt{words} of all the words in your lexicon, and use
\texttt{random.choice(words)} to generate sequences of 6-12 randomly
selected words.  Report any grammatical sentences which your grammar
rejects, and any ungrammatical sentences which your grammar accepts.
Discuss your observations using inline comments.

\section{Language Modelling}

Develop an $n$-gram language model incorporating backoff and smoothing.
Train two separate instances of the model on two sets of documents from
different genres.  Possible sources include newsgroup postings in
the \texttt{twenty\_newsgroups} corpus, or text from the \texttt{gutenberg}
corpus (both supported by the \texttt{nltk.corpus} module).

Use the models to generate random text as follows.  Suppose words
$..., w_{i-1}, w_{i}$ have already been generated.  Use this $n$-gram
to probe the language model for a probability distribution over the
possible following words.  Choose the next word $w_{i+1}$ randomly from
among the most likely words.

Write two functions, \texttt{p4a()} and \texttt{p4b()}, each
generating 100 words of random text according to the two models.  How
intelligible is the text you generate?  What are some novel
expressions your system creates?  Are the different genres clearly
reflected in the text?  Discuss any observations, and report your
findings using inline comments.

\section*{Submission}

Submission will be in the form of a single Python program, submitted
using \texttt{submit 460 3 username.py}.  Points will be awarded for
well-structured and well-documented code.  Points will be deducted for
late submissions (20\% per day or part thereof).  Assignments must be
strictly original work.

\end{document}
