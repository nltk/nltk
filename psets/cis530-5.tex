\documentclass{cis530}
\usepackage{url}
\num{5}\due{1:30pm Wednesday 31 October 2001}

\begin{document}
\maketitle

Please read the \emph{Chunking} tutorial.

{\bf Accessing NLTK.}  
To access the centrally installed version of NLTK on unagi you will
need to have\\ \texttt{/spd25/cis530/nltk/src} in your PYTHONPATH (see
Lutz p. 20).  If you are working from your own copy of NLTK, it will
be necessary to download the latest version (0.4); please see the
course website for details.

{\bf Grading.}
Points will be awarded for well-structured and well-documented code,
and for original and insightful discussion.
Points will be deducted for late submissions.  Assignments must be
strictly original work.

{\bf Submission.}  
Please email your completed assignment as a
\emph{single Python program} to both instructors
\texttt{sb@cis, edloper@gradient.cis}, as a MIME attachment
(mime type: \texttt{text/plain}).
\vspace{2ex}

%% Notes:
%% - We should adjust the ``95%'' figure to something that is not trivial but
%%   that shouldn't take more than 15-20 minutes to find.
%% - Replace n with an appropriate value in ``pn''
%% - Rewrite ``contest'' section, depending on what we want to do.

\section{Chunk Parsing}
  % This problem is intended to reinforce the concepts behind
  % ChunkParsing, and to provide students with a chance to play with
  % different chunk parsing techniques.  It is intended to take an hour
  % to complete, once the student understands the basic concepts.
  %
  % The problem also includes an optional contest, which gives the
  % students some incentive to go beyond the basics, and really get
  % some experience as to what works and what doesn't.

  The following code creates and tests a simple chunk parser for
  finding noun phrase chunks.

\begin{verbatim}
from nltk.token import LineTokenizer
from nltk.chunkparser import ChunkedTaggedTokenizer, ChunkScore, unchunk
from nltk.rechunkparser import *

# Create the chunk parser.
rule1 = ChunkRule('<DT|JJ|NN>+', "Chunk sequences of DT, JJ, and NN")
chunkparser = REChunkParser( [rule1] )

# Read the test corpus.
text = open('testcorpus.txt').read()
sentences = LineTokenizer().tokenize(text)

# Test the chunk parser.
chunkscore = ChunkScore()
ctt = ChunkedTaggedTokenizer()
for sentence in sentences:
    correct = ctt.tokenize(sentence.type(), source=sentence.loc())
    unchunked = unchunk(correct)
    guess = chunkparser.parse(unchunked)
    chunkscore.score(correct, guess)

print chunkscore
\end{verbatim}

  Create and test a more advanced \texttt{REChunkParser} for finding
  NP chunks.  Your chunk parser should achieve an F-Measure value of
  at least 95\% on the example corpus.  Your parser can use any
  combination of rules (\texttt{ChunkRule}, \texttt{ChinkRule},
  \texttt{UnChunkRule}, \texttt{MergeRule}, \texttt{SplitRule}, and
  \texttt{REChunkParserRule}).

  Define a function \textit{pn}, which returns your parser.

\section{Contest (Optional)}

  We will be testing your chunk parsers on an unpublished section of
  the Wall Street Journal, and ranking them according to the
  F-Measure.  Students in the first three places will receive fame
  and fortune (and also a prize).

  We recommend that you avoid overfitting by using your own
  \emph{development test set} and \emph{final test set}.
  Please see Manning 206ff for details.

  \emph{Note that the performance of your chunk parser on the unpublished
  test corpus will have no effect on your grade.}

\end{document}
