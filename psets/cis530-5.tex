\documentclass{cis530}
\usepackage{url}
\num{5}\due{1:30pm Wednesday 31 October 2001}

\begin{document}
\maketitle

Please read the \emph{Chunking} tutorial.

{\bf Accessing NLTK.}  
To access the centrally installed version of NLTK on unagi you will
need to have\\ \texttt{/spd25/cis530/nltk/src} in your PYTHONPATH (see
Lutz p. 20).  If you are working from your own copy of NLTK, it will
be necessary to download the latest version (0.41); please see the
course website for details.

{\bf Grading.}
Points will be awarded for well-structured and well-documented code,
and for original and insightful discussion.
Points will be deducted for late submissions.  Assignments must be
strictly original work.

{\bf Submission.}  
Please email your completed assignment as a
\emph{single Python program} to both instructors
\texttt{sb@cis, edloper@gradient.cis}, as a MIME attachment
(mime type: \texttt{text/plain}).
\vspace{2ex}

%% Notes:
%% - We should adjust the ``95%'' figure to something that is not trivial but
%%   that shouldn't take more than 15-20 minutes to find.
%% - Replace n with an appropriate value in ``pn''
%% - Rewrite ``contest'' section, depending on what we want to do.

\section{Chunk Parsing}
  % This problem is intended to reinforce the concepts behind
  % ChunkParsing, and to provide students with a chance to play with
  % different chunk parsing techniques.  It is intended to take an hour
  % to complete, once the student understands the basic concepts.
  %
  % The problem also includes an optional contest, which gives the
  % students some incentive to go beyond the basics, and really get
  % some experience as to what works and what doesn't.

  The chunking tutorial contains example code for a simple NP
  chunk parser.
  Create and test a more advanced \texttt{REChunkParser} instance
  to find NP chunks.

  Use data from the WSJ corpus on the resources CDROM, or on
  unagi (\texttt{/spd25/cis530/cdrom/resources/data/wsj/tagged/}).
  Your task is to model this messy data, not to
  implement a linguistic theory about noun phrases.

  You should aim to get an F-Measure score of at least
  85\% on the training data (files 
  \texttt{wsj\_0001.pos} to \texttt{wsj\_0099.pos}).
  Your parser can use any
  combination of rules (\texttt{ChunkRule}, \texttt{ChinkRule},
  \texttt{UnChunkRule}, \texttt{MergeRule}, \texttt{SplitRule}, and
  \texttt{REChunkParserRule}).

  Define a function \textit{pn}, which returns your parser.

\section{Contest (Optional)}

  We will be testing your chunk parsers on an unpublished section of
  the Wall Street Journal, and ranking them according to the
  F-Measure.  Students in first and second places will receive fame
  and fortune, and also a prize!

  We recommend that you avoid overfitting by using your own
  \emph{development test set} and \emph{final test set}.
  Please see Manning 206ff for details.

  Additional chunking data has been added to the online resources
  directory, files \texttt{wsj\_0100.pos} to \texttt{wsj\_0399.pos}.

  \emph{Note that the performance of your chunk parser on the unpublished
  test corpus will have no effect on your grade.}

\end{document}
