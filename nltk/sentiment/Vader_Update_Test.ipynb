{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "# Natural Language Toolkit: vader\n",
    "#\n",
    "# Copyright (C) 2001-2019 NLTK Project\n",
    "# Author: C.J. Hutto <Clayton.Hutto@gtri.gatech.edu>\n",
    "#         Ewan Klein <ewan@inf.ed.ac.uk> (modifications)\n",
    "#         Pierpaolo Pantone <24alsecondo@gmail.com> (modifications)\n",
    "#         George Berry <geb97@cornell.edu> (modifications)\n",
    "#         Malavika Suresh <malavika.suresh0794@gmail.com> (modifications)\n",
    "# URL: <http://nltk.org/>\n",
    "# For license information, see LICENSE.TXT\n",
    "#\n",
    "# Modifications to the original VADER code have been made in order to\n",
    "# integrate it into NLTK. These have involved changes to\n",
    "# ensure Python 3 compatibility, and refactoring to achieve greater modularity.\n",
    "\n",
    "\"\"\"\n",
    "If you use the VADER sentiment analysis tools, please cite:\n",
    "\n",
    "Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for\n",
    "Sentiment Analysis of Social Media Text. Eighth International Conference on\n",
    "Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import re\n",
    "import string\n",
    "from itertools import product\n",
    "import nltk.data\n",
    "from util import pairwise\n",
    "\n",
    "##Constants##\n",
    "\n",
    "# (empirically derived mean sentiment intensity rating increase for booster words)\n",
    "B_INCR = 0.293\n",
    "B_DECR = -0.293\n",
    "\n",
    "# (empirically derived mean sentiment intensity rating increase for using\n",
    "# ALLCAPs to emphasize a word)\n",
    "C_INCR = 0.733\n",
    "\n",
    "N_SCALAR = -0.74\n",
    "\n",
    "# for removing punctuation\n",
    "REGEX_REMOVE_PUNCTUATION = re.compile('[{0}]'.format(re.escape(string.punctuation)))\n",
    "\n",
    "PUNC_LIST = [\n",
    "    \".\",\n",
    "    \"!\",\n",
    "    \"?\",\n",
    "    \",\",\n",
    "    \";\",\n",
    "    \":\",\n",
    "    \"-\",\n",
    "    \"'\",\n",
    "    \"\\\"\",\n",
    "    \"!!\",\n",
    "    \"!!!\",\n",
    "    \"??\",\n",
    "    \"???\",\n",
    "    \"?!?\",\n",
    "    \"!?!\",\n",
    "    \"?!?!\",\n",
    "    \"!?!?\",\n",
    "]\n",
    "NEGATE = {\n",
    "    \"aint\",\n",
    "    \"arent\",\n",
    "    \"cannot\",\n",
    "    \"cant\",\n",
    "    \"couldnt\",\n",
    "    \"darent\",\n",
    "    \"didnt\",\n",
    "    \"doesnt\",\n",
    "    \"ain't\",\n",
    "    \"aren't\",\n",
    "    \"can't\",\n",
    "    \"couldn't\",\n",
    "    \"daren't\",\n",
    "    \"didn't\",\n",
    "    \"doesn't\",\n",
    "    \"dont\",\n",
    "    \"hadnt\",\n",
    "    \"hasnt\",\n",
    "    \"havent\",\n",
    "    \"isnt\",\n",
    "    \"mightnt\",\n",
    "    \"mustnt\",\n",
    "    \"neither\",\n",
    "    \"don't\",\n",
    "    \"hadn't\",\n",
    "    \"hasn't\",\n",
    "    \"haven't\",\n",
    "    \"isn't\",\n",
    "    \"mightn't\",\n",
    "    \"mustn't\",\n",
    "    \"neednt\",\n",
    "    \"needn't\",\n",
    "    \"never\",\n",
    "    \"none\",\n",
    "    \"nope\",\n",
    "    \"nor\",\n",
    "    \"not\",\n",
    "    \"nothing\",\n",
    "    \"nowhere\",\n",
    "    \"oughtnt\",\n",
    "    \"shant\",\n",
    "    \"shouldnt\",\n",
    "    \"uhuh\",\n",
    "    \"wasnt\",\n",
    "    \"werent\",\n",
    "    \"oughtn't\",\n",
    "    \"shan't\",\n",
    "    \"shouldn't\",\n",
    "    \"uh-uh\",\n",
    "    \"wasn't\",\n",
    "    \"weren't\",\n",
    "    \"without\",\n",
    "    \"wont\",\n",
    "    \"wouldnt\",\n",
    "    \"won't\",\n",
    "    \"wouldn't\",\n",
    "    \"rarely\",\n",
    "    \"seldom\",\n",
    "    \"despite\"\n",
    "}\n",
    "\n",
    "# booster/dampener 'intensifiers' or 'degree adverbs'\n",
    "# http://en.wiktionary.org/wiki/Category:English_degree_adverbs\n",
    "\n",
    "BOOSTER_DICT = {\n",
    "    \"absolutely\": B_INCR,\n",
    "    \"amazingly\": B_INCR,\n",
    "    \"awfully\": B_INCR,\n",
    "    \"completely\": B_INCR,\n",
    "    \"considerably\": B_INCR,\n",
    "    \"decidedly\": B_INCR,\n",
    "    \"deeply\": B_INCR,\n",
    "    \"effing\": B_INCR,\n",
    "    \"enormously\": B_INCR,\n",
    "    \"entirely\": B_INCR,\n",
    "    \"especially\": B_INCR,\n",
    "    \"exceptionally\": B_INCR,\n",
    "    \"extremely\": B_INCR,\n",
    "    \"fabulously\": B_INCR,\n",
    "    \"flipping\": B_INCR,\n",
    "    \"flippin\": B_INCR,\n",
    "    \"fricking\": B_INCR,\n",
    "    \"frickin\": B_INCR,\n",
    "    \"frigging\": B_INCR,\n",
    "    \"friggin\": B_INCR,\n",
    "    \"fully\": B_INCR,\n",
    "    \"fucking\": B_INCR,\n",
    "    \"greatly\": B_INCR,\n",
    "    \"hella\": B_INCR,\n",
    "    \"highly\": B_INCR,\n",
    "    \"hugely\": B_INCR,\n",
    "    \"incredibly\": B_INCR,\n",
    "    \"intensely\": B_INCR,\n",
    "    \"majorly\": B_INCR,\n",
    "    \"more\": B_INCR,\n",
    "    \"most\": B_INCR,\n",
    "    \"particularly\": B_INCR,\n",
    "    \"purely\": B_INCR,\n",
    "    \"quite\": B_INCR,\n",
    "    \"really\": B_INCR,\n",
    "    \"remarkably\": B_INCR,\n",
    "    \"so\": B_INCR,\n",
    "    \"substantially\": B_INCR,\n",
    "    \"thoroughly\": B_INCR,\n",
    "    \"totally\": B_INCR,\n",
    "    \"tremendously\": B_INCR,\n",
    "    \"uber\": B_INCR,\n",
    "    \"unbelievably\": B_INCR,\n",
    "    \"unusually\": B_INCR,\n",
    "    \"utterly\": B_INCR,\n",
    "    \"very\": B_INCR,\n",
    "    \"almost\": B_DECR,\n",
    "    \"barely\": B_DECR,\n",
    "    \"hardly\": B_DECR,\n",
    "    \"just enough\": B_DECR,\n",
    "    \"kind of\": B_DECR,\n",
    "    \"kinda\": B_DECR,\n",
    "    \"kindof\": B_DECR,\n",
    "    \"kind-of\": B_DECR,\n",
    "    \"less\": B_DECR,\n",
    "    \"little\": B_DECR,\n",
    "    \"marginally\": B_DECR,\n",
    "    \"occasionally\": B_DECR,\n",
    "    \"partly\": B_DECR,\n",
    "    \"scarcely\": B_DECR,\n",
    "    \"slightly\": B_DECR,\n",
    "    \"somewhat\": B_DECR,\n",
    "    \"sort of\": B_DECR,\n",
    "    \"sorta\": B_DECR,\n",
    "    \"sortof\": B_DECR,\n",
    "    \"sort-of\": B_DECR,\n",
    "    \"only\": B_DECR\n",
    "}\n",
    "\n",
    "# check for special case idioms using a sentiment-laden keyword known to SAGE\n",
    "SPECIAL_CASE_IDIOMS = {\n",
    "    \"the shit\": 3,\n",
    "    \"the bomb\": 3,\n",
    "    \"bad ass\": 1.5,\n",
    "    \"yeah right\": -2,\n",
    "    \"cut the mustard\": 2,\n",
    "    \"kiss of death\": -1.5,\n",
    "    \"hand to mouth\": -2,\n",
    "}\n",
    "\n",
    "\n",
    "##Static methods##\n",
    "\n",
    "\n",
    "def negated(input_words, include_nt=True):\n",
    "    \"\"\"\n",
    "    Determine if input contains negation words\n",
    "    \"\"\"\n",
    "    neg_words = NEGATE\n",
    "    if any(word.lower() in neg_words for word in input_words):\n",
    "        return True\n",
    "    if include_nt:\n",
    "        if any(\"n't\" in word.lower() for word in input_words):\n",
    "            return True\n",
    "    for first, second in pairwise(input_words):\n",
    "        if second.lower() == \"least\" and first.lower() != 'at':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def normalize(score, alpha=15):\n",
    "    \"\"\"\n",
    "    Normalize the score to be between -1 and 1 using an alpha that\n",
    "    approximates the max expected value\n",
    "    \"\"\"\n",
    "    norm_score = score / math.sqrt((score * score) + alpha)\n",
    "    return norm_score\n",
    "\n",
    "\n",
    "def allcap_differential(words):\n",
    "    \"\"\"\n",
    "    Check whether just some words in the input are ALL CAPS\n",
    "\n",
    "    :param list words: The words to inspect\n",
    "    :returns: `True` if some but not all items in `words` are ALL CAPS\n",
    "    \"\"\"\n",
    "    is_different = False\n",
    "    allcap_words = 0\n",
    "    for word in words:\n",
    "        if word.isupper():\n",
    "            allcap_words += 1\n",
    "    cap_differential = len(words) - allcap_words\n",
    "    if 0 < cap_differential < len(words):\n",
    "        is_different = True\n",
    "    return is_different\n",
    "\n",
    "\n",
    "def scalar_inc_dec(word, valence, is_cap_diff):\n",
    "    \"\"\"\n",
    "    Check if the preceding words increase, decrease, or negate/nullify the\n",
    "    valence\n",
    "    \"\"\"\n",
    "    scalar = 0.0\n",
    "    word_lower = word.lower()\n",
    "    if word_lower in BOOSTER_DICT:\n",
    "        scalar = BOOSTER_DICT[word_lower]\n",
    "        if valence < 0:\n",
    "            scalar *= -1\n",
    "        # check if booster/dampener word is in ALLCAPS (while others aren't)\n",
    "        if word.isupper() and is_cap_diff:\n",
    "            if valence > 0:\n",
    "                scalar += C_INCR\n",
    "            else:\n",
    "                scalar -= C_INCR\n",
    "    return scalar\n",
    "\n",
    "\n",
    "class SentiText(object):\n",
    "    \"\"\"\n",
    "    Identify sentiment-relevant string-level properties of input text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text):\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text.encode('utf-8'))\n",
    "        self.text = text\n",
    "        self.words_and_emoticons = self._words_and_emoticons()\n",
    "        # doesn't separate words from\\\n",
    "        # adjacent punctuation (keeps emoticons & contractions)\n",
    "        self.is_cap_diff = allcap_differential(self.words_and_emoticons)\n",
    "\n",
    "    def _words_plus_punc(self):\n",
    "        \"\"\"\n",
    "        Returns mapping of form:\n",
    "        {\n",
    "            'cat,': 'cat',\n",
    "            ',cat': 'cat',\n",
    "        }\n",
    "        \"\"\"\n",
    "        no_punc_text = REGEX_REMOVE_PUNCTUATION.sub('', self.text)\n",
    "        # removes punctuation (but loses emoticons & contractions)\n",
    "        words_only = no_punc_text.split()\n",
    "        # remove singletons\n",
    "        words_only = set(w for w in words_only if len(w) > 1)\n",
    "        # the product gives ('cat', ',') and (',', 'cat')\n",
    "        punc_before = {''.join(p): p[1] for p in product(PUNC_LIST, words_only)}\n",
    "        punc_after = {''.join(p): p[0] for p in product(words_only, PUNC_LIST)}\n",
    "        words_punc_dict = punc_before\n",
    "        words_punc_dict.update(punc_after)\n",
    "        return words_punc_dict\n",
    "\n",
    "    def _words_and_emoticons(self):\n",
    "        \"\"\"\n",
    "        Removes leading and trailing puncutation\n",
    "        Leaves contractions and most emoticons\n",
    "            Does not preserve punc-plus-letter emoticons (e.g. :D)\n",
    "        \"\"\"\n",
    "        wes = self.text.split()\n",
    "        words_punc_dict = self._words_plus_punc()\n",
    "        wes = [we for we in wes if len(we) > 1]\n",
    "        for i, we in enumerate(wes):\n",
    "            if we in words_punc_dict:\n",
    "                wes[i] = words_punc_dict[we]\n",
    "        return wes\n",
    "\n",
    "\n",
    "class SentimentIntensityAnalyzer(object):\n",
    "    \"\"\"\n",
    "    Give a sentiment intensity score to sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, lexicon_file=\"sentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\"\n",
    "    ):\n",
    "        self.lexicon_file = nltk.data.load(lexicon_file)\n",
    "        self.lexicon = self.make_lex_dict()\n",
    "\n",
    "    def make_lex_dict(self):\n",
    "        \"\"\"\n",
    "        Convert lexicon file to a dictionary\n",
    "        \"\"\"\n",
    "        lex_dict = {}\n",
    "        for line in self.lexicon_file.split('\\n'):\n",
    "            (word, measure) = line.strip().split('\\t')[0:2]\n",
    "            lex_dict[word] = float(measure)\n",
    "        return lex_dict\n",
    "\n",
    "    def polarity_scores(self, text):\n",
    "        \"\"\"\n",
    "        Return a float for sentiment strength based on the input text.\n",
    "        Positive values are positive valence, negative value are negative\n",
    "        valence.\n",
    "        \"\"\"\n",
    "        sentitext = SentiText(text)\n",
    "        # text, words_and_emoticons, is_cap_diff = self.preprocess(text)\n",
    "\n",
    "        sentiments = []\n",
    "        words_and_emoticons = sentitext.words_and_emoticons\n",
    "        for item in words_and_emoticons:\n",
    "            valence = 0\n",
    "            i = words_and_emoticons.index(item)\n",
    "            if (\n",
    "                i < len(words_and_emoticons) - 1\n",
    "                and item.lower() == \"kind\"\n",
    "                and words_and_emoticons[i + 1].lower() == \"of\"\n",
    "            ) or item.lower() in BOOSTER_DICT:\n",
    "                sentiments.append(valence)\n",
    "                continue\n",
    "\n",
    "            sentiments = self.sentiment_valence(valence, sentitext, item, i, sentiments)\n",
    "\n",
    "        sentiments = self._but_check(words_and_emoticons, sentiments)\n",
    "        \n",
    "        sentiments = self._only_if_check(words_and_emoticons, sentiments)\n",
    "        \n",
    "        sentiments = self._in_spite_of_check(words_and_emoticons, sentiments)\n",
    "\n",
    "        return self.score_valence(sentiments, text)\n",
    "\n",
    "    def sentiment_valence(self, valence, sentitext, item, i, sentiments):\n",
    "        is_cap_diff = sentitext.is_cap_diff\n",
    "        words_and_emoticons = sentitext.words_and_emoticons\n",
    "        item_lowercase = item.lower()\n",
    "        if item_lowercase in self.lexicon:\n",
    "            # get the sentiment valence\n",
    "            valence = self.lexicon[item_lowercase]\n",
    "\n",
    "            # check if sentiment laden word is in ALL CAPS (while others aren't)\n",
    "            if item.isupper() and is_cap_diff:\n",
    "                if valence > 0:\n",
    "                    valence += C_INCR\n",
    "                else:\n",
    "                    valence -= C_INCR\n",
    "\n",
    "            for start_i in range(0, 3):\n",
    "                if (\n",
    "                    i > start_i\n",
    "                    and words_and_emoticons[i - (start_i + 1)].lower()\n",
    "                    not in self.lexicon\n",
    "                ):\n",
    "                    # dampen the scalar modifier of preceding words and emoticons\n",
    "                    # (excluding the ones that immediately preceed the item) based\n",
    "                    # on their distance from the current item.\n",
    "                    s = scalar_inc_dec(\n",
    "                        words_and_emoticons[i - (start_i + 1)], valence, is_cap_diff\n",
    "                    )\n",
    "                    if start_i == 1 and s != 0:\n",
    "                        s = s * 0.95\n",
    "                    if start_i == 2 and s != 0:\n",
    "                        s = s * 0.9\n",
    "                    valence = valence + s\n",
    "                    valence = self._never_check(\n",
    "                        valence, words_and_emoticons, start_i, i\n",
    "                    )\n",
    "                    if start_i == 2:\n",
    "                        valence = self._idioms_check(valence, words_and_emoticons, i)\n",
    "\n",
    "                        # future work: consider other sentiment-laden idioms\n",
    "                        # other_idioms =\n",
    "                        # {\"back handed\": -2, \"blow smoke\": -2, \"blowing smoke\": -2,\n",
    "                        #  \"upper hand\": 1, \"break a leg\": 2,\n",
    "                        #  \"cooking with gas\": 2, \"in the black\": 2, \"in the red\": -2,\n",
    "                        #  \"on the ball\": 2,\"under the weather\": -2}\n",
    "\n",
    "            valence = self._least_check(valence, words_and_emoticons, i)\n",
    "\n",
    "        sentiments.append(valence)\n",
    "        return sentiments\n",
    "\n",
    "    def _least_check(self, valence, words_and_emoticons, i):\n",
    "        # check for negation case using \"least\"\n",
    "        if (\n",
    "            i > 1\n",
    "            and words_and_emoticons[i - 1].lower() not in self.lexicon\n",
    "            and words_and_emoticons[i - 1].lower() == \"least\"\n",
    "        ):\n",
    "            if (\n",
    "                words_and_emoticons[i - 2].lower() != \"at\"\n",
    "                and words_and_emoticons[i - 2].lower() != \"very\"\n",
    "            ):\n",
    "                valence = valence * N_SCALAR\n",
    "        elif (\n",
    "            i > 0\n",
    "            and words_and_emoticons[i - 1].lower() not in self.lexicon\n",
    "            and words_and_emoticons[i - 1].lower() == \"least\"\n",
    "        ):\n",
    "            valence = valence * N_SCALAR\n",
    "        return valence\n",
    "\n",
    "    def _but_check(self, words_and_emoticons, sentiments):\n",
    "        # check for modification in sentiment due to contrastive conjunctions\n",
    "        words_and_emoticons_lower = [str(w).lower() for w in words_and_emoticons]\n",
    "        cc_list = ['but', 'however', 'except']\n",
    "        bi = 0\n",
    "        for cc in cc_list:\n",
    "            if cc in words_and_emoticons_lower:\n",
    "                bi = words_and_emoticons_lower.index(cc)\n",
    "                for si,sentiment in enumerate(sentiments):\n",
    "                    if si < bi:\n",
    "                        sentiments[si] = sentiment * 0.5\n",
    "                    elif si > bi:\n",
    "                        sentiments[si] = sentiment * 1.5\n",
    "                            \n",
    "        # Future work: \n",
    "        # 1.Consider usage of though/although/even though\n",
    "        \n",
    "        return sentiments\n",
    "    \n",
    "    def _only_if_check(self, words_and_emoticons, sentiments):\n",
    "        words_and_emoticons_lower = [str(w).lower() for w in words_and_emoticons]\n",
    "        check = 'only'\n",
    "        if check in words_and_emoticons_lower:\n",
    "            i = words_and_emoticons_lower.index(check)\n",
    "            if len(words_and_emoticons)>i+1 and 'if' == words_and_emoticons_lower[i+1]:\n",
    "                for si, sentiment in enumerate(sentiments):\n",
    "                    if si < i:\n",
    "                        sentiments[si] = sentiment * 0.5\n",
    "        return sentiments\n",
    "    \n",
    "    def _in_spite_of_check(self, words_and_emoticons, sentiments):\n",
    "        words_and_emoticons_lower = [str(w).lower() for w in words_and_emoticons]\n",
    "        check = 'in'\n",
    "        if check in words_and_emoticons_lower:\n",
    "            i = words_and_emoticons_lower.index(check)\n",
    "            if len(words_and_emoticons)>i+2 and 'spite' == words_and_emoticons_lower[i+1] and 'of' == words_and_emoticons_lower[i+2]:\n",
    "                for si, sentiment in enumerate(sentiments):\n",
    "                    if si == i+1:\n",
    "                        sentiments[si] = 0\n",
    "                    elif si < i+1:\n",
    "                        sentiments[si] = sentiment * 1.5\n",
    "                    elif si > i+1:\n",
    "                        sentiments[si] = sentiment * 0.5\n",
    "        return sentiments\n",
    "\n",
    "    def _idioms_check(self, valence, words_and_emoticons, i):\n",
    "        onezero = \"{0} {1}\".format(words_and_emoticons[i - 1], words_and_emoticons[i])\n",
    "\n",
    "        twoonezero = \"{0} {1} {2}\".format(\n",
    "            words_and_emoticons[i - 2],\n",
    "            words_and_emoticons[i - 1],\n",
    "            words_and_emoticons[i],\n",
    "        )\n",
    "\n",
    "        twoone = \"{0} {1}\".format(\n",
    "            words_and_emoticons[i - 2], words_and_emoticons[i - 1]\n",
    "        )\n",
    "\n",
    "        threetwoone = \"{0} {1} {2}\".format(\n",
    "            words_and_emoticons[i - 3],\n",
    "            words_and_emoticons[i - 2],\n",
    "            words_and_emoticons[i - 1],\n",
    "        )\n",
    "\n",
    "        threetwo = \"{0} {1}\".format(\n",
    "            words_and_emoticons[i - 3], words_and_emoticons[i - 2]\n",
    "        )\n",
    "\n",
    "        sequences = [onezero, twoonezero, twoone, threetwoone, threetwo]\n",
    "\n",
    "        for seq in sequences:\n",
    "            if seq in SPECIAL_CASE_IDIOMS:\n",
    "                valence = SPECIAL_CASE_IDIOMS[seq]\n",
    "                break\n",
    "\n",
    "        if len(words_and_emoticons) - 1 > i:\n",
    "            zeroone = \"{0} {1}\".format(\n",
    "                words_and_emoticons[i], words_and_emoticons[i + 1]\n",
    "            )\n",
    "            if zeroone in SPECIAL_CASE_IDIOMS:\n",
    "                valence = SPECIAL_CASE_IDIOMS[zeroone]\n",
    "        if len(words_and_emoticons) - 1 > i + 1:\n",
    "            zeroonetwo = \"{0} {1} {2}\".format(\n",
    "                words_and_emoticons[i],\n",
    "                words_and_emoticons[i + 1],\n",
    "                words_and_emoticons[i + 2],\n",
    "            )\n",
    "            if zeroonetwo in SPECIAL_CASE_IDIOMS:\n",
    "                valence = SPECIAL_CASE_IDIOMS[zeroonetwo]\n",
    "\n",
    "        # check for booster/dampener bi-grams such as 'sort of' or 'kind of'\n",
    "        if threetwo in BOOSTER_DICT or twoone in BOOSTER_DICT:\n",
    "            valence = valence + B_DECR\n",
    "        return valence\n",
    "\n",
    "    def _never_check(self, valence, words_and_emoticons, start_i, i):\n",
    "        if start_i == 0:\n",
    "            if negated([words_and_emoticons[i - 1]]):\n",
    "                valence = valence * N_SCALAR\n",
    "        if start_i == 1:\n",
    "            if words_and_emoticons[i - 2] == \"never\" and (\n",
    "                words_and_emoticons[i - 1] == \"so\"\n",
    "                or words_and_emoticons[i - 1] == \"this\"\n",
    "            ):\n",
    "                valence = valence * 1.5\n",
    "            elif negated([words_and_emoticons[i - (start_i + 1)]]):\n",
    "                valence = valence * N_SCALAR\n",
    "        if start_i == 2:\n",
    "            if (\n",
    "                words_and_emoticons[i - 3] == \"never\"\n",
    "                and (\n",
    "                    words_and_emoticons[i - 2] == \"so\"\n",
    "                    or words_and_emoticons[i - 2] == \"this\"\n",
    "                )\n",
    "                or (\n",
    "                    words_and_emoticons[i - 1] == \"so\"\n",
    "                    or words_and_emoticons[i - 1] == \"this\"\n",
    "                )\n",
    "            ):\n",
    "                valence = valence * 1.25\n",
    "            elif negated([words_and_emoticons[i - (start_i + 1)]]):\n",
    "                valence = valence * N_SCALAR\n",
    "        return valence\n",
    "\n",
    "    def _punctuation_emphasis(self, sum_s, text):\n",
    "        # add emphasis from exclamation points and question marks\n",
    "        ep_amplifier = self._amplify_ep(text)\n",
    "        qm_amplifier = self._amplify_qm(text)\n",
    "        punct_emph_amplifier = ep_amplifier + qm_amplifier\n",
    "        return punct_emph_amplifier\n",
    "\n",
    "    def _amplify_ep(self, text):\n",
    "        # check for added emphasis resulting from exclamation points (up to 4 of them)\n",
    "        ep_count = text.count(\"!\")\n",
    "        if ep_count > 4:\n",
    "            ep_count = 4\n",
    "        # (empirically derived mean sentiment intensity rating increase for\n",
    "        # exclamation points)\n",
    "        ep_amplifier = ep_count * 0.292\n",
    "        return ep_amplifier\n",
    "\n",
    "    def _amplify_qm(self, text):\n",
    "        # check for added emphasis resulting from question marks (2 or 3+)\n",
    "        qm_count = text.count(\"?\")\n",
    "        qm_amplifier = 0\n",
    "        if qm_count > 1:\n",
    "            if qm_count <= 3:\n",
    "                # (empirically derived mean sentiment intensity rating increase for\n",
    "                # question marks)\n",
    "                qm_amplifier = qm_count * 0.18\n",
    "            else:\n",
    "                qm_amplifier = 0.96\n",
    "        return qm_amplifier\n",
    "\n",
    "    def _sift_sentiment_scores(self, sentiments):\n",
    "        # want separate positive versus negative sentiment scores\n",
    "        pos_sum = 0.0\n",
    "        neg_sum = 0.0\n",
    "        neu_count = 0\n",
    "        for sentiment_score in sentiments:\n",
    "            if sentiment_score > 0:\n",
    "                pos_sum += (\n",
    "                    float(sentiment_score) + 1\n",
    "                )  # compensates for neutral words that are counted as 1\n",
    "            if sentiment_score < 0:\n",
    "                neg_sum += (\n",
    "                    float(sentiment_score) - 1\n",
    "                )  # when used with math.fabs(), compensates for neutrals\n",
    "            if sentiment_score == 0:\n",
    "                neu_count += 1\n",
    "        return pos_sum, neg_sum, neu_count\n",
    "\n",
    "    def score_valence(self, sentiments, text):\n",
    "        if sentiments:\n",
    "            sum_s = float(sum(sentiments))\n",
    "            # compute and add emphasis from punctuation in text\n",
    "            punct_emph_amplifier = self._punctuation_emphasis(sum_s, text)\n",
    "            if sum_s > 0:\n",
    "                sum_s += punct_emph_amplifier\n",
    "            elif sum_s < 0:\n",
    "                sum_s -= punct_emph_amplifier\n",
    "\n",
    "            compound = normalize(sum_s)\n",
    "            # discriminate between positive, negative and neutral sentiment scores\n",
    "            pos_sum, neg_sum, neu_count = self._sift_sentiment_scores(sentiments)\n",
    "\n",
    "            if pos_sum > math.fabs(neg_sum):\n",
    "                pos_sum += punct_emph_amplifier\n",
    "            elif pos_sum < math.fabs(neg_sum):\n",
    "                neg_sum -= punct_emph_amplifier\n",
    "\n",
    "            total = pos_sum + math.fabs(neg_sum) + neu_count\n",
    "            pos = math.fabs(pos_sum / total)\n",
    "            neg = math.fabs(neg_sum / total)\n",
    "            neu = math.fabs(neu_count / total)\n",
    "\n",
    "        else:\n",
    "            compound = 0.0\n",
    "            pos = 0.0\n",
    "            neg = 0.0\n",
    "            neu = 0.0\n",
    "\n",
    "        sentiment_dict = {\n",
    "            \"neg\": round(neg, 3),\n",
    "            \"neu\": round(neu, 3),\n",
    "            \"pos\": round(pos, 3),\n",
    "            \"compound\": round(compound, 4),\n",
    "        }\n",
    "\n",
    "        return sentiment_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('vader_lexicon') #This is one time\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 0.697, 'pos': 0.303, 'compound': 0.3832}\n",
      "{'neg': 0.0, 'neu': 0.555, 'pos': 0.445, 'compound': 0.7512}\n",
      "{'neg': 0.0, 'neu': 0.511, 'pos': 0.489, 'compound': 0.7717}\n",
      "{'neg': 0.348, 'neu': 0.652, 'pos': 0.0, 'compound': -0.4951}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.209, 'neu': 0.791, 'pos': 0.0, 'compound': -0.2144}\n",
      "{'neg': 0.256, 'neu': 0.744, 'pos': 0.0, 'compound': -0.3415}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.238, 'neu': 0.762, 'pos': 0.0, 'compound': -0.3595}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(analyser.polarity_scores(\"only\"))\n",
    "print(analyser.polarity_scores(\"The only good thing is the breakfast.\"))\n",
    "print(analyser.polarity_scores(\"The only good thing is that the breakfast was brilliant.\"))\n",
    "print(analyser.polarity_scores(\"The good thing is that the breakfast was brilliant.\"))\n",
    "print(analyser.polarity_scores(\"The only bad thing is the breakfast.\"))\n",
    "\n",
    "print(analyser.polarity_scores(\"Come here only if you don't mind climbing.\")) ##\n",
    "print(analyser.polarity_scores(\"It is a problem only if you have kids.\"))\n",
    "print(analyser.polarity_scores(\"It is only a problem if you have kids.\"))\n",
    "\n",
    "print(analyser.polarity_scores(\"If only I'd stayed here longer!\")) ##\n",
    "print(analyser.polarity_scores(\"If only I'd known there would be no food!\"))\n",
    "\n",
    "print(analyser.polarity_scores(\"The place has only 1 bed.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.406, 'neu': 0.427, 'pos': 0.167, 'compound': -0.5859}\n",
      "{'neg': 0.406, 'neu': 0.427, 'pos': 0.167, 'compound': -0.5859}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(analyser.polarity_scores(\"Beds were good but food was bad.\"))\n",
    "print(analyser.polarity_scores(\"Beds were good however food was bad.\"))\n",
    "print(analyser.polarity_scores(\"but\"))\n",
    "print(analyser.polarity_scores(\"however\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 0.328, 'pos': 0.672, 'compound': 0.6249}\n",
      "{'neg': 0.0, 'neu': 0.702, 'pos': 0.298, 'compound': 0.3716}\n",
      "{'neg': 0.282, 'neu': 0.544, 'pos': 0.173, 'compound': -0.3818}\n",
      "{'neg': 0.327, 'neu': 0.472, 'pos': 0.201, 'compound': -0.3818}\n",
      "{'neg': 0.327, 'neu': 0.472, 'pos': 0.201, 'compound': -0.3818}\n"
     ]
    }
   ],
   "source": [
    "print(analyser.polarity_scores(\"except\"))\n",
    "print(analyser.polarity_scores(\"Everything was awesome\"))\n",
    "print(analyser.polarity_scores(\"Everything was awesome except for the food\"))\n",
    "print(analyser.polarity_scores(\"Everything was awesome except for the food which was terrible\"))\n",
    "print(analyser.polarity_scores(\"Everything was awesome but the food was terrible\"))\n",
    "print(analyser.polarity_scores(\"Everything was awesome. But the food was terrible\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.5267}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 0.435, 'pos': 0.565, 'compound': 0.5994}\n",
      "{'neg': 0.0, 'neu': 0.627, 'pos': 0.373, 'compound': 0.7469}\n"
     ]
    }
   ],
   "source": [
    "print(analyser.polarity_scores(\"spite\"))\n",
    "print(analyser.polarity_scores(\"in spite of\"))\n",
    "print(analyser.polarity_scores(\"The structure was beautiful\"))\n",
    "print(analyser.polarity_scores(\"The structure was beautiful in spite of being so old\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
