--------------------------------------------------------------------------------
Unit tests for the utility parse functions.
--------------------------------------------------------------------------------

    >>> from nltk import tokenize

--------------------------------------------------------------------------------
Unit tests for the CFG (Context Free Grammar) class
--------------------------------------------------------------------------------

    >>> from nltk import cfg

    >>> nt1 = cfg.Nonterminal('NP')
    >>> nt2 = cfg.Nonterminal('VP')

    >>> nt1.symbol()
    'NP'

    >>> nt1 == cfg.Nonterminal('NP')
    True

    >>> nt1 == nt2
    False

    >>> S, NP, VP, PP = cfg.nonterminals('S, NP, VP, PP')
    >>> N, V, P, DT = cfg.nonterminals('N, V, P, DT') 

    >>> prod1 = cfg.Production(S, [NP, VP])
    >>> prod2 = cfg.Production(NP, [DT, NP])

    >>> prod1.lhs()
    <S>

    >>> prod1.rhs()
    (<NP>, <VP>)

    >>> prod1 == cfg.Production(S, [NP, VP])
    True

    >>> prod1 == prod2
    False

    >>> grammar = cfg.parse_cfg("""
    ... S -> NP VP
    ... PP -> P NP
    ... NP -> DT N | N PP | DT N PP
    ... VP -> V NP | V PP | V NP PP
    ... DT -> 'a'
    ... DT -> 'the'
    ... N -> 'cat'
    ... N -> 'dog'
    ... N -> 'rug'
    ... V -> 'chased'
    ... V -> 'sat'
    ... P -> 'in'
    ... P -> 'on'
    ... """)

--------------------------------------------------------------------------------
Unit tests for the rd (Recursive Descent Parser) class
--------------------------------------------------------------------------------


Create and run a recursive descent parser over both a syntactically ambiguous
and unambiguous sentence.

    >>> from nltk.parse import RecursiveDescent
    >>> rd = RecursiveDescent(grammar)

    >>> sentence1 = list(tokenize.whitespace('the cat chased the dog'))
    >>> sentence2 = list(tokenize.whitespace('the cat chased the dog on the rug'))

    >>> for t in rd.get_parse_list(sentence1):
    ...     print t
    (S (NP (DT the) (N cat)) (VP (V chased) (NP (DT the) (N dog))))

    >>> for t in rd.get_parse_list(sentence2):
    ...     print t
    (S
      (NP (DT the) (N cat))
      (VP
        (V chased)
        (NP (DT the) (N dog) (PP (P on) (NP (DT the) (N rug))))))
    (S
      (NP (DT the) (N cat))
      (VP
        (V chased)
        (NP (DT the) (N dog))
        (PP (P on) (NP (DT the) (N rug)))))

-------------------------------------------------
Unit tests for the sr (Shift Reduce Parser) class
-------------------------------------------------

Create and run a shift reduce parser over both a syntactically ambiguous
and unambiguous sentence. Note that unlike the recursive descent parser, one
and only one parse is ever returned.

    >>> from nltk.parse import ShiftReduce
    >>> sr = ShiftReduce(grammar)

    >>> sentence1 = list(tokenize.whitespace('the cat chased the dog'))
    >>> sentence2 = list(tokenize.whitespace('the cat chased the dog on the rug'))

    >>> for t in sr.get_parse_list(sentence1):
    ...     print t
    (S (NP (DT the) (N cat)) (VP (V chased) (NP (DT the) (N dog))))


The shift reduce parser uses heuristics to decide what to do when there are
multiple possible shift or reduce operations available - for the supplied
grammar clearly the wrong operation is selected.

    >>> for t in sr.get_parse_list(sentence2):
    ...     print t

--------------------------------------------------------------------------------
Unit tests for the Chart parser class
--------------------------------------------------------------------------------

    >>> from nltk.parse import ChartParse, EarleyChartParse, BU_STRATEGY, TD_STRATEGY

Define a grammar.

    >>> grammar = cfg.parse_cfg("""
    ... S -> NP VP
    ... PP -> P NP
    ... NP -> DT N | N | NP PP
    ... VP -> V NP |  VP PP
    ... DT -> 'a' | 'the'
    ... N -> 'Marc' | 'man' | 'park' | 'telescope'
    ... V -> 'has' | 'saw'
    ... P -> 'in'
    ... """)

Some example sentences, one ambiguous and one unambiguous.

    >>> sentence1 = list(tokenize.whitespace("Marc has a telescope"))
    >>> sentence2 = list(tokenize.whitespace("Marc saw a man in the park"))

Create a chart parser. First give it a bottom-up strategy.

    >>> parser = ChartParse(grammar, BU_STRATEGY)

    >>> for t in parser.get_parse_list(sentence1):
    ...     print t
    (S (NP (N Marc)) (VP (V has) (NP (DT a) (N telescope))))

    >>> for t in parser.get_parse_list(sentence2):
    ...     print t
    (S
      (NP (N Marc))
      (VP
        (V saw)
        (NP (NP (DT a) (N man)) (PP (P in) (NP (DT the) (N park))))))
    (S
      (NP (N Marc))
      (VP
        (VP (V saw) (NP (DT a) (N man)))
        (PP (P in) (NP (DT the) (N park)))))

Redefine the chart parser to use a top-down strategy.

    >>> parser = ChartParse(grammar, TD_STRATEGY)

    >>> for t in parser.get_parse_list(sentence1):
    ...     print t
    (S (NP (N Marc)) (VP (V has) (NP (DT a) (N telescope))))

    >>> for t in parser.get_parse_list(sentence2):
    ...     print t
    (S
      (NP (N Marc))
      (VP
        (V saw)
        (NP (NP (DT a) (N man)) (PP (P in) (NP (DT the) (N park))))))
    (S
      (NP (N Marc))
      (VP
        (VP (V saw) (NP (DT a) (N man)))
        (PP (P in) (NP (DT the) (N park)))))

Create and test the Earley variant chart parser. This requires a lexicon.

    >>> syntactic_productions = [
    ... cfg.Production(S, [NP, VP]),
    ... cfg.Production(PP, [P, NP]),
    ... cfg.Production(NP, [DT, N]),
    ... cfg.Production(NP, [N]),
    ... cfg.Production(NP, [NP, PP]),
    ... cfg.Production(VP, [V, NP]),
    ... cfg.Production(VP, [VP, PP])
    ... ]

    >>> grammar = cfg.Grammar(S, syntactic_productions)

    >>> lexical_productions = [
    ... cfg.Production(DT, ['a']),
    ... cfg.Production(DT, ['the']),
    ... cfg.Production(N, ['man']),
    ... cfg.Production(N, ['Marc']),
    ... cfg.Production(N, ['park']),
    ... cfg.Production(N, ['telescope']),
    ... cfg.Production(V, ['has']),
    ... cfg.Production(V, ['saw']),
    ... cfg.Production(P, ['in'])
    ... ]

    >>> lexicon = {}
    >>> for production in lexical_productions:
    ...     lexicon.setdefault(production.rhs()[0], []).append(production.lhs())

    >>> parser = EarleyChartParse(grammar, lexicon)

    >>> for t in parser.get_parse_list(sentence1):
    ...     print t
    (S (NP (N Marc)) (VP (V has) (NP (DT a) (N telescope))))

    >>> for t in parser.get_parse_list(sentence2):
    ...     print t
    (S
      (NP (N Marc))
      (VP
        (V saw)
        (NP (NP (DT a) (N man)) (PP (P in) (NP (DT the) (N park))))))
    (S
      (NP (N Marc))
      (VP
        (VP (V saw) (NP (DT a) (N man)))
        (PP (P in) (NP (DT the) (N park)))))

------------------
GrammarFile class
------------------

    >>> from nltk.parse import GrammarFile
    >>> g = GrammarFile('../parse/test.cfg')
    >>> print g.earley_grammar()
    Grammar with 3 productions (start state = S)
        S[sem=ApplicationExpression('?vp', '?subj')] -> NP[sem=?subj] VP[sem=?vp]
        VP[sem=ApplicationExpression('?v', '?obj')] -> V[sem=?v] NP[sem=?obj]
        VP[sem=?v] -> V[sem=?v]
    >>> print g.earley_lexicon()
    {'I': [NP[sem='i']], 'sleeps': [V[sem='\\x.(sleeps x)', tns=pres]], 'like': [V[sem='\\x y.(like x y)', tns=pres]], 'Kim': [NP[sem='kim']]}
    >>> p = g.earley_parser()

--------------------------------------------------------------------------------
Unit tests for the Probabilistic CFG class
--------------------------------------------------------------------------------

    >>> from nltk.corpus import treebank
    >>> from itertools import islice

Create a set of probabilistic CFG productions.

    >>> grammar = cfg.parse_pcfg("""
    ... A -> B B [.3] | C B C [.7]
    ... B -> B D [.5] | C [.5]
    ... C -> 'a' [.1] | 'b' [0.9]
    ... D -> 'b' [1.0]
    ... """)
    >>> prod = grammar.productions()[0]
    >>> prod
    A -> B B [0.3]

    >>> prod.lhs()
    <A>

    >>> prod.rhs()
    (<B>, <B>)

    >>> prod.prob()
    0.29999999999999999

    >>> grammar.start()
    <A>

    >>> grammar.productions()
    [A -> B B [0.3], A -> C B C [0.7], B -> B D [0.5], B -> C [0.5], C -> 'a' [0.1], C -> 'b' [0.9], D -> 'b' [1.0]]

Induce some productions using parsed Treebank data.

    >>> productions = []
    >>> for t in treebank.parsed()[:3]:
    ...     productions += t.productions()

    >>> grammar = cfg.induce_pcfg(S, productions)
    >>> grammar
    <Grammar with 71 productions>

    >>> grammar.productions()[:5]
    [PP -> IN NP [1.0], NNP -> 'Nov.' [0.0714285714286], NNP -> 'Agnew' [0.0714285714286], JJ -> 'industrial' [0.142857142857], NP -> CD NNS [0.133333333333]]

--------------------------------------------------------------------------------
Unit tests for the Probabilistic Chart Parse classes
--------------------------------------------------------------------------------

    >>> tokens = list(tokenize.whitespace("Jack saw Bob with my cookie"))
    >>> grammar = cfg.toy_pcfg2
    >>> print grammar
    Grammar with 23 productions (start state = S)
        S -> NP VP [1.0]
        VP -> V NP [0.59]
        VP -> V [0.4]
        VP -> VP PP [0.01]
        NP -> Det N [0.41]
        NP -> Name [0.28]
        NP -> NP PP [0.31]
        PP -> P NP [1.0]
        V -> 'saw' [0.21]
        V -> 'ate' [0.51]
        V -> 'ran' [0.28]
        N -> 'boy' [0.11]
        N -> 'cookie' [0.12]
        N -> 'table' [0.13]
        N -> 'telescope' [0.14]
        N -> 'hill' [0.5]
        Name -> 'Jack' [0.52]
        Name -> 'Bob' [0.48]
        P -> 'with' [0.61]
        P -> 'under' [0.39]
        Det -> 'the' [0.41]
        Det -> 'a' [0.31]
        Det -> 'my' [0.28]

Create several parsers using different queuing strategies and show the
resulting parses.

    >>> from nltk.parse import pchart
    
    >>> parser = pchart.InsideParse(grammar)
    >>> parser.get_parse_list(tokens)
    [('S': ('NP': ('Name': 'Jack') (p=0.52)) (p=0.1456) ('VP': ('V': 'saw') (p=0.21) ('NP': ('NP': ('Name': 'Bob') (p=0.48)) (p=0.1344) ('PP': ('P': 'with') (p=0.61) ('NP': ('Det': 'my') (p=0.28) ('N': 'cookie') (p=0.12)) (p=0.013776)) (p=0.00840336)) (p=0.00035011759104)) (p=4.33795695299e-05)) (p=6.31606532355e-06), ('S': ('NP': ('Name': 'Jack') (p=0.52)) (p=0.1456) ('VP': ('VP': ('V': 'saw') (p=0.21) ('NP': ('Name': 'Bob') (p=0.48)) (p=0.1344)) (p=0.01665216) ('PP': ('P': 'with') (p=0.61) ('NP': ('Det': 'my') (p=0.28) ('N': 'cookie') (p=0.12)) (p=0.013776)) (p=0.00840336)) (p=1.39934095258e-06)) (p=2.03744042695e-07)]

    >>> parser = pchart.RandomParse(grammar)
    >>> parser.get_parse_list(tokens)
    [('S': ('NP': ('Name': 'Jack') (p=0.52)) (p=0.1456) ('VP': ('V': 'saw') (p=0.21) ('NP': ('NP': ('Name': 'Bob') (p=0.48)) (p=0.1344) ('PP': ('P': 'with') (p=0.61) ('NP': ('Det': 'my') (p=0.28) ('N': 'cookie') (p=0.12)) (p=0.013776)) (p=0.00840336)) (p=0.00035011759104)) (p=4.33795695299e-05)) (p=6.31606532355e-06), ('S': ('NP': ('Name': 'Jack') (p=0.52)) (p=0.1456) ('VP': ('VP': ('V': 'saw') (p=0.21) ('NP': ('Name': 'Bob') (p=0.48)) (p=0.1344)) (p=0.01665216) ('PP': ('P': 'with') (p=0.61) ('NP': ('Det': 'my') (p=0.28) ('N': 'cookie') (p=0.12)) (p=0.013776)) (p=0.00840336)) (p=1.39934095258e-06)) (p=2.03744042695e-07)]

    >>> parser = pchart.UnsortedParse(grammar)
    >>> parser.get_parse_list(tokens)
    [('S': ('NP': ('Name': 'Jack') (p=0.52)) (p=0.1456) ('VP': ('V': 'saw') (p=0.21) ('NP': ('NP': ('Name': 'Bob') (p=0.48)) (p=0.1344) ('PP': ('P': 'with') (p=0.61) ('NP': ('Det': 'my') (p=0.28) ('N': 'cookie') (p=0.12)) (p=0.013776)) (p=0.00840336)) (p=0.00035011759104)) (p=4.33795695299e-05)) (p=6.31606532355e-06), ('S': ('NP': ('Name': 'Jack') (p=0.52)) (p=0.1456) ('VP': ('VP': ('V': 'saw') (p=0.21) ('NP': ('Name': 'Bob') (p=0.48)) (p=0.1344)) (p=0.01665216) ('PP': ('P': 'with') (p=0.61) ('NP': ('Det': 'my') (p=0.28) ('N': 'cookie') (p=0.12)) (p=0.013776)) (p=0.00840336)) (p=1.39934095258e-06)) (p=2.03744042695e-07)]

    >>> parser = pchart.LongestParse(grammar)
    >>> parser.get_parse_list(tokens)
    [('S': ('NP': ('Name': 'Jack') (p=0.52)) (p=0.1456) ('VP': ('V': 'saw') (p=0.21) ('NP': ('NP': ('Name': 'Bob') (p=0.48)) (p=0.1344) ('PP': ('P': 'with') (p=0.61) ('NP': ('Det': 'my') (p=0.28) ('N': 'cookie') (p=0.12)) (p=0.013776)) (p=0.00840336)) (p=0.00035011759104)) (p=4.33795695299e-05)) (p=6.31606532355e-06), ('S': ('NP': ('Name': 'Jack') (p=0.52)) (p=0.1456) ('VP': ('VP': ('V': 'saw') (p=0.21) ('NP': ('Name': 'Bob') (p=0.48)) (p=0.1344)) (p=0.01665216) ('PP': ('P': 'with') (p=0.61) ('NP': ('Det': 'my') (p=0.28) ('N': 'cookie') (p=0.12)) (p=0.013776)) (p=0.00840336)) (p=1.39934095258e-06)) (p=2.03744042695e-07)]

    >>> parser = pchart.InsideParse(grammar, beam_size = len(tokens)+1)
    >>> parser.get_parse_list(tokens)
    []

----------------------------------------
Unit tests for the Viterbi Parse classes
----------------------------------------

    >>> from nltk.parse import ViterbiParse
    >>> tokens = list(tokenize.whitespace("Jack saw Bob with my cookie"))
    >>> grammar = cfg.toy_pcfg2

Parse the tokenized sentence.

    >>> parser = ViterbiParse(grammar)
    >>> parser.get_parse_list(tokens)
    [('S': ('NP': ('Name': 'Jack') (p=0.52)) (p=0.1456) ('VP': ('V': 'saw') (p=0.21) ('NP': ('NP': ('Name': 'Bob') (p=0.48)) (p=0.1344) ('PP': ('P': 'with') (p=0.61) ('NP': ('Det': 'my') (p=0.28) ('N': 'cookie') (p=0.12)) (p=0.013776)) (p=0.00840336)) (p=0.00035011759104)) (p=4.33795695299e-05)) (p=6.31606532355e-06)]

--------------------------------------------------------------------------------
Unit tests for the Category class
--------------------------------------------------------------------------------

    >>> from nltk.parse import Category, GrammarCategory
    >>> Category(pos='n', agr=Category(number='pl', gender='f'))
    [agr=[gender='f', number='pl'], pos='n']

    >>> GrammarCategory.parse('VP[+fin]/NP[+pl]')
    VP[+fin]/NP[+pl]

--------------------------------------------------------------------------------
Unit tests for the Feature Chart Parser classes
--------------------------------------------------------------------------------

    >>> S = GrammarCategory.parse('S')
    >>> VP = GrammarCategory.parse('VP')
    >>> NP = GrammarCategory.parse('NP')
    >>> PP = GrammarCategory.parse('PP')
    >>> V = GrammarCategory.parse('V')
    >>> N = GrammarCategory.parse('N')
    >>> P = GrammarCategory.parse('P')
    >>> Name = GrammarCategory.parse('Name')
    >>> Det = GrammarCategory.parse('Det')
    >>> DetSg = GrammarCategory.parse('Det[-pl]')
    >>> DetPl = GrammarCategory.parse('Det[+pl]')
    >>> NSg = GrammarCategory.parse('N[-pl]')
    >>> NPl = GrammarCategory.parse('N[+pl]')

Define some grammatical productions.

    >>> grammatical_productions = [
    ...     cfg.Production(S, (NP, VP)),  cfg.Production(PP, (P, NP)),
    ...     cfg.Production(NP, (NP, PP)),
    ...     cfg.Production(VP, (VP, PP)), cfg.Production(VP, (V, NP)),
    ...     cfg.Production(VP, (V,)), cfg.Production(NP, (DetPl, NPl)),
    ...     cfg.Production(NP, (DetSg, NSg))]

Define some lexical productions.

    >>> lexical_productions = [
    ...     cfg.Production(NP, ('John',)), cfg.Production(NP, ('I',)),
    ...     cfg.Production(Det, ('the',)), cfg.Production(Det, ('my',)),
    ...     cfg.Production(Det, ('a',)),
    ...     cfg.Production(NSg, ('dog',)),   cfg.Production(NSg, ('cookie',)),
    ...     cfg.Production(V, ('ate',)),  cfg.Production(V, ('saw',)),
    ...     cfg.Production(P, ('with',)), cfg.Production(P, ('under',))]
    
    >>> earley_grammar = cfg.Grammar(S, grammatical_productions)
    >>> earley_lexicon = {}

    >>> for prod in lexical_productions:
    ...     earley_lexicon.setdefault(prod.rhs()[0].upper(), []).append(prod.lhs())

    >>> tokens = list(tokenize.whitespace('I saw John with a dog with my cookie'))
    >>> cp = FeatureEarleyChartParse(earley_grammar, earley_lexicon)
    >>> trees = cp.get_parse_list(tokens)
    >>> for tree in trees:
    ...     print tree
    ([INIT][]:
      (S[]:
        (NP[]: 'I')
        (VP[]:
          (VP[]:
            (VP[]: (V[]: 'saw') (NP[]: 'John'))
            (PP[]:
              (P[]: 'with')
              (NP[]: (Det[]: 'a') (N[ pl  = False ]: 'dog'))))
          (PP[]:
            (P[]: 'with')
            (NP[]: (Det[]: 'my') (N[ pl  = False ]: 'cookie'))))))
    ([INIT][]:
      (S[]:
        (NP[]: 'I')
        (VP[]:
          (VP[]: (V[]: 'saw') (NP[]: 'John'))
          (PP[]:
            (P[]: 'with')
            (NP[]:
              (NP[]: (Det[]: 'a') (N[ pl  = False ]: 'dog'))
              (PP[]:
                (P[]: 'with')
                (NP[]: (Det[]: 'my') (N[ pl  = False ]: 'cookie'))))))))
    ([INIT][]:
      (S[]:
        (NP[]: 'I')
        (VP[]:
          (VP[]:
            (V[]: 'saw')
            (NP[]:
              (NP[]: 'John')
              (PP[]:
                (P[]: 'with')
                (NP[]: (Det[]: 'a') (N[ pl  = False ]: 'dog')))))
          (PP[]:
            (P[]: 'with')
            (NP[]: (Det[]: 'my') (N[ pl  = False ]: 'cookie'))))))
    ([INIT][]:
      (S[]:
        (NP[]: 'I')
        (VP[]:
          (V[]: 'saw')
          (NP[]:
            (NP[]:
              (NP[]: 'John')
              (PP[]:
                (P[]: 'with')
                (NP[]: (Det[]: 'a') (N[ pl  = False ]: 'dog'))))
            (PP[]:
              (P[]: 'with')
              (NP[]: (Det[]: 'my') (N[ pl  = False ]: 'cookie')))))))
    ([INIT][]:
      (S[]:
        (NP[]: 'I')
        (VP[]:
          (V[]: 'saw')
          (NP[]:
            (NP[]: 'John')
            (PP[]:
              (P[]: 'with')
              (NP[]:
                (NP[]: (Det[]: 'a') (N[ pl  = False ]: 'dog'))
                (PP[]:
                  (P[]: 'with')
                  (NP[]: (Det[]: 'my') (N[ pl  = False ]: 'cookie')))))))))

