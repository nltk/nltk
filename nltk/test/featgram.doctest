=========================
 Feature Grammar Parsing
=========================

.. include:: ../../doc/definitions.rst

Grammars can be parsed from strings.

    >>> import nltk
    >>> from nltk import grammar, parse
    >>> g = """
    ... % start DP
    ... DP[agr=?a] -> D[agr=?a] N[agr=?a]
    ... D[agr=[num='sg', pers=3]] -> 'this' | 'that'
    ... D[agr=[num='pl', pers=3]] -> 'these' | 'those'
    ... D[agr=[num='pl', pers=1]] -> 'we'
    ... D[agr=[pers=2]] -> 'you'
    ... N[agr=[num='sg', gend='m']] -> 'boy'
    ... N[agr=[num='pl', gend='m']] -> 'boys'
    ... N[agr=[num='sg', gend='f']] -> 'girl'
    ... N[agr=[num='pl', gend='f']] -> 'girls'
    ... N[agr=[num='sg']] -> 'student'
    ... N[agr=[num='pl']] -> 'students'
    ... """
    >>> grammar = grammar.parse_fcfg(g)
    >>> tokens = 'these girls'.split()
    >>> parser = parse.FeatureEarleyChartParser(grammar)
    >>> trees = parser.nbest_parse(tokens)
    >>> for tree in trees: print tree
    (DP[agr=[gend='f', num='pl', pers=3]]
      (D[agr=[num='pl', pers=3]] these)
      (N[agr=[gend='f', num='pl']] girls))

In general, when we are trying to develop even a very small grammar,
it is convenient to put the rules in a file where they can be edited,
tested and revised. Let's assume that we have saved feat0cfg_ as a file named
``'feat0.fcfg'`` and placed it in the NLTK ``data`` directory. We can
inspect it as follows:

.. _feat0cfg: http://nltk.svn.sourceforge.net/svnroot/nltk/trunk/nltk/data/grammars/feat0.fcfg

    >>> nltk.data.show_cfg('grammars/book_grammars/feat0.fcfg')
    % start S
    # ############################
    # Grammar Rules
    # ############################
    # S expansion rules
    S -> NP[NUM=?n] VP[NUM=?n]
    # NP expansion rules
    NP[NUM=?n] -> N[NUM=?n] 
    NP[NUM=?n] -> PropN[NUM=?n] 
    NP[NUM=?n] -> Det[NUM=?n] N[NUM=?n]
    NP[NUM=pl] -> N[NUM=pl] 
    # VP expansion rules
    VP[TENSE=?t, NUM=?n] -> IV[TENSE=?t, NUM=?n]
    VP[TENSE=?t, NUM=?n] -> TV[TENSE=?t, NUM=?n] NP
    # ############################
    # Lexical Rules
    # ############################
    Det[NUM=sg] -> 'this' | 'every'
    Det[NUM=pl] -> 'these' | 'all'
    Det -> 'the' | 'some'
    PropN[NUM=sg]-> 'Kim' | 'Jody'
    N[NUM=sg] -> 'dog' | 'girl' | 'car' | 'child'
    N[NUM=pl] -> 'dogs' | 'girls' | 'cars' | 'children' 
    IV[TENSE=pres,  NUM=sg] -> 'disappears' | 'walks'
    TV[TENSE=pres, NUM=sg] -> 'sees' | 'likes'
    IV[TENSE=pres,  NUM=pl] -> 'disappear' | 'walk'
    TV[TENSE=pres, NUM=pl] -> 'see' | 'like'
    IV[TENSE=past, NUM=?n] -> 'disappeared' | 'walked'
    TV[TENSE=past, NUM=?n] -> 'saw' | 'liked'

Assuming we have saved feat0cfg_ as a file named
``'feat0.fcfg'``, the function ``parse.load_earley`` allows us to
read the grammar into NLTK, ready for use in parsing.


    >>> cp = parse.load_earley('grammars/book_grammars/feat0.fcfg', trace=3)
    >>> sent = 'Kim likes children'
    >>> tokens = sent.split()
    >>> tokens
    ['Kim', 'likes', 'children']
    >>> trees = cp.nbest_parse(tokens)
              |.K.l.c.|
    Processing queue 0
    Predictor |> . . .| [0:0] S[] -> * NP[NUM=?n] VP[NUM=?n] {}
    Predictor |> . . .| [0:0] NP[NUM=?n] -> * N[NUM=?n] {}
    Predictor |> . . .| [0:0] NP[NUM=?n] -> * PropN[NUM=?n] {}
    Predictor |> . . .| [0:0] NP[NUM=?n] -> * Det[NUM=?n] N[NUM=?n] {}
    Predictor |> . . .| [0:0] NP[NUM='pl'] -> * N[NUM='pl'] {}
    Scanner   |[-] . .| [0:1] 'Kim'
    Scanner   |[-] . .| [0:1] PropN[NUM='sg'] -> 'Kim' *
    Processing queue 1
    Completer |[-] . .| [0:1] NP[NUM='sg'] -> PropN[NUM='sg'] *
    Completer |[-> . .| [0:1] S[] -> NP[NUM=?n] * VP[NUM=?n] {?n: 'sg'}
    Predictor |. > . .| [1:1] VP[NUM=?n, TENSE=?t] -> * IV[NUM=?n, TENSE=?t] {}
    Predictor |. > . .| [1:1] VP[NUM=?n, TENSE=?t] -> * TV[NUM=?n, TENSE=?t] NP[] {}
    Scanner   |. [-] .| [1:2] 'likes'
    Scanner   |. [-] .| [1:2] TV[NUM='sg', TENSE='pres'] -> 'likes' *
    Processing queue 2
    Completer |. [-> .| [1:2] VP[NUM=?n, TENSE=?t] -> TV[NUM=?n, TENSE=?t] * NP[] {?n: 'sg', ?t: 'pres'}
    Predictor |. . > .| [2:2] NP[NUM=?n] -> * N[NUM=?n] {}
    Predictor |. . > .| [2:2] NP[NUM=?n] -> * PropN[NUM=?n] {}
    Predictor |. . > .| [2:2] NP[NUM=?n] -> * Det[NUM=?n] N[NUM=?n] {}
    Predictor |. . > .| [2:2] NP[NUM='pl'] -> * N[NUM='pl'] {}
    Scanner   |. . [-]| [2:3] 'children'
    Scanner   |. . [-]| [2:3] N[NUM='pl'] -> 'children' *
    Processing queue 3
    Completer |. . [-]| [2:3] NP[NUM='pl'] -> N[NUM='pl'] *
    Completer |. [---]| [1:3] VP[NUM='sg', TENSE='pres'] -> TV[NUM='sg', TENSE='pres'] NP[] *
    Completer |[=====]| [0:3] S[] -> NP[NUM='sg'] VP[NUM='sg'] *
    Completer |[=====]| [0:3] [INIT][] -> S[] *
    >>> for tree in trees: print tree
    (S[]
      (NP[NUM='sg'] (PropN[NUM='sg'] Kim))
      (VP[NUM='sg', TENSE='pres']
        (TV[NUM='sg', TENSE='pres'] likes)
        (NP[NUM='pl'] (N[NUM='pl'] children))))

Feature structures in NLTK are ... Atomic feature values can be strings or
integers.

    >>> fs1 = nltk.FeatStruct(TENSE='past', NUM='sg')
    >>> print fs1
    [ NUM   = 'sg'   ]
    [ TENSE = 'past' ]

We can think of a feature structure as being like a Python dictionary,
and access its values by indexing in the usual way.

    >>> fs1 = nltk.FeatStruct(PER=3, NUM='pl', GND='fem')
    >>> print fs1['GND']
    fem

We can also define feature structures which have complex values, as
discussed earlier.

    >>> fs2 = nltk.FeatStruct(POS='N', AGR=fs1)
    >>> print fs2
    [       [ GND = 'fem' ] ]
    [ AGR = [ NUM = 'pl'  ] ]
    [       [ PER = 3     ] ]
    [                       ]
    [ POS = 'N'             ]
    >>> print fs2['AGR']
    [ GND = 'fem' ]
    [ NUM = 'pl'  ]
    [ PER = 3     ]
    >>> print fs2['AGR']['PER']
    3

Feature structures can also be constructed using the ``parse()``
method of the ``nltk.FeatStruct`` class. Note that in this case, atomic
feature values do not need to be enclosed in quotes.
    
    >>> f1 = nltk.FeatStruct("[NUMBER = sg]")
    >>> f2 = nltk.FeatStruct("[PERSON = 3]")
    >>> print nltk.unify(f1, f2)
    [ NUMBER = 'sg' ]
    [ PERSON = 3    ]

    >>> f1 = nltk.FeatStruct("[A = [B = b, D = d]]")
    >>> f2 = nltk.FeatStruct("[A = [C = c, D = d]]")
    >>> print nltk.unify(f1, f2)
    [     [ B = 'b' ] ]
    [ A = [ C = 'c' ] ]
    [     [ D = 'd' ] ]


Feature Structures as Graphs
----------------------------

Feature structures are not inherently tied to linguistic objects; they are
general purpose structures for representing knowledge. For example, we
could encode information about a person in a feature structure:

    >>> person01 = nltk.FeatStruct("[NAME=Lee, TELNO='01 27 86 42 96',AGE=33]")
    >>> print person01
    [ AGE   = 33               ]
    [ NAME  = 'Lee'            ]
    [ TELNO = '01 27 86 42 96' ]

There are a number of notations for representing reentrancy in
matrix-style representations of feature structures. In NLTK, we adopt
the following convention: the first occurrence of a shared feature structure 
is prefixed with an integer in parentheses, such as ``(1)``, and any
subsequent reference to that structure uses the notation
``->(1)``, as shown below.

 
    >>> fs = nltk.FeatStruct("""[NAME=Lee, ADDRESS=(1)[NUMBER=74, STREET='rue Pascal'], 
    ...                               SPOUSE=[NAME=Kim, ADDRESS->(1)]]""")
    >>> print fs
    [ ADDRESS = (1) [ NUMBER = 74           ] ]
    [               [ STREET = 'rue Pascal' ] ]
    [                                         ]
    [ NAME    = 'Lee'                         ]
    [                                         ]
    [ SPOUSE  = [ ADDRESS -> (1)  ]           ]
    [           [ NAME    = 'Kim' ]           ]

There can be any number of tags within a single feature structure.

    >>> fs3 = nltk.FeatStruct("[A=(1)[B=b], C=(2)[], D->(1), E->(2)]")
    >>> print fs3
    [ A = (1) [ B = 'b' ] ]
    [                     ]
    [ C = (2) []          ]
    [                     ]
    [ D -> (1)            ]
    [ E -> (2)            ]
    >>> fs1 = nltk.FeatStruct(NUMBER=74, STREET='rue Pascal')
    >>> fs2 = nltk.FeatStruct(CITY='Paris')
    >>> print nltk.unify(fs1, fs2)
    [ CITY   = 'Paris'      ]
    [ NUMBER = 74           ]
    [ STREET = 'rue Pascal' ]

Unification is symmetric:

    >>> nltk.unify(fs1, fs2) == nltk.unify(fs2, fs1)
    True

Unification is commutative:

    >>> fs3 = nltk.FeatStruct(TELNO='01 27 86 42 96')
    >>> nltk.unify(nltk.unify(fs1, fs2), fs3) == nltk.unify(fs1, nltk.unify(fs2, fs3))
    True

Unification between `FS`:math:\ :subscript:`0` and `FS`:math:\
:subscript:`1` will fail if the two feature structures share a path |pi|,
but the value of |pi| in `FS`:math:\ :subscript:`0` is a distinct
atom from the value of |pi| in `FS`:math:\ :subscript:`1`. In NLTK,
this is implemented by setting the result of unification to be
``None``.

    >>> fs0 = nltk.FeatStruct(A='a')
    >>> fs1 = nltk.FeatStruct(A='b')
    >>> print nltk.unify(fs0, fs1)
    None

Now, if we look at how unification interacts with structure-sharing,
things become really interesting.



    >>> fs0 = nltk.FeatStruct("""[NAME=Lee, 
    ...                                ADDRESS=[NUMBER=74, 
    ...                                         STREET='rue Pascal'], 
    ...                                SPOUSE= [NAME=Kim,
    ...                                         ADDRESS=[NUMBER=74, 
    ...                                                  STREET='rue Pascal']]]""")
    >>> print fs0
    [ ADDRESS = [ NUMBER = 74           ]               ]
    [           [ STREET = 'rue Pascal' ]               ]
    [                                                   ]
    [ NAME    = 'Lee'                                   ]
    [                                                   ]
    [           [ ADDRESS = [ NUMBER = 74           ] ] ]
    [ SPOUSE  = [           [ STREET = 'rue Pascal' ] ] ]
    [           [                                     ] ]
    [           [ NAME    = 'Kim'                     ] ]


    >>> fs1 = nltk.FeatStruct("[SPOUSE=[ADDRESS=[CITY=Paris]]]")
    >>> print nltk.unify(fs0, fs1)
    [ ADDRESS = [ NUMBER = 74           ]               ]
    [           [ STREET = 'rue Pascal' ]               ]
    [                                                   ]
    [ NAME    = 'Lee'                                   ]
    [                                                   ]
    [           [           [ CITY   = 'Paris'      ] ] ]
    [           [ ADDRESS = [ NUMBER = 74           ] ] ]
    [ SPOUSE  = [           [ STREET = 'rue Pascal' ] ] ]
    [           [                                     ] ]
    [           [ NAME    = 'Kim'                     ] ]

    >>> fs2 = nltk.FeatStruct("""[NAME=Lee, ADDRESS=(1)[NUMBER=74, STREET='rue Pascal'],
    ...                                SPOUSE=[NAME=Kim, ADDRESS->(1)]]""")

 
    >>> print fs2
    [ ADDRESS = (1) [ NUMBER = 74           ] ]
    [               [ STREET = 'rue Pascal' ] ]
    [                                         ]
    [ NAME    = 'Lee'                         ]
    [                                         ]
    [ SPOUSE  = [ ADDRESS -> (1)  ]           ]
    [           [ NAME    = 'Kim' ]           ]


    >>> print nltk.unify(fs2, fs1)
    [               [ CITY   = 'Paris'      ] ]
    [ ADDRESS = (1) [ NUMBER = 74           ] ]
    [               [ STREET = 'rue Pascal' ] ]
    [                                         ]
    [ NAME    = 'Lee'                         ]
    [                                         ]
    [ SPOUSE  = [ ADDRESS -> (1)  ]           ]
    [           [ NAME    = 'Kim' ]           ]


    >>> fs1 = nltk.FeatStruct("[ADDRESS1=[NUMBER=74, STREET='rue Pascal']]")
    >>> fs2 = nltk.FeatStruct("[ADDRESS1=?x, ADDRESS2=?x]")
    >>> print fs2
    [ ADDRESS1 = ?x ]
    [ ADDRESS2 = ?x ]
    >>> print nltk.unify(fs1, fs2)
    [ ADDRESS1 = (1) [ NUMBER = 74           ] ]
    [                [ STREET = 'rue Pascal' ] ]
    [                                          ]
    [ ADDRESS2 -> (1)                          ]

 


    >>> sent = 'who do you claim that you like'
    >>> tokens = sent.split()
    >>> cp = parse.load_earley('grammars/book_grammars/feat1.fcfg', trace=2)
    >>> trees = cp.nbest_parse(tokens)
              |.w.d.y.c.t.y.l.|
    Scanner   |[-] . . . . . .| [0:1] 'who'
    Scanner   |[-] . . . . . .| [0:1] NP[+WH] -> 'who' *
    Completer |[-> . . . . . .| [0:1] S[-INV] -> NP[] * S[]/NP[] {}
    Scanner   |. [-] . . . . .| [1:2] 'do'
    Scanner   |. [-] . . . . .| [1:2] V[+AUX, SUBCAT=3] -> 'do' *
    Completer |. [-> . . . . .| [1:2] S[+INV]/?x[] -> V[+AUX] * NP[] VP[]/?x[] {}
    Scanner   |. . [-] . . . .| [2:3] 'you'
    Scanner   |. . [-] . . . .| [2:3] NP[-WH] -> 'you' *
    Completer |. [---> . . . .| [1:3] S[+INV]/?x[] -> V[+AUX] NP[] * VP[]/?x[] {}
    Scanner   |. . . [-] . . .| [3:4] 'claim'
    Scanner   |. . . [-] . . .| [3:4] V[-AUX, SUBCAT=2] -> 'claim' *
    Completer |. . . [-> . . .| [3:4] VP[]/?x[] -> V[-AUX, SUBCAT=2] * S-BAR[]/?x[] {}
    Scanner   |. . . . [-] . .| [4:5] 'that'
    Scanner   |. . . . [-] . .| [4:5] Comp[] -> 'that' *
    Completer |. . . . [-> . .| [4:5] S-BAR[]/?x[] -> Comp[] * S[-INV]/?x[] {}
    Scanner   |. . . . . [-] .| [5:6] 'you'
    Scanner   |. . . . . [-] .| [5:6] NP[-WH] -> 'you' *
    Completer |. . . . . [-> .| [5:6] S[-INV]/?x[] -> NP[] * VP[]/?x[] {}
    Scanner   |. . . . . . [-]| [6:7] 'like'
    Scanner   |. . . . . . [-]| [6:7] V[-AUX, SUBCAT=1] -> 'like' *
    Completer |. . . . . . [->| [6:7] VP[]/?x[] -> V[-AUX, SUBCAT=1] * NP[]/?x[] {}
    Completer |. . . . . . [-]| [6:7] VP[]/NP[] -> V[-AUX, SUBCAT=1] NP[]/NP[] *
    Completer |. . . . . [---]| [5:7] S[-INV]/NP[] -> NP[] VP[]/NP[] *
    Completer |. . . . [-----]| [4:7] S-BAR[]/NP[] -> Comp[] S[-INV]/NP[] *
    Completer |. . . [-------]| [3:7] VP[]/NP[] -> V[-AUX, SUBCAT=2] S-BAR[]/NP[] *
    Completer |. [-----------]| [1:7] S[+INV]/NP[] -> V[+AUX] NP[] VP[]/NP[] *
    Completer |[=============]| [0:7] S[-INV] -> NP[] S[]/NP[] *
    Completer |[=============]| [0:7] [INIT][] -> S[] *
    >>> for tree in trees: print tree
    (S[-INV]
      (NP[+WH] who)
      (S[+INV]/NP[]
        (V[+AUX, SUBCAT=3] do)
        (NP[-WH] you)
        (VP[]/NP[]
          (V[-AUX, SUBCAT=2] claim)
          (S-BAR[]/NP[]
            (Comp[] that)
            (S[-INV]/NP[]
              (NP[-WH] you)
              (VP[]/NP[] (V[-AUX, SUBCAT=1] like) (NP[]/NP[] )))))))


Let's load a German grammar:

    >>> cp = parse.load_earley('grammars/book_grammars/german.fcfg', trace=0)
    >>> sent = 'die katze sieht den hund'
    >>> tokens = sent.split()
    >>> trees = cp.nbest_parse(tokens)
    >>> for tree in trees: print tree
    (S[]
      (NP[AGR=[GND='fem', NUM='sg', PER=3], CASE='nom']
        (Det[AGR=[GND='fem', NUM='sg', PER=3], CASE='nom'] die)
        (N[AGR=[GND='fem', NUM='sg', PER=3]] katze))
      (VP[AGR=[NUM='sg', PER=3]]
        (TV[AGR=[NUM='sg', PER=3], OBJCASE='acc'] sieht)
        (NP[AGR=[GND='masc', NUM='sg', PER=3], CASE='acc']
          (Det[AGR=[GND='masc', NUM='sg', PER=3], CASE='acc'] den)
          (N[AGR=[GND='masc', NUM='sg', PER=3]] hund))))

Grammar with Binding Operators
------------------------------
The `bindop.fcfg`_ grammar is a semantic grammar that uses lambda
calculus.  Each element has a core semantics, which is a single lambda
calculus expression; and a set of binding operators, which bind
variables.

.. _bindop.fcfg: http://nltk.svn.sourceforge.net/svnroot/nltk/trunk/nltk/data/grammars/bindop.fcfg

In order to make the binding operators work right, they need to
instantiate their bound variable every time they are added to the
chart.  To do this, we use a special subclass of `Chart`, called
`InstantiateVarsChart`.

    >>> from nltk.parse.featurechart import InstantiateVarsChart
    >>> cp = parse.load_earley('grammars/sample_grammars/bindop.fcfg', trace=2,
    ...                        chart_class=InstantiateVarsChart)

A simple intransitive sentence:

    >>> from nltk.sem import logic
    >>> logic._counter._value = 100
     
    >>> trees = cp.nbest_parse('john barks'.split())
              |.jo.ba.|
    Scanner   |[--]  .| [0:1] 'john'
    Scanner   |[--]  .| [0:1] NP[sem=[bo={bo(\P.P(John),@x)}, core=<@x>]] -> 'john' *
    Completer |[-->  .| [0:1] S[sem=[bo={?b1+?b2}, core=<?vp(?subj)>]] -> NP[sem=[bo=?b1, core=?subj]] * VP[sem=[bo=?b2, core=?vp]] {?b1: {bo(\P.P(John),z101)}, ?subj: <IndividualVariableExpression z101>}
    Scanner   |.  [--]| [1:2] 'barks'
    Scanner   |.  [--]| [1:2] IV[sem=[bo={/}, core=<\x.bark(x)>]] -> 'barks' *
    Completer |.  [--]| [1:2] VP[sem=[bo={/}, core=<\x.bark(x)>]] -> IV[sem=[bo={/}, core=<\x.bark(x)>]] *
    Completer |[=====]| [0:2] S[sem=[bo={bo(\P.P(John),z101)}, core=<bark(z101)>]] -> NP[sem=[bo={bo(\P.P(John),z101)}, core=<z101>]] VP[sem=[bo={/}, core=<\x.bark(x)>]] *
    Completer |[=====]| [0:2] [INIT][] -> S[] *
    >>> for tree in trees: print tree
    (S[sem=[bo={bo(\P.P(John),z101)}, core=<bark(z101)>]]
      (NP[sem=[bo={bo(\P.P(John),z101)}, core=<z101>]] john)
      (VP[sem=[bo={/}, core=<\x.bark(x)>]]
        (IV[sem=[bo={/}, core=<\x.bark(x)>]] barks)))

A transitive sentence:

    >>> trees = cp.nbest_parse('john feeds a dog'.split())
              |.j.f.a.d.|
    Scanner   |[-] . . .| [0:1] 'john'
    Scanner   |[-] . . .| [0:1] NP[sem=[bo={bo(\P.P(John),@x)}, core=<@x>]] -> 'john' *
    Completer |[-> . . .| [0:1] S[sem=[bo={?b1+?b2}, core=<?vp(?subj)>]] -> NP[sem=[bo=?b1, core=?subj]] * VP[sem=[bo=?b2, core=?vp]] {?b1: {bo(\P.P(John),z102)}, ?subj: <IndividualVariableExpression z102>}
    Scanner   |. [-] . .| [1:2] 'feeds'
    Scanner   |. [-] . .| [1:2] TV[sem=[bo={/}, core=<\x y.feed(y,x)>]] -> 'feeds' *
    Completer |. [-> . .| [1:2] VP[sem=[bo={?b1+?b2}, core=<?v(?obj)>]] -> TV[sem=[bo=?b1, core=?v]] * NP[sem=[bo=?b2, core=?obj]] {?b1: {/}, ?v: <LambdaExpression \x y.feed(y,x)>}
    Scanner   |. . [-] .| [2:3] 'a'
    Scanner   |. . [-] .| [2:3] Det[sem=[bo={/}, core=<\Q P.exists x.(Q(x) & P(x))>]] -> 'a' *
    Completer |. . [-> .| [2:3] NP[sem=[bo={?b1+?b2+{bo(?det(?n),@x)}}, core=<@x>]] -> Det[sem=[bo=?b1, core=?det]] * N[sem=[bo=?b2, core=?n]] {?b1: {/}, ?det: <LambdaExpression \Q P.exists x.(Q(x) & P(x))>}
    Scanner   |. . . [-]| [3:4] 'dog'
    Scanner   |. . . [-]| [3:4] N[sem=[bo={/}, core=<dog>]] -> 'dog' *
    Completer |. . [---]| [2:4] NP[sem=[bo={bo(\P.exists x.(dog(x) & P(x)),@x)}, core=<@x>]] -> Det[sem=[bo={/}, core=<\Q P.exists x.(Q(x) & P(x))>]] N[sem=[bo={/}, core=<dog>]] *
    Completer |. [-----]| [1:4] VP[sem=[bo={bo(\P.exists x.(dog(x) & P(x)),z103)}, core=<\y.feed(y,z103)>]] -> TV[sem=[bo={/}, core=<\x y.feed(y,x)>]] NP[sem=[bo={bo(\P.exists x.(dog(x) & P(x)),z103)}, core=<z103>]] *
    Completer |[=======]| [0:4] S[sem=[bo={bo(\P.P(John),z102), bo(\P.exists x.(dog(x) & P(x)),z103)}, core=<feed(z102,z103)>]] -> NP[sem=[bo={bo(\P.P(John),z102)}, core=<z102>]] VP[sem=[bo={bo(\P.exists x.(dog(x) & P(x)),z103)}, core=<\y.feed(y,z103)>]] *
    Completer |[=======]| [0:4] [INIT][] -> S[] *
    >>> for tree in trees: print tree
    (S[sem=[bo={bo(\P.P(John),z102), bo(\P.exists x.(dog(x) & P(x)),z103)}, core=<feed(z102,z103)>]]
      (NP[sem=[bo={bo(\P.P(John),z102)}, core=<z102>]] john)
      (VP[sem=[bo={bo(\P.exists x.(dog(x) & P(x)),z103)}, core=<\y.feed(y,z103)>]]
        (TV[sem=[bo={/}, core=<\x y.feed(y,x)>]] feeds)
        (NP[sem=[bo={bo(\P.exists x.(dog(x) & P(x)),z103)}, core=<z103>]]
          (Det[sem=[bo={/}, core=<\Q P.exists x.(Q(x) & P(x))>]] a)
          (N[sem=[bo={/}, core=<dog>]] dog))))

Turn down the verbosity:

    >>> cp = parse.load_earley('grammars/sample_grammars/bindop.fcfg', trace=0,
    ...                        chart_class=InstantiateVarsChart)

Reuse the same lexical item twice:

    >>> trees = cp.nbest_parse('john feeds john'.split())
    >>> for tree in trees: print tree
    (S[sem=[bo={bo(\P.P(John),z104), bo(\P.P(John),z105)}, core=<feed(z104,z105)>]]
      (NP[sem=[bo={bo(\P.P(John),z104)}, core=<z104>]] john)
      (VP[sem=[bo={bo(\P.P(John),z105)}, core=<\y.feed(y,z105)>]]
        (TV[sem=[bo={/}, core=<\x y.feed(y,x)>]] feeds)
        (NP[sem=[bo={bo(\P.P(John),z105)}, core=<z105>]] john)))

    >>> trees = cp.nbest_parse('a dog feeds a dog'.split())
    >>> for tree in trees: print tree
    (S[sem=[bo={bo(\P.exists x.(dog(x) & P(x)),z106), bo(\P.exists x.(dog(x) & P(x)),z107)}, core=<feed(z106,z107)>]]
      (NP[sem=[bo={bo(\P.exists x.(dog(x) & P(x)),z106)}, core=<z106>]]
        (Det[sem=[bo={/}, core=<\Q P.exists x.(Q(x) & P(x))>]] a)
        (N[sem=[bo={/}, core=<dog>]] dog))
      (VP[sem=[bo={bo(\P.exists x.(dog(x) & P(x)),z107)}, core=<\y.feed(y,z107)>]]
        (TV[sem=[bo={/}, core=<\x y.feed(y,x)>]] feeds)
        (NP[sem=[bo={bo(\P.exists x.(dog(x) & P(x)),z107)}, core=<z107>]]
          (Det[sem=[bo={/}, core=<\Q P.exists x.(Q(x) & P(x))>]] a)
          (N[sem=[bo={/}, core=<dog>]] dog))))
