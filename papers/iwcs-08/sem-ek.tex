\documentclass[11pt,a4paper]{article}

% \usepackage[pdftex,colorlinks=true,
%                       pdfstartview=FitV,
%                       linkcolor=blue,
%                       citecolor=blue,
%                       urlcolor=blue
%           ]{hyperref}

\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{url}
\usepackage{wrapfig}

\newcommand{\DRS}{\textsc{drs}}
\newcommand{\DRT}{\textsc{drt}}
\newcommand{\FOL}{\textsc{fol}}
\newcommand{\NLP}{\textsc{nlp}}
\newcommand{\NLTK}{\textsc{nltk}}
\newcommand{\LF}{\textsc{lf}}

%\pdfinfo{
%   /Author 		(Daniel H. Garrette)
%   /Title  		(Computational Semantics in the Natural Language Toolkit)
%   /Subject 		(Computational Semantics)
%   /Keywords 		(Natural Language Processing;Computational Semantics;Natural Language Toolkit;NLTK)
%}

\usepackage[round]{natbib}
% \bibpunct{[}{]}{;}{a}{,}{,}
\bibliographystyle{plainnat}

\usepackage{lingmacros}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Copied from covington.sty by Michael A. Covington
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\dhgdrs}[2]
{
    {
    \it
    \begin{tabular}{|l|}
    \hline
    ~ \vspace{-2ex} \\
    #1
    \\
    ~ \vspace{-2ex} \\
    \hline
    ~ \vspace{-2ex} \\
    #2
    \\
    ~ \\    % can't vspace here or the line will come out wrong
    \hline
    \end{tabular}
    }
}
\newcommand{\dhgsdrs}[3]
{\begin{tabular}{l}
\mbox{\it #1} \\
~ \\
\dhgdrs{#2}{#3}
\end{tabular}}\newcommand{\dhgifdrs}[4]
{
  \mbox{\dhgdrs{#1}{#2}~~{\large $\Rightarrow$}~\dhgdrs{#3}{#4}}
}
\newcommand{\dhgalifdrs}[4]
{
  \mbox{$\!\!\!$\dhgdrs{#1}{#2}~~{\large $\Rightarrow$}~\dhgdrs{#3}{#4}}
}
\newcommand{\dhgnegdrs}[2]
{
  \mbox{{\large $\neg$}\dhgdrs{#1}{#2}}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
% END covington.sty
%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newcommand{\dhgcode}[1]{{\tt #1}}
%\newcommand{\dhgcode}[1]{\begin{verbatim}#1\end{verbatim}}

\begin{document}


\title{An Extensible Toolkit for Computational Semantics}
\author{Daniel H. Garrette \and Ewan Klein}
\date{\today}
\maketitle

\section{Introduction}

In this paper we focus on the software for computational semantics provided
by the Python-based Natural Language Toolkit (\NLTK). The semantics
modules in \NLTK\ are
inspired in large part by the approach developed in \citet{BB}.
Since Blackburn and Bos have also provided a software suite to
accompany their excellent textbook, one might ask what the
justification is for the \NLTK\ offering, which is similarly oriented
towards teaching introductory courses.

This question can be answered in a number of ways. First, we believe
there is intrinsic merit in the availability of different
software tools for semantic analysis, even when there is some
duplication of coverage; and this will become more true as
computational semantics starts to be as widely studied as computational
syntax. One rarely hears the objection that there are too many
parsers available, for example. 

Second, whatever the relative merits of Prolog vs.\ Python as
programming languages, there is surely an advantage in offering
students and instructors a choice in this respect. Given that many
students have either been exposed to Java, or have had no programming
experience at all, Python offers an easy route to accomplishing
interesting results at the expense of only a shallow learning curve.

Third, \NLTK\ is a rapidly developing, open source
project\footnote{See \url{http://nltk.org}} with a broad coverage of
natural language processing (\NLP); see \citet{Multidisciplinary} for
a recent overview. This wide functionality has a number of benefits,
most notably that lexical, syntactic and semantic processing can be
carried out within a uniform computational framework. As a result,
\NLTK\ makes it much easier to include some computational semantics in
a broad course on natural language analysis, rather than having to
devote a whole course exclusively to computational semantics.

Fourth, \NLTK\ is distributed along with a
large collection of corpora, along with easy-to-use corpus readers.
This collection, which currently stands at 45 corpora, includes
parsed, POS-tagged, plain text, categorized text, and
lexicons.\footnote{\url{http://nltk.org/corpora.html}} This makes it
easy for students to go beyond writing toy grammars, and instead
starting to grapple with the complexities of semantically analysing
realistic bodies of text.

Fifth, \NLTK\ is not just for students. Although Python is slower than
languages like Java and C++, its ease of use makes rapid prototyping
an attractive option for the researcher. Building an experimental
set-up in \NLTK\ to test a hypothesis is straightforward and quick,
and the rich variety of existing components in the toolkit allows
rapid assembly of quite sophisticated processing pipelines.



% The semantics-related code in NLTK is inspired in a large part by the
% approach developed by Blackburn and Bos \cite{BB}.  However, by
% incorporating these ideas into NLTK, we provide one unified
% environment in which students and researchers may study and experiment
% with a wide range of NLP techniques.

\section{Overview}
\label{sec:overview}

% \citet{Klein06altw} describes a preliminary effort in \NLTK\ to
% support model checking of first order formulas, using the untyped
% lambda calculus as a syntactic basis. Since then, there has been a
% rapid growth 
% The Natural Language Toolkit (NLTK)\footnote{www.nltk.org} is a
% collection of open-source modules and corpora that allow students and
% researchers to experiment with natural language processing techniques.
% The toolkit is written entirely in the programming language
% Python\footnote{www.python.org}, an object-oriented language
% well-suited for the study of NLP \cite{Multidisciplinary}.  The last
% two years have seen tremendous growth 
% in the amount of semantics-related code in the toolkit.  For example,
% \NLTK\ now contains modules for first order predicate logic, Discourse
% Representation Theory (\DRT) \cite{KampReyle}, Hole Semantics
% \cite{BB}, Glue Semantics \cite{Dalrymple2001}, first order theorem proving (both
% third-party and home-grown), model building and nonmonotonic
% reasoning. 
% In the following sections, we will present an overview of the
% computational semantics components in \NLTK\ and illustrate their use.


\begin{wrapfigure}{r}{3in}
%\label{modules}
  \centering
\includegraphics[scale=.6]{modules}  
%  \caption{Overview of semantic processing in NLTK}
\end{wrapfigure}

Like \citet{BB}, we assume that one of the most important tasks for
the teacher is to ground students in the basic concepts of first order
logic and the lambda calculus, model-theoretic interpretation and
inference. This provides a basis for exploring more modern approaches
like Discourse Representation Theory (\DRT; \citet{KampReyle}) and
underspecification.

In the accompanying figure, we give a diagrammatic overview of the main
semantics-related functionality that is currently available in \NLTK.
Logical forms (\LF s) can be induced as result of syntactic parsing,
using either feature-based grammars that are processed with an Earley
chart parser, or else by associating \LF s with the output of a
broad-coverage dependency parser. Our basic \LF s are expressions of
first order logic, supplemented with the lambda operator. However, we
also admit Discoures Representation Structures (\DRS s), and
underspecified \LF s can be built using either Hole Semantics
\citep{BB} or Glue Semantics \citep{Dalrymple:1999:RRB,Dalrymple2001}. Once we have
constructed \LF s, they can be evaluated in a first order model
\citep{Klein06altw}, tested for equivalance and validity in a variety
of theorem provers, or tested for consistency in a model builder. Note
that we have interfaces to third party components, specifically
Prover9 and Mace.

We do not have space in this paper to discuss all of these components,
but will try to present some of the key aspects, in particular noting
some points of difference \textit{vis-\`a-vis} \citet{BB}.


\section{Logical Form}

\subsection{First Order Predicate Logic}
From a pedagogical point of view, it is usually important to ensure
that students have some grasp of the language of first order predicate
logic (\FOL), and can also manipulate $\lambda$ abstracts.  The
\dhgcode{nltk.sem.logic} module contains an object-oriented approach
to representing first-order predicate logic plus
$\lambda$-abstraction. Logical formulas are typically input to the
\texttt{logic} parser as strings, and then represented as instances of
various subclasses of \texttt{Expression}, as we will see shortly.

%  Shown below,
% (\ref{logic1}) depicts a representation of ``Every girl chases a dog''.

% \enumsentence{\label{logic1}\dhgcode{all x.(girl(x) -$>$ exists y.(dog(x) \& chase(x,y)))}}

% The syntax used attempts to mimic the standard syntax of first-order logic.  The logical connectives used are ``\textbf{\dhgcode{-}}", ``\textbf{\dhgcode{\&}}", ``\textbf{\dhgcode{$|$}}", ``\textbf{\dhgcode{-$>$}}", and ``\textbf{\dhgcode{$<$-$>$}}" for \emph{negation}, \emph{conjunction}, \emph{disjunction}, \emph{implication}, and \emph{biconditional}, respectively.  The quantifiers $\forall$ and $\exists$ are represented as \textbf{\dhgcode{all}} and \textbf{\dhgcode{exists}}.  Additionally, for lambda-abstraction, the backslash, ``\textbf{\dhgcode{\textbackslash}}", is used to represent \textbf{\emph{$\lambda$}}.

Note that one of the features of Python is an interactive interpreter
which allows the user to load modules (i.e., using an \texttt{import}
statement) and to enter Python expressions and statements for
evaluation. In the example below and subsequently, \verb!>>>! is the
Python interpreter's prompt. 
\begin{verbatim}
>>> from nltk.sem import logic
>>> lp = logic.LogicParser()
>>> e = lp.parse('all x.(girl(x) -> exists y.(dog(y) & chase(x,y)))')
>>> e
<AllExpression all x.(girl(x) -> exists y.(dog(y) & chase(x,y)))>
\end{verbatim}

As illustrated, \texttt{e} is an object belonging to the class
\texttt{AllExpression}, itself a subclass of \texttt{Expression}.  All
such subclasses have numerous methods that can be used to perform
standard logical operations. For example, the \dhgcode{simplify()}
method is used to perform $\beta$-conversion; the \dhgcode{free()}
method can be used to find all the free variables in an expression;
and quantified expressions (such as \texttt{AllExpression}s) have an
\dhgcode{alpha\_convert()} method that will perform
$\alpha$-conversion.  The \texttt{logic} module will also $\alpha$-convert
automatically when it is appropriate to avoid name-clashes in the
\dhgcode{replace()} method.\footnote{Since '\` is a special character
  in Python, we use the \texttt{r} prefix on strings to indicate that
  we are using `raw' strings.}

\begin{verbatim}
>>> e1 = lp.parse(r'\x.P(x)(y)')
>>> print e1.simplify()
P(y)
>>> e2 = lp.parse('all x.P(x,a,b)')
>>> print e2.free()
set([<Variable('a'), Variable('b')])
>>> print e2.alpha_convert(Variable('z'))
all z.P(z,a,b)
>>> e3 = lp.parse('x')
>>> print e2.replace(Variable('b'), e3)
all z1.P(z1,a,x)
\end{verbatim}

% A feature of Python that lends itself well to working with logical
% expressions is its operator overloading.  The operators \dhgcode{-},
% \dhgcode{\&}, \dhgcode{$|$}, \dhgcode{$>$}, \dhgcode{$<$} can be used
% for \emph{negation}, \emph{conjunction}, \emph{disjunction},
% \emph{implication}, and \emph{biconditional}, respectively.  The
% parenthesis of a function call are overloaded to perform function
% application.  Python's built-in \dhgcode{lambda} operator can also be
% used with logical expressions because it uses function calls.

% \begin{verbatim}
% >>> P = parser.parse('P')
% >>> x = parser.parse('x')
% >>> y = parser.parse('y')
% >>> print P(x) & P(y)
% (P(x) & Q(y))
% >>> print (lambda x: P(x))(y)
% P(y)
% \end{verbatim}

Allowing students to build simple first order models, and evaluate
expressions in those models, can be useful for helping them clarify
their intuitions about quantification. In the next example, we show a
simple means of specifying a model, and then using it to determine the
set of satisfiers of an open formula such as
\verb!exists y.(girl(y) & chase(x,y))!.\footnote{The triple quotes """
  allow us to break a logical line across several physical lines.}
\begin{verbatim}
>>> from nltk.sem import evaluate, parse_valuation
>>> v = """
... suzie => s
... fido => f
... rover => r
... girl => {s}
... chase => {(f, s), (r, s), (s, f)}
... """
>>> val = parse_valuation(v) #create a Valuation
>>> m = evaluate.Model(val.domain, val) #initialize a Model
>>> g = evaluate.Assignment(val.domain) #initialize an Assignment
>>> e4 = lp.parser('exists y. (girl(y) & chase(x, y))')
>>> m.satisfiers(e4, 'x', g) #check satisfiers of e4 wrt to x
set(['r', 'f'])
\end{verbatim}

In \citet{BB}, $\lambda$-abstracts are second-class citizens, used
exclusively as a `glue' mechanism for composing meaning
representations. Although we use $\lambda$-abstracts as glue too,
abstracts over individual variables are interpreted in \NLTK, namely as
characteristic functions.

\subsection{Discourse Representation Theory}
As mentioned earlier, \NLTK\ contains an extension to the
\dhgcode{logic} module for working with Discourse Representation
Theory (\DRT) \citep{KampReyle}.  Using \NLTK's \dhgcode{nltk.sem.drt}
library, we can use the standard linear representation of a \DRS\ as a
pair of discourse referents and conditions:
% (\ref{drt3}), where the
% first set of brackets holds a list of variables representing the
% discourse referents and the second set of brackets holds a list of
% expressions representing the discourse conditions.

%\enumsentence{\label{drt2} \dhgdrs{j,d}{(John = j)\\dog(d)\\sees(j,d)}}
\enumsentence{\label{drt3} \dhgcode{DRS([j,d],[(John = j), dog(d), sees(j,d)])}}

\noindent
In addition to the functionality available in the \texttt{logic} module
for first order logic, \DRT\ expressions have a ``DRS concatenation"
operator, represented as the ``+" symbol.  The concatenation of two
\DRS s results in a single \DRS\ containing the discourse referents and the conditions from both arguments.  \DRS\ concatenation will
also automatically $\alpha$-convert bound variables to avoid
name-clashes.  The ``+" symbol is recognized by the parser as well as
being overloaded so that \DRT\ expressions can be "added" together
easily.

\begin{verbatim}
>>> from nltk.sem import drt
>>> dp = drt.DrtParser()
>>> d1 = dp.parse('DRS([x],[walk(x)]) + DRS([y],[run(y)])')
>>> print d1
(DRS([x],[walk(x)]) + DRS([y],[run(y)]))
>>> print d1.simplify()
DRS([x,y],[walk(x), run(y)])
>>> d2 = dp.parse('DRS([x,y], [Bill(x), Fred(y)])')
>>> d3 = dp.parse("""DRS([], [DRS([u],[Porsche(u), own(x,u)])
...  ->  DRS([v],[Ferrari(v),own(y,u)])])""")
>>> d4 = d2 + d3
>>> print d4.simplify()
DRS([x,y],[Bill(x), Fred(y), (DRS([u],[Porsche(u), own(x,u)]) 
-> DRS([v],[Ferrari(v), own(y,u)]))])
\end{verbatim}

\noindent
\DRT\ expressions can be converted to their first-order predicate
logic equivalents using the \dhgcode{toFol()} method. There is also a
\texttt{draw()} method which produces a graphical rendering of a \DRS.
\hfill\\
\begin{wrapfigure}{r}{2in}
\includegraphics[scale=.5]{drs.png}
%\caption{A DRS Screenshot} 
\end{wrapfigure}

\begin{verbatim}
>>> print d1.toFol()
(exists x.walk(x) & exists y.run(y))
>>> d4.simplify().draw()
\end{verbatim}

% \begin{verbatim}
% >>> d4 = drtparse(r'DRS([x],[walk(x), talk(x)])')
% >>> print d4.toFol()
% exists x.(walk(x) & talk(x))
% >>> d4.draw()
% \end{verbatim}

Since the $\lambda$ operator is also applicable to \DRT\ expressions,
the \dhgcode{sem.drt} may be used as a replacement for
\dhgcode{sem.logic} in building compositional semantics.

\clearpage

% \subsection{Anaphora Resolution}
% \NLTK\ also includes a extension to the DRT module that allows the user to resolve anaphoric pronouns, located in \dhgcode{nltk.sem.drt\_resolve\_anaphora}.  For instance, the three sentence discourse ``John walks.  Bill runs.  He talks." is shown below, along with its resolution.  The anaphora resolution procedure used by \NLTK\ replaces any instance of the function``PRO(x)" with an expression equating ``x'' to a list of possible antecedents of ``x".  The list of possible antecedents contains any discourse referent in an accessible DRS.

% \begin{verbatim}
% >>> d = drtparse(r'DRS([j,b,x],[(John=j), walk(j), (Bill=b), 
% run(b), PRO(x), talk(x)])')
% >>> print d.resolve_anaphora()
% DRS([j,b,x],[(John=j), walk(j), (Bill=b), run(b), (x=[j,b]), 
% talk(x)])
% \end{verbatim}

% The anaphora resolution logic has been separated into a separate module so that users may write their own anaphora resolution procedures and swap them in.

\section{Syntax, Scope Ambiguity and Underspecification}

Two key questions in introducing students to computational semantics are:
\begin{itemize}
\item How are semantic representations constructed from input sentences?
\item What is scope ambiguity and how is it captured?
\end{itemize}
A standard approach is to start off with a rule-to-rule induction of
logical forms, followed by some kind of underspecified representations
which can be resolved to produce different readings of ambiguous
sentences. 

\NLTK\ includes a suite of parsing tools, amongst which is a chart
parser for context free grammars augmented with feature structures. A
`semantics' feature \texttt{sem} allows us to compose the
contributions of constituents in order to build a logical form for a
complete sentence.  To illustrate, the following simple grammar
handles quantification and intransitive verbs (where values such as
\texttt{?subj} and \texttt{?vp} are unification variables, while
\texttt{P} and \texttt{Q} are $\lambda$-bound object language
variables):

\begin{verbatim}
S[sem = <?subj(?vp)>] -> NP[sem=?subj] VP[sem=?vp]
VP[sem=?v] -> IV[sem=?v]
NP[sem=<?det(?n)>] -> Det[sem=?det] N[sem=?n]
Det[sem=<\P \Q.exists x.(P(x) & Q(x))>] -> 'a'
N[sem=<\x.dog(x)>] -> 'dog'
IV[sem=<\x.bark(x)>] -> 'barks'
\end{verbatim}
Using this grammar, we can parse the sentence \textit{A dog barks} and view
its semantics. 
%  It should be noted that if the
% \dhgcode{load\_earley()} method's argument \dhgcode{trace} is set to a
% positive number, then the parser prints detailed information showing
% how the sentence was parsed.  
The \dhgcode{load\_earley()} method
takes as an optional parameter \dhgcode{logic\_parser} the name of a
logic-parser to process the value of the \texttt{sem} feature, thus
allowing different kinds of logical forms to be constructed.
\begin{verbatim}
>>> from nltk.parse import load_earley
>>> parser = load_earley('grammars/sem1.fcfg', trace=0)
>>> tokens = 'a dog barks'.split()
>>> trees = parser.nbest_parse(tokens)
>>> print trees[0].node['sem'].simplify()
exists x.(dog(x) & bark(x))
\end{verbatim}

Underspecified logical forms allow us to loosen the relation between
syntactic and semantic representations. We consider in turn Hole
Semantics and Glue Semantics. Since the former will be familiar from 
\citep{BB}, we devote most of our attention to presenting Glue
Semantics.

\subsection{Hole Semantics}

Hole Semantics in \NLTK\ is handled by the
\dhgcode{nltk.sem.hole} module, which uses a context free grammar to
generate an underspecified logical form.  Since the latter is itself a
formula of first order logic, we can continue to use the \texttt{sem} feature
in the context free grammar:

\small
\begin{verbatim}
N[sem=<\x h l.(PRED(l,dog,x) & LEQ(l,h) & HOLE(h) & LABEL(l))>] -> 'dog'
\end{verbatim}
\normalsize

The Hole Semantics module uses a `plugging algorithm' to derive the
sentence's readings based on the underspecified LF.  Because the `sem'
feature can also be given \DRT\ terms as values, the Hole Semantics module can
easily be extended to generate \DRS s.

\begin{verbatim}
>>> from nltk.sem import hole
>>> readings = hole.main('every girl chases a dog')
>>> for r in reading: print r
exists z1.(dog(z1) & all z2.(girl(z2) -> chase(z1,z2)))
all z2.(girl(z2) -> exists z1.(dog(z1) & chase(z1,z2)))
\end{verbatim}


\subsection{Glue Semantics}
Glue Semantics \citep{Dalrymple:1999:RRB,Dalrymple2001}, or Glue for
short, is an approach
to compositionality that tries to handle semantic ambiguity in an
elegant way by using resource-sensitive logic to assemble meaning
expressions.  

The approach uses proofs over `meaning constructors' of the form
$\cal{M}: \cal{G}$, where $\cal{M}$ is a meaning represention and
$\cal{G}$ is a term of linear logic.  The linear logic terms dictate
how the meaning expressions can be combined.  Each different proof
that can be derived reflects a different semantic reading of the
entire sentence.

The variant of linear logic that we use has \emph{implication} as its
only operator, so the primary operation during the proof is Modus
Ponens.  Linear logic is an appropriate logic to serve as `glue'
because it is resource-sensitive.  This means that when Modus Ponens
is used to combine two terms to create a new one, the two original
terms are `consumed', and cannot be used again in the proof.
Additionally, every premise must be used for the proof to be valid.
This resource-sensitivity dictates that each word contributes its
meaning exactly once to the meaning of the whole.

\enumsentence{\label{glue1} A, (A $\multimap$ B) $\vdash$ B}
\enumsentence{\label{glue2} A, (A $\multimap$ B) $\nvdash$ A, B}
\enumsentence{\label{glue3} A, A, (A $\multimap$ B) $\nvdash$ B}
\NLTK's \dhgcode{nltk\_contrib.gluesemantics.linearlogic} module
contains an implementation of linear logic. 

The primary rule used to compose glue formulas is given as
(\ref{glue4}).  This reveals that the function-application of meaning
expressions reflects (via the Curry-Howard isomorphism) the
application of Modus Ponens in a linear logic proof, which
is what allows Glue to guide the construction of complex meaning expressions.

\enumsentence{\label{glue4} $\phi$ : A, $\psi$ : (A $\multimap$ B)
  $\vdash$ $\psi$($\phi$) : B}
\NLTK\ includes
\dhgcode{nltk\_contrib.gluesemantics.glue} for Glue based on \FOL\
meaning expressions, and a parallel module
\dhgcode{nltk\_contrib.gluesemantics.drt\_glue} for Glue based on \DRT.  

The following example shows the basic way that
Glue formulas are created and combined to derive a logical form for
\textit{John walks}: 

\begin{verbatim}
>>> from nltk_contrib.gluesemantics.glue import GlueFormula
>>> john = GlueFormula(r'john', 'g')
>>> walks = GlueFormula(r'\x.walk(x)', '(g -o f)')
>>> john_walks = walks.applyto(john)
>>> print john_walks.meaning.simplify()
walk(john)
\end{verbatim}
Thus, the non-logical constant \textit{john} is associated with the
Glue term $g$, while the meaning expression $\lambda x.walk(x)$ is
associated with $(g \multimap f)$ since it is a function that
takes $g$ as input and returns the meaning expression $f$,
corresponding to the whole
sentence.  Consequently, a proof of $f$ from the premises is a derivation
of a meaning representation for the sentence.

%\subsection{Semantic Ambiguity}

Scope ambiguity, for example in quantifiers, requires the use of
\textbf{variables} in the Glue terms. Such variables may be bound to
any linear logic constant, so long as the binding is consistent with
previous bindings. Let's assume that the quantified noun phrase
\textit{every girl} is represented as (\ref{glue5}):

\enumsentence{\label{glue5} \textbf{[every girl]} $\lambda Q.\forall x.(girl(x) \rightarrow Q(x))$ : ((g $\multimap$ G) $\multimap$ G)}
Then the glue derivation procedure shown below correctly
generates two readings for the sentence \textit{Every girl chases a dog}:
\begin{verbatim}
>>> from nltk_contrib.gluesemantics.glue import GlueFormula, Glue
>>> a = GlueFormula(r'\Q.all x.(girl(x) -> Q(x))', '((g -o G) -o G)')
>>> b = GlueFormula(r'\x y.chase(x,y)', '(g -o (h -o f))')
>>> c = GlueFormula(r'\Q.exists x.(dog(x)&Q(x))', '((h -o H) -o H)')
>>> for reading in Glue.compile_to_meaning([a,b,c]):
...     print reading.meaning.simplify()
all x.(girl(x) -> exists y.(dog(x) & chase(x,y)))
exists y.(dog(x) & all x.(girl(x) -> chase(x,y)))
\end{verbatim}


\section{Inference tools}
In order to perform inferences with semantic representations, \NLTK\
includes interfaces to call Theorem Provers and Model Builders.  As is
discussed in \cite{BB}, these tools can be very useful in
computational semantics.  Below is a demonstration showing how \NLTK\
can be used to verify that if every dog barks, and Rover is a dog,
then it is true that Rover barks:

\begin{verbatim}
>>> from nltk.inference import inference
>>> a = parser.parse('all x.(dog(x) -> bark(x))')
>>> b = parser.parse('dog(rover)')
>>> c = parser.parse('bark(rover)')
>>> prover = inference.get_prover(c, [a,b])
>>> prover.prove()
True
\end{verbatim}

\subsection{Logical Equivalence}
A theorem prover can also be used to check the logical equivalence of
expressions.  For two expressions $A$ and $B$, we can pass $(A\iff B)$
into a theorem prover and know that the theorem will be proved if and
only if the expressions are logically equivalent.  NLTK's standard
equality operator for Expressions (``==") is able to handle situations
where the only different between two expressions are the bound
variable's names by alpha converting one expression to make it look
like the other, but more complex logic rules would be too intensive to
use every time we checked for equality of two expressions.  Therefore,
the logic package in \NLTK\ has a separate method,
\dhgcode{tp\_equals}, for checking ``logical equality".

\begin{verbatim}
>>> a = parser.parse('all x.walk(x)')
>>> b = parser.parse('all y.walk(y)')
>>> a == b
True
>>> c = parser.parse('-(P(x) & Q(x))')
>>> d = parser.parse('-P(x) | -Q(x)')
>>> c == d
False
>>> c.tp_equals(d)
True
\end{verbatim}

\subsection{Inference tools in NLTK}
\NLTK\ contains a tableau-based first-order theorem prover implemented
totally in Python.  This module allows students to study how the
tableau method of theorem proving works, and to provides an
opportunity for experimentation.  In addition, \NLTK\ provides
interfaces to the off-the-shelf products Prover9, a theorem prover,
and Mace4, a model builder \cite{McCune}.  Both Prover9 and Mace4 are
high-performing tools suitable for research.



% \subsection{Nonmonotonic Reasoning}
% \NLTK\ contains a few simple demonstrations of nonmonotonic reasoning techniques.  There are three nonmonotonic ``assumptions" implemented in NLTK.  Each assumption is implemented as a "decorator" of a theorem prover object.  The decorator works by modifying the list of assumptions and the goal that gets passed to the prover.  By using the decorator pattern, the theorem prover can be wrapped by more than one decorator, thus applying more than one nonmonotonic ``assumption" during the proof.

% The closed domain assumption states that there are no entities in the domain other than the entities found in the assumptions and goal.  The closed domain decorator, therefore, finds all the domain entities and then removes quantifications by spelling out all the entire domain instead.  For example, if the domain contained ``$john$" and ``$mary$", then the assumption ``$\forall x.walk(x)$" would be replaced with ``$walk(john)~\&~walk(mary)$" and ``$\exists x.walk(x)$" would be replaced with ``$walk(john)~|~walk(mary)$".

% \begin{verbatim}
% >>> p1 = lp.parse(r'walk(Socrates)')
% >>> p2 = lp.parse(r'walk(Bill)')
% >>> c = lp.parse(r'all x.walk(x)')
% >>> prover = inference.get_prover(c, [p1,p2])
% >>> prover.prove()
% False
% >>> cdp = ClosedDomainProver(prover)
% >>> print cdp.goal()
% (walk(Socrates) & walk(Bill))
% >>> cdp.prove()
% True
% \end{verbatim}

% The unique names assumption states that two distinct names specify different domain entities unless it can be proven that they are equal.  For every pair of domain entities ``$d1$" and ``$d2$", if ``$d1 = d2$" cannot be proven from the starting list of assumptions, then the unique names decorator adds the assumption ``$-(d1 = d2)$".

% The closed domain assumption states that the only domain entities that have a particular property are the entities that it can be proven have the property.  For a property ``$P$", the decorator finds any individuals ``$A$" such that ``$P(A)$" and any properties ``$Q$" such that ``$\forall x.(Q(x) \rightarrow P(x))$".  The decorator then \textbf{completes} ``$P$" by adding the assumption ``$\forall x.(P(x) \rightarrow (Q(x)~|~(x = A)))$".  In the case where no domain entities can be proven to have property ``$P$", then the assumption ``$\forall x.-P(x)$" is added.

\section{Putting Things Together}

\subsection{Discourse Processing}
\NLTK\ contains a discourse processing module,
\dhgcode{nltk.inference.discourse}, similar to the CURT program
presented in \cite{BB}.  This module handles sentences incrementally,
keeping track of all possible threads when there is ambiguity.

The module can use theorem proving and model building tools to check
for consistency and informativeness when processing new sentences.
The module checks for informativeness by using a theorem prover to
check whether the previous sentences entail the new sentence.  If the
new sentence is entailed, then it is not informative.  The discourse
processor checks for consistency by passing all previous sentences
along with the new sentence into a model builder as a list of
assumptions.  The discourse is treated as consistent if and only if
the model builder is able to build a model.

\subsection{Textual Entailment}
Recognizing Textual Entailment is a task in which a computer is given
a piece of text and a hypothesized conclusion and asked to determine
whether the text entails the hypothesis.  For example if the text is
``John owns a red convertible" and the hypothesis is ``John has a red
car", then the computer would have to answer ``True" since ``owns" is
a synonym of ``has" and a convertible is a type of car.

\NLTK\ includes a few simple modules to demonstrate how entailment can
be recognized.  The simplest is the Bag of Words RTE tagger, located
in \dhgcode{nltk\_contrib.rte.bow}.  The BOW tagger works by
tokenizing both the text and the hypothesis, creating a set of the
words in the text and a set for the words in the hypothesis, and
computing the overlap by dividing the number of words that appear in
both sets by the total number of words.  If the overlap is more than a
certain percentage, say 33 percent, then the the tagger deems that the
hypothesis is entailed.  This approach is clearly overly simplistic
for a real entailment system because it does not take into account any
relationships between the words in sentences.

A more sophisticated, and semantically-oriented, approach is seen in
the Logical Entailment RTE tagger, located in
\dhgcode{nltk\_contrib.rte.logicentail}.  Based on \cite{BosRTE}, the
tagger begins by building a first-order logic semantic representation
of both the text and the hypothesis.  It then runs a theorem prover
and model builder in parallel with the text as the assumption and the
hypothesis as the goal to check whether the text entails the
hypothesis.  If a proof is found by the theorem prover, then we know
that entailment holds.

To add some robustness to the logical entailment, since it is unlikely
that entailment will be proven without any additional information, if
the first attempt fails, then the tagger will generate some background
knowledge.  The background knowledge is a set of formulas
automatically generated based on the words in text and hypothesis.
For example, if both ``convertible" and ``car" appeared in the set of
words in the text and hypothesis, then we should generate the formula
``$\forall x.(convertible(x) \rightarrow car(x))$" since a convertible
is a type of car.  The primary source of background knowledge is a
computerized dictionary called WordNet.  \NLTK\ includes an interface
to WordNet in \dhgcode{nltk.wordnet}.  The WordNet interface allows
the user to find, for any given word, its synonyms, hypernyms,
hyponyms, part holoyms, and member meronyms.  Each of these
relationships can be converted into an appropriate formula for
background knowledge.


\section{Conclusions}
The primary advantage to using \NLTK\ is its wealth of NLP tools.
NLTK's code base includes support for corpus access, tokenizing,
stemming, tagging, chunking, parsing, clustering, classification,
language modeling, unification, and more within a unified environment
\cite{Multidisciplinary}.  This range of modules makes it quick and
easy to write complex functionality.  Many of the demonstrations of
the semantics modules utilize other NLP tools; for example, textual
entailment uses stemming and Glue Semantics uses dependency parsing.

NLTK's semantics functionality has been written with extensibility in
mind.  Within the \dhgcode{logic} module, the \dhgcode{LogicParser}
uses a basic parsing template and contains hooks that an extending
module can use to supplement or substitute functionality.  Also, the
base \dhgcode{Expression} class in \dhgcode{logic} as well as any
derived classes can be extended, allowing variations to reuse the
existing functionality.  For example, the DRT and linear logic modules
are implemented as extensions to \dhgcode{logic.py}.

The theorem prover and model builder code has also been carefully
architected to allow extensions.  The \dhgcode{nltk.inference.api}
library exposes the framework for the inference architecture.  The
library may be used to create interfaces with other theorem provers
and model builders just as we have done with Prover9, Mace, and the
tableau prover.  The \dhgcode{ProverDecorator} base class is also
available so that new decorators for the theorem provers can be
implemented, similar to those in \dhgcode{nonmonotonic.py}.


% \section{Acknowledgments}
% I would like to thank Ewan Klein for his continuous support and guidance on my work with \NLTK\ and on computational semantics in general, as well as initially inviting me to join the \NLTK\ project.  I would also like to thank Steven Bird and Edward Loper for their advice and feedback on my \NLTK\ contributions
.


\bibliography{sem}{}

\end{document}
