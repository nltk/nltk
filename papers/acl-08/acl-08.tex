% $Rev%\documentclass[11pt]{article}\usepackage{acl08}\usepackage{times}\usepackage{latexsym}\usepackage{epsfig,url}\newcommand{\NLP}{\textsc{nlp}}\newcommand{\NLTK}{\textsc{nltk}}\setlength\titlebox{6.5cm}    % Expanding the titlebox\title{Multidisciplinary Instruction with the Natural Language Toolkit}\author{}% \author{Joakim Nivre \\%   School of Mathematics and Systems Engineering \\%   V\"{a}xj\"{o} University \\%   SE-35195, V\"{a}xj\"{o}, Sweden \\%   {\tt nivre@msi.vxu.se} \And%   Noah A. Smith \\%   Language Technologies Institute \\%   Carnegie Mellon University \\%   Pittsburgh, PA 15213, USA\\%   {\tt nasmith@cs.cmu.edu}}\date{}\begin{document}\maketitle\begin{abstract}  The Natural Language Toolkit (\NLTK) is widely used for teaching  natural language processing to students majoring in linguists or  computer science.  This paper describes the design of \NLTK, and  reports on how it has been used effectively in classes that involve  a combination of linguists and computer scientists.  It focusses  on three key issues: getting started with a course, fostering  classroom interaction, and organizing project work.\end{abstract}\section{Introduction}Natural Language Processing (\NLP) is often taught within the confinesof a single-semester course at advanced undergraduate level orpostgraduate level. Many instructors have found that it is difficultto cover both the theoretical and practical sides of the subject insuch a short span of time, especially when some of the students havelimited programming experience. Some courses focus on theory to theexclusion of practical exercises, and deprive students of thechallenge and excitement of writing programs to automatically processlanguage. Other courses are simply designed to teach programming forlinguists, and do not manage to cover any significant \NLP\ content. TheNatural Language Toolkit (\NLTK) was originally developed to addressthis problem, making it feasible to cover a substantial amount oftheory and practice within a single-semester course, for an audienceconsisting of both linguists and computer scientists.\NLTK\ is a suite of Python modulesdistributed under the GPL open source license from \url{nltk.org}.\NLTK\ comes with a large collection of corpora, extensivedocumentation, and hundreds of exercises.  It has been used in over 50university courses in more than 15countries.\footnote{\url{http://nltk.org/courses.html}}Since its inception in 2001, \NLTK\ has undergone considerableevolution, to a large extent under the influence of feedback from awide range of users. One feature which has turned out to beparticularly important is the development of the original HOWTO-style`tutorials' on \NLTK\ into a detailed, self-contained, online book(\url{http://nltk.org/book.html}). The book has been designed to stay in lock-stepwith the \NLTK\ code, and is intended to provide a major support ingetting students to learn \NLP\ by doing it. In consequence, webelieve \NLTK\ is unique in providing a comprehensive framework forstudents to develop a computational understanding of language.This paper describes the design of \NLTK, and reports on how it hasbeen used effectively in classes that involve a combination oflinguists and computer scientists.  First we discuss aspects of thedesign of the toolkit that arose from our need to teach computationallinguistics to a multidisciplinary audience.  The next sectionscover three distinct challenges:getting started with a course (\S\ref{sec:getting-started});fostering classroom interaction (\S\ref{sec:classroom-interaction});and organizing student projects (\S\ref{sec:student-projects}).\section{Design Decisions Affecting Teaching}\label{sec:design}The original conception of \NLTK\  was to develop data structures,algorithms and methods for \NLP\ in code that was simple and clearenough that the basic principles could beeasily grasped by students manipulating and reading the code. Althoughthis is still a central aspect of \NLTK, with the benefit of hindsight wecan see that it is rather biased towards a computer science audience.Computer science students come to \NLP\ with significantly differentexpectations than linguistics students.  The former expect to learnabout \NLP\ algorithms and data structures, and we have found thatthey benefit from being able to read and manipulate theimplementations provided with \NLTK.  The latter are interested inunderstanding the algorithms and data structures only insofar as it helps themto use computational tools to perform analytic tasks from `core linguistics',e.g., writing a set of CFG productions to parse some sentences, orplugging together \NLP\ components in order to derive the subcategorizationrequirements of verbs in a corpus.They are usually not interested in reading significant chunks of code;it isn't what they care about and theyprobably lack the self-confidence to poke around in source files.In a nutshell, the computer science students typically want to analyze thetools and synthesize new tools, while the linguists typically want to \emph{use}the tools to analyze language and synthesize new theories.Notes: gets back to definition of NLP and CL.  Linguists need to understandthat NLP is not just computer-based housekeeping, the next step up from usingoffice productivity software...\subsection{Python}We chose Python because it has a shallow learning curve, its syntaxand semantics are transparent, and it has good string-handlingfunctionality.  As an interpreted language, Python facilitatesinteractive exploration.  As an object-oriented language, Pythonpermits data and methods to be encapsulated and re-used easily.  Python comes with an extensivestandard library, including tools for graphical programming andnumerical processing.We have taken the step of incorporating a detailed introduction toPython programming in the \NLTK\ book, taking care to motivateprogramming constructs with linguistic examples. Extensive feedbackfrom students has been humbling, and revealed that for students withno prior programming experience, it is almost impossible toover-explain. Despite the difficulty of providing a completelyself-contained introduction to Python for linguists, we neverthelesshave also had very positive feedback, and in combination with theteaching techniques described below, have managed to bring alarge group of non-programmer students rapidly to a point where theycould carry out interesting and useful exercises in text processing.help function, access to \NLTK\ documentation on any module.Python itself fosters aninteractive style of teaching.  For instance, we've found it quitenatural to build up moderately complex programs in front of a class,with the weaker students transcribing it into a Python session ontheir laptop to satisfy themselves it works (but not necessarilyunderstanding everything they enter first time), while the strongerstudents quickly grasp the theoretical concepts and algorithms.  Whileboth groups can be served by the same presentation, they tend to askquite different questions.  However, this is addressed by dividingthem into smaller clusters and having TAs visit them separately todiscuss issues arising from the content.\subsection{Coding Requirements}Consistency, extensibility, simplicity, modularity.Non-requirements: comprehensiveness, efficiency, cleverness.\cite{LoperBird02}Code is readable -- a student who doesn't understand the maths of HMMs,smoothing, etc can get another angle on it via the code.\subsection{Naming}One issue which has absorbed a considerable amount of attention is thenaming of user-oriented functions in \NLTK. To a large extent, thesystem of naming \emph{is} the user interface to the toolkit, and it isimportant that users should be able to guess what action might beperformed by a given function. Consequently, naming conventions needto be consistent and semantically transparent. At the same time, there is acountervailing pressure for relatively succinct names, since excessive verbositycan also hinder comprehension and usability. An additionalcomplication is that adopting an object-oriented style of programmingmay be well-motivated for a number of reasons but neverthelessbaffling to the linguist student. For example, although it isperfectly respectable to invoke an instance method\texttt{WhitespaceTokenizer().tokenize(text)} (for some inputstring \texttt{text}), from a presentational point of view, \texttt{WhitespaceTokenizer(text)} works a lot better.\subsection{Corpus Access}Uniform corpus access.  After importing \NLTK, one can access all the corporausing the \texttt{nltk.corpus} module:{\small\begin{verbatim}>>> nltk.corpus.NAME.METHOD(PARAMETERS)\end{verbatim}}Here, \texttt{NAME} is any of the 45 corpora distributed with NLTK, includingparsed, POS-tagged, plain text, categorized text, and lexicons.\footnote{\url{http://nltk.org/corpora.html}}The \texttt{METHOD} can be any of\texttt{raw}, for the raw contents of the corpus;\texttt{words}, for a list of tokenized words;\texttt{sents}, for the same list grouped into sentences;\texttt{tagged\_words}, for a list of (word,tag) pairs;\texttt{tagged\_sents}, for the same list grouped into sentences;\texttt{parsed\_sents}, for a list of parse trees.The following example shows how to access the Brown Corpus:{\small\begin{verbatim}>>> nltk.corpus.brown.tagged_words()[('The', 'at'), ('Fulton', 'np-tl'),('County', 'nn-tl'), ('Grand', 'jj-tl'),('Jury', 'nn-tl'), ('said', 'vbd'), ...]\end{verbatim}}Note that not all methods are available for all corpora (e.g., we canask for the words from just about any corpus, but only for\texttt{parsed\_sents} from the parsed corpora).The \texttt{PARAMETERS} are typically used to restrict the amount of material returned,e.g. to a section of a corpus, or an individual corpus file.\subsection{Diversity of Programming Experience}Self-paced learning: tutorials, hundreds of graded exercises (self-evaluation)\NLTK\ supports assignments of varying difficulty and scope. In thesimplest assignments, students experiment with existing components toperform a wide variety of \NLP\ tasks. This may involve no programmingat all, in the case of the existing demonstrations, or simply changinga line or two of program code. As students become more familiar withthe toolkit they can be asked to modify existing components or tocreate complete systems out of existing components. \NLTK\ also providesstudents with a flexible framework for advanced projects, such asdeveloping a multi-component system, by integrating and extending \NLTK\components, and adding on entirely new components. Here \NLTK\ helps byproviding standard implementations of all the basic data structuresand algorithms, interfaces to standard corpora, substantial corpussamples, and a flexible and extensible architecture.\section{Getting Started}\label{sec:getting-started}\subsection{The First Lecture}Motivating and exemplifying NLP to a mixed audience.Some possible starting points:a) the holy grail: machines that understand language:- how this relates to linguists (writing programs to help us  understand the human language faculty)- how this relates to CS students: technologies that demonstrate some level of NLU:  SLDS, QA, Summarization, MTb) richness of language- extracting information from the web (economic incentive)- the recursive structure of natural language (prospects for applying  parsing techniques normally used for compilers to natural language)- studying large corpora  (no end to corpus formats, incompatible tools (linguists)  exploratory data analysis (find patterns not supported by existing software))\subsection{CDROM}ISO image -- give out CDs\subsection{First Assignment}Breaking the ice; force people to learn Python.\section{Classroom Interaction}\label{sec:classroom-interaction}\subsection{Demonstrations with the Python Interpreter}try it and seeNLTK book has many examples...An effective way to deliver the materials is through interactivepresentation of the examples, entering them at the Python prompt,observing what they do, and modifying them to explore some empiricalor theoretical question.\subsection{Interactive Demonstrations}A significant fraction of any \NLP\ syllabus covers fundamental datastructures and algorithms. These are usually taught with the help offormal notations and complex diagrams. Large trees and charts arecopied onto the board and edited in tedious slow motion, orlaboriously prepared for presentation slides. It is more effective touse live demonstrations in which those diagrams are generated andupdated automatically. \NLTK\ provides interactive graphical userinterfaces, making it possible to view program state and to studyprogram execution step-by-step. Most NLTK components have ademonstration mode, and will perform an interesting task withoutrequiring any special input from the user. It is even possible to makeminor modifications to programs in response to ``what if'' questions. Inthis way, students learn the mechanics of NLP quickly, gain deeperinsights into the data structures and algorithms, and acquire newproblem-solving skills.An example of a particularly effective set of demos are those forshift-reduce and recursive descent parsing. These make the differencebetween the algorithms glaringly obvious. More importantly, studentsget a very concrete sense of many issues that affect the design ofalgorithms for tasks like parsing. The partial analysis constructed bythe recursive descent parser bobs up and down as it steps forward andbacktracks, and students often go wide-eyed as the parser retraces itssteps and does ``dumb'' things like expanding N to {\it man} when ithas already tried the rule unsuccessfully (but is now trying to matcha bare NP rather than an NP with a PP modifier). Linguistics studentswho are extremely knowledgeable about context-free grammars and thusunderstand the representations gain a new appreciation for just hownaive an algorithm can be. This gives them a very concrete appreciatefor the need for techniques like dynamic programming and motivatesthem to learn how they work and can be used to solve such problemsmuch more efficiently.Another highly useful aspect of NLTK is the ability to define acontext-free grammar using a very simply format and display treestructures graphically. This can be used to teach context-freegrammars interactively, where the instructor and the students developa grammar from scratch and check its coverage against a testbed ofsentences (including grammatical and ungrammatical ones). Because itis so easy to modify the grammar and check its behavior, studentsreadily participate and suggest various solutions. When the grammarproduces an analysis for an ungrammatical sentence in the testbed, thetree structure can be displayed graphically and inspected to see whatwent wrong. Conversely, textual representations of the CKY parse chartcan be used to see where the grammar failed on grammatical sentences.NLTK's easy access to many corpora also greatly facilitates classroominstruction. It is straightforward to pull in different sections ofcorpora and build programs in class for many different tasks, fromsimple things like doing word counts to building rule-basedpart-of-speech taggers. This not only makes it easier to experimentwith ideas on the fly, but also allows students to replicate theexercises easily outside of class. Graphical displays that show thedispersion of terms throughout a text also give students excellentexamples of how a few simple statistics collected from a corpus canprovide useful and interesting views on a text---including seeing thefrequency with which various characters appear in a novel. This can inturn be related to other resources like Google Trends, which shows thefrequency with which a term has been referenced in news reports orbeen used in search terms over several years.\subsection{Assignments}Many exercises are provided with the NLTK documentation. Theseexercises have the tremendous advantage of building on the NLTKinfrastructure--both code and documentation. For students who arelearning to program as part of a computational linguistics course, theparallels between the examples in the documentation and therequirements of the assignments is very helpful. As a result, theywere able use NLTK to do far more complex tasks than they couldotherwise have hoped to do. The availability of online examples thatthey could try out in interactive Python were a huge help forthem. The exercises are also highly adaptable. It is common for instructorsto build homework assignments off of these as a base and add their owntwists to them. Some instructors prefer to include problems that do not allow studentsto take advantage of some built in NLTK functionality, e.g., countingword frequencies in the Brown corpus using the corpus source ratherthan NLTK access methods.  This is an important part of buildingfacility with working with general text processing with Python, sinceeventually students will have to play outside of the NLTK sandbox whenthey want to work with corpora outside of NLTK'sofferings. Nonetheless, students often use NLTK functionality as partof their solutions, such as managing frequencies anddistributions. Again, this flexibility is a good thing: students learnto work with resources they know how to use, and can branch out to newproblems with that basis. When course content includes discussion ofUnix command line utilities for text processing, students canfurthermore have a better appreciation of the pros and cons of writingtheir own scripts versus using an appropriate Unix pipe.\subsection{Small Group Discussion}animate this with a quiz, presented as a slide or a handout, giving code samples and asking what they do.\subsection{Chatroom}useful during intensive summer program; otherwise couldn't be staffed adequately\section{Student Projects}\label{sec:student-projects}Group projects involving a mixture of linguists and CS students:initial appeal is that a CS student will help the linguist student with programming,and vice versa.  However, there's acomplex dynamic, unpredictable success, linguist probably won't get to programin the interests of a good project mark.Multi-stage project mandating stages that require linguistic and CS content: difficultto foster continuous collaboration, more likely to get e.g. parser being developed bya CS team member, then thrown over the wall to a linguist member to develop a grammar.Instead, believe it is more productive in the context of a single-semester introductorycourse to have students work on their own projects.  Devise distinct projects forstudents depending on background.  Provide a list and give them the option of proposingother projects.\url{http://nltk.org/projects.html}Peer review (including code review) to improve quality of programming, andemphasize the communicative dimension of programming.(Even grade a student on the quality of his/her peer review of another student.)\bibliographystyle{acl}\bibliography{acl-08}\end{document}