% $Rev%
\documentclass[11pt]{article}
\usepackage{acl08}
\usepackage{times}
\usepackage{latexsym}
\usepackage{epsfig,url}

\newcommand{\NLP}{\textsc{nlp}}
\newcommand{\NLTK}{\textsc{nltk}}
\newcommand{\code}[1]{\texttt{\small #1}}

\pretolerance 250
\tolerance 500
\hyphenpenalty 200
\exhyphenpenalty 100
\doublehyphendemerits 7500
\finalhyphendemerits 7500
\brokenpenalty 10000
\lefthyphenmin 3
\righthyphenmin 3
\widowpenalty 10000
\clubpenalty 10000
\displaywidowpenalty 10000
\looseness 1

\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{Multidisciplinary Instruction with the Natural Language Toolkit}
\author{}

% \author{Joakim Nivre \\
%   School of Mathematics and Systems Engineering \\
%   V\"{a}xj\"{o} University \\
%   SE-35195, V\"{a}xj\"{o}, Sweden \\
%   {\tt nivre@msi.vxu.se} \And
%   Noah A. Smith \\
%   Language Technologies Institute \\
%   Carnegie Mellon University \\
%   Pittsburgh, PA 15213, USA\\
%   {\tt nasmith@cs.cmu.edu}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  The Natural Language Toolkit (\NLTK) is widely used for teaching
  natural language processing to students majoring in linguists or
  computer science.  This paper describes the design of \NLTK, and
  reports on how it has been used effectively in classes that involve
  different mixes of
  linguistics and computer science students.  We focus
  on three key issues: getting started with a course, fostering
  classroom interaction, and organizing project work.
  In each case, we report on practical experience and make
  recommendations on how to use \NLTK\ to maximum effect.
\end{abstract}

\section{Introduction}

It is relatively easy to teach natural language processing (\NLP) in a
single-disciplinary mode to a uniform cohort of students.  Linguists
can be taught to program, leading to projects where students
manipulate their own linguistic data.  Computer scientists can be
taught methods for automatic text processing, leading to projects on
text mining and chatbots.  Yet these approaches have almost nothing in
common, and it is a stretch to call either of these \NLP: more apt
titles for such courses might be `linguistic data management' and
`text technologies'.

To the extent that \NLP\ is a coherent discipline, it has core knowledge
and skills that must be learned by all students.
The Natural Language Toolkit (\NLTK) was developed to address
these issues, making it feasible to cover a substantial amount of
theory and practice with an audience consisting
of both linguists and computer scientists.
\NLTK\ is a suite of Python modules
distributed under the GPL open source license.\footnote{\url{http://nltk.org}}
\NLTK\ comes with a large collection of corpora, extensive
documentation, and hundreds of exercises, making
\NLTK\ unique in providing a comprehensive framework for
students to develop a computational understanding of language.
\NLTK's code base of 100,000 lines of Python code includes
support for corpus access, tokenizing, stemming, tagging, chunking,
parsing, clustering, classification, language modelling, semantic
interpretation, unification, and much else besides.\footnote{\url{http://nltk.org/code.html}}
As a measure of its impact,
\NLTK\ has been used in over 50 university courses in more than 15
countries.\footnote{\url{http://nltk.org/courses.html}}

Since its inception in 2001, \NLTK\ has undergone considerable
evolution, based on the experience gained by teaching courses at
several universities, and based on feedback from many teachers and
students.  Over this period, a series of practical online tutorials
about \NLTK\ has grown up into a comprehensive online book.\footnote{\url{http://nltk.org/book.html}}
The book has been designed to stay in lock-step
with the \NLTK\ library, and is intended to provide a major support in
getting students to learn \NLP\ by doing it.

This paper describes the design of \NLTK, and reports on how it has
been used effectively in classes that involve a combination of
linguists and computer scientists.  First we discuss aspects of the
design of the toolkit that arose from our need to teach computational
linguistics to a multidisciplinary audience.  The next sections
cover three distinct challenges:
getting started with a course (\S\ref{sec:getting-started});
fostering classroom interaction (\S\ref{sec:classroom-interaction});
and organizing assignments and student projects (\S\ref{sec:projects}).

\section{Design Decisions Affecting Teaching}
\label{sec:design}

\subsection{Python}

We chose Python because it has a shallow learning curve, its syntax
and semantics are transparent, and it has good string-handling
functionality.  As an interpreted language, Python facilitates
interactive exploration.  As an object-oriented language, Python
permits data and methods to be encapsulated and re-used easily.  Python comes with an extensive
standard library, including tools for graphical programming and
numerical processing, which means it can be used for a wide range
of non-trivial applications.
Python is ideal in a context
serving newcomers and experienced programmers \cite{Shannon03}.

We have taken the step of incorporating a detailed introduction to
Python programming in the \NLTK\ book, taking care to motivate
programming constructs with linguistic examples. Extensive feedback
from students has been humbling, and revealed that for students with
no prior programming experience, it is almost impossible to
over-explain. Despite the difficulty of providing a
self-contained introduction to Python for linguists, we nevertheless
have also had very positive feedback, and in combination with the
teaching techniques described below, have managed to bring a
large group of non-programmer students rapidly to a point where they
could carry out interesting and useful exercises in text processing.

In addition to the \NLTK\ book, the code in the \NLTK\ core is richly
documented, using Python docstrings and
\textsc{epydoc}\footnote{\url{http://epydoc.sourceforge.net/}} support
for API documentation.\footnote{\url{http://nltk.org/doc/api/}} Access
to the code documentation is available using the Python \code{help()}
command at the interactive prompt, and this can be especially useful
for checking the input parameters and return type of functions.

%help function, access to \NLTK\ documentation on any module.

key libraries: numpy, pylab

Python itself fosters an
interactive style of teaching.  For instance, we've found it quite
natural to build up moderately complex programs in front of a class,
with the weaker students transcribing it into a Python session on
their laptop to satisfy themselves it works (but not necessarily
understanding everything they enter first time), while the stronger
students quickly grasp the theoretical concepts and algorithms.  While
both groups can be served by the same presentation, they tend to ask
quite different questions.  However, this is addressed by dividing
them into smaller clusters and having TAs visit them separately to
discuss issues arising from the content.

Diversity of Programming Experience:
Self-paced learning: tutorials, hundreds of graded exercises (self-evaluation)



\subsection{Coding Requirements}

As discussed in Loper \& Bird~\shortcite{LoperBird02}, the priorities for \NLTK\ code
focus on its teaching role. When code is readable, a student who
doesn't understand the maths of HMMs, smoothing, and so on may benefit
from looking at how an algorithm is implemented. Thus consistency,
simplicity, modularity are all vital features of \NLTK\ code. A
similar importance is place on extensibility, since this helps to
ensure that the code grows as a coherent whole, rather than by
unpredictable and haphazard additions.  

By contrast,
comprehensiveness of coverage has never been an overriding concern of
\NLTK---indeed, some students seem to positively prefer the restricted
scope of \NLTK, since they find it less overwhelming.
Similarly, although efficiency cannot be completely ignored, it has
always taken second place to simplicity and clarity of coding. In a
similar vein, we have tried to eschew `clever' programming tricks,
since these typically hinder intelligiblity of the code.






\subsection{Naming}

One issue which has absorbed a considerable amount of attention is the
naming of user-oriented functions in \NLTK. To a large extent, the
system of naming \emph{is} the user interface to the toolkit, and it is
important that users should be able to guess what action might be
performed by a given function. Consequently, naming conventions need
to be consistent and semantically transparent. At the same time, there is a
countervailing pressure for relatively succinct names, since excessive verbosity
can also hinder comprehension and usability. An additional
complication is that adopting an object-oriented style of programming
may be well-motivated for a number of reasons but nevertheless
baffling to the linguist student. For example, although it is
perfectly respectable to invoke an instance method
\code{WordPunctTokenizer().tokenize(text)} (for some input
string \code{text}), from a presentational point of view, 
\code{wordpunct\_tokenize(text)} works a lot better
(both are available).

\subsection{Corpus Access}

Uniform corpus access.  After importing \NLTK, one can access all the corpora
using the \code{nltk.corpus} module:

{\small
\begin{verbatim}
>>> nltk.corpus.NAME.METHOD(PARAMETERS)
\end{verbatim}}

Here, \code{NAME} is any of the 45 corpora distributed with NLTK, including
parsed, POS-tagged, plain text, categorized text, and lexicons.\footnote{\url{http://nltk.org/corpora.html}}
The \code{METHOD} can be:
\code{raw}, for the raw contents of the corpus;
\code{words}, for a list of tokenized words;
\code{sents}, for the same list grouped into sentences;
\code{tagged\_words}, for a list of (word,tag) pairs;
\code{tagged\_sents}, for the same list grouped into sentences;
\code{parsed\_sents}, for a list of parse trees.
The following example shows how to access the Brown Corpus:

{\small
\begin{verbatim}
>>> nltk.corpus.brown.tagged_words()
[('The', 'AT'), ('Fulton', 'NP-TL'),
('County', 'NN-TL'), ('Grand', 'JJ-TL'),
('Jury', 'NN-TL'), ('said', 'VBD'), ...]
\end{verbatim}}

Note that not all methods are available for all corpora (e.g., we can
ask for the words from just about any corpus, but only for
\code{parsed\_sents} from the parsed corpora).

The \code{PARAMETERS} are typically used to restrict the amount of material returned,
e.g. to a section of a corpus, or an individual corpus file.

\subsection{Accessing Shoebox Files}

\NLTK\ provides functionality for working with Shoebox data
\cite{robinson:etal:2007}. Shoebox is a system used by many
documentary linguists to produce lexicons and interlinear glossed text.  The
ability to work straightforwardly with Shoebox data promises to open
up a new avenue for encouraging linguists to learn about the promise
of computational methods. 

As an example, in the Linguistics Department at the University of
XXX, a course has been offered on Python programming and
working with corpora, but so far uptake from the
target audience of core linguistics students has been low. They usually have very
practical needs and concerns when it comes to computational
linguistics, and many of them are also intimidated by the very idea of
programming. % So, there is a high bar for them to get over to sign up
% for any computational linguistics course. 

We believe that the appeal of this course can be significantly
enhanced by designing a significant component
with the goal of helping documentary linguistics students take control of their
\emph{own} Shoebox data. This will give them  skills that are
useful for their research and also transferable to other activities.
Although the \NLTK\ Shoebox functionality was not
originally designed with instruction in mind, its relevance to
students of documentary linguistics is highly fortuitous and
may be relevant at other similar linguistics departments.

\section{Getting Started}
\label{sec:getting-started}

\NLP\ is usually only available as an elective course, and students
will vote with their feet after attending one or two classes.  This
initial period is important for attracting and retaining students.  In
particular, students need to get a sense of the richness of language
in general, and \NLP\ in particular, while gaining a realistic
impression of what will be accomplished during the course and what
skills they will have by the end.  During this time when rapport needs
to be rapidly established, it is easy for instructors to alienate
students through the use of linguistic or computational concepts and
terminology that are foreign to students, or to bore students by
getting bogged down in defining terms like `noun phrase' or `function'
which are basic to one audience and new for the other.  Thus, we
believe it is crucial for instructors to understand and shape the
student's expectations, and to get off to a good start.  The best
overall strategy that we have found is to use succinct `nuggets' of
\NLTK\ code to stimulate students' interest in both data and
processing techniques.

\subsection{Student Expectations}

Computer science students come to \NLP\ expecting to learn about \NLP\
algorithms and data structures.  They typically have enough
mathematical preparation to be confident in `playing' with abstract
formal systems (including systems of linguistic rules).  Moreover,
they are already proficient in multiple programming languages, and
have little difficulty in learning \NLP\ algorithms by reading and
manipulating the implementations provided with \NLTK. At the same
time, they typically have little grasp of the terminology or concepts
that linguists take for granted, and may struggle to come up with
`reasonable' linguistic analyses of data.

Linguistics students, on the other hand, are interested in
understanding \NLP\ algorithms and data structures only insofar as it helps them
to use computational tools to perform analytic tasks from `core linguistics',
e.g.\ writing a set of CFG productions to parse some sentences, or
plugging together \NLP\ components in order to derive the subcategorization
requirements of verbs in a corpus.
They are usually not interested in reading significant chunks of code;
it isn't what they care about and they
probably lack the self-confidence to poke around in source files.

In a nutshell, the computer science students typically want to analyze
the tools and synthesize new implementations, while the linguists
typically want to use the tools to analyze language and
synthesize new theories.  There is a risk that the former group
never really gets to grips with natural language, while the latter
group never really gets to trips with processing.  Instead,
computer science students need to learn that \NLP\ is not just an
application of techniques from formal language theory and compiler
construction, and linguistics students need to understand that \NLP\ is not
just computer-based housekeeping and a solution to the shortcomings of
office productivity software for managing their linguistic data.

In many courses, linguistics students or computer science students
will dominate the class numerically, simply because the course is only
listed in one department.  In such cases it is usually enough to
provide additional support in the form of some extra readings,
tutorials, and exercises in the opening stages of the course.  In
other cases, e.g.\ courses we have taught at the universities of YYY
and ZZZ, or in summer intensive programs in several countries, there
is more of an even split, and the challenge of serving both cohorts of
students becomes acute.

\subsection{Articulating the Goals}

Despite an instructor's efforts to add a cross-disciplinary angle, students
easily `revert to type'.  The pressure of assessment encourages students to emphasize
what they do well.  Students' desire to understand what is expected of them encourages
instructors to stick to familiar assessment instruments.  As a consequence,
the path of least resistance is for students to remain firmly
`monolingual' in their own discipline, while
acquiring a smattering of words from a foreign language, at a level we might
call `survival linguistics' or `survival computer science'.

Asking computer science students to write their first essay in years, or asking
linguistics students to write their first ever program, leads to
stressed students who complain that they don't know what is expected
of them.

Students need to confront the challenge of becoming bilingual, of
working hard to learn the basics of another discipline.  In parallel,
instructors need to confront the challenge of synthesizing material
from linguistics and computer science into a coherent whole, and
devising effective methods for teaching, learning, and assessment.


\subsection{The First Lecture}

Should be easy given that language and language technologies are interesting
to a wide audience.

Motivating and exemplifying \NLP\ to a mixed audience.
Some possible starting points:

a) the holy grail: machines that understand language:

- how this relates to linguists (writing programs to help us
  understand the human language faculty)

- how this relates to CS students: technologies that demonstrate some level of NLU:
  SLDS, QA, Summarization, MT

b) richness of language

- extracting information from the web (economic incentive)

- the recursive structure of natural language (prospects for applying
  parsing techniques normally used for compilers to natural language)

- studying large corpora
  (no end to corpus formats, incompatible tools (linguists)
  exploratory data analysis (find patterns not supported by existing software))

c) shared history of CS and linguistics: philosophical logic, formal language theory;
overlaps with a different purpose:
manipulating text (studying corpora vs text processing),
formal semantics (natural vs artificial languages)

\subsection{First Assignment}

The first coursework assignment can be a significant step forwards in
helping students get to grips with the material, and is best given out
early, perhaps even in week 1.
% Breaking the ice; force people to learn Python.  Give out early (first week?)
We have found it advisable for this assignment to
include both programming and linguistics content. One example is to
ask students to carry out NP chunking of some data (e.g.\ a section of
the Brown Corpus). The \code{nltk.RegexpParser} class is initialized
with set of chunking rules using a simple, regular expression-oriented
syntax, and the resulting chunk parser can be run over tagged input
text. Given a Gold Standard test set like the CoNLL-2000 data,
precision and recall of the chunk grammar can be easily determined.
Thus, if students are given an existing, incomplete set of rules as
their starting point, they just have to modify and test their rules. 

There are distinctive outcomes for each set of students: linguistics students
learn to write grammar fragments that respect the literal-minded
needs of the computer, and also come to appreciate the noisiness of
typical \NLP\ corpora (including automatically annotated corpora like
CoNLL-2000).
Computer science students become more familiar with parts of speech
and with typical syntactic structures in English. Both groups learn
the importance of formal evaluation using precision and recall.

% NP Chunking using template code from book: students supplied with a
% complete program and just have to modify the chunking rules.

% Linguistics students learn: care in editing code, computer only does what it is told,
% skills with developing a grammar fragment extend to debugging code that mainly consists
% of a grammar fragment,
% corpus data is somewhat arbitrary and their own linguistic intuitions aren't always
% borne out.

% CS students learn: definition of parts of speech, definition of noun phrase, notion that
% language is highly structured

% Both learn: formal evaluation using precision and recall


\section{Classroom Interaction}
\label{sec:classroom-interaction}

\subsection{Demonstrations with the Python Interpreter}

try it and see

NLTK book has many examples...

An effective way to deliver the materials is through interactive
presentation of the examples, entering them at the Python prompt,
observing what they do, and modifying them to explore some empirical
or theoretical question.

But NB it may be necessary to keep  and distribute logs of the sessions, since some students have
difficulty keeping up, particularly at the outset.
(Also mention the challenges of keeping the font size big enough?)

\subsection{Interactive Demonstrations}

A significant fraction of any \NLP\ syllabus covers fundamental data
structures and algorithms. These are usually taught with the help of
formal notations and complex diagrams. Large trees and charts are
copied onto the board and edited in tedious slow motion, or
laboriously prepared for presentation slides. It is more effective to
use live demonstrations in which those diagrams are generated and
updated automatically. \NLTK\ provides interactive graphical user
interfaces, making it possible to view program state and to study
program execution step-by-step. Most NLTK components have a
demonstration mode, and will perform an interesting task without
requiring any special input from the user. It is even possible to make
minor modifications to programs in response to ``what if'' questions. In
this way, students learn the mechanics of \NLP\ quickly, gain deeper
insights into the data structures and algorithms, and acquire new
problem-solving skills.

An example of a particularly effective set of demos are those for
shift-reduce and recursive descent parsing. These make the difference
between the algorithms glaringly obvious. More importantly, students
get a very concrete sense of many issues that affect the design of
algorithms for tasks like parsing. The partial analysis constructed by
the recursive descent parser bobs up and down as it steps forward and
backtracks, and students often go wide-eyed as the parser retraces its
steps and does ``dumb'' things like expanding N to {\it man} when it
has already tried the rule unsuccessfully (but is now trying to match
a bare NP rather than an NP with a PP modifier). Linguistics students
who are extremely knowledgeable about context-free grammars and thus
understand the representations gain a new appreciation for just how
naive an algorithm can be. This gives them a very concrete appreciate
for the need for techniques like dynamic programming and motivates
them to learn how they work and can be used to solve such problems
much more efficiently.

Another highly useful aspect of \NLTK\ is the ability to define a
context-free grammar using a very simply format and display tree
structures graphically. This can be used to teach context-free
grammars interactively, where the instructor and the students develop
a grammar from scratch and check its coverage against a testbed of
sentences (including grammatical and ungrammatical ones). Because it
is so easy to modify the grammar and check its behavior, students
readily participate and suggest various solutions. When the grammar
produces an analysis for an ungrammatical sentence in the testbed, the
tree structure can be displayed graphically and inspected to see what
went wrong. Conversely, textual representations of the CKY parse chart
can be used to see where the grammar failed on grammatical sentences.

\NLTK's easy access to many corpora also greatly facilitates classroom
instruction. It is straightforward to pull in different sections of
corpora and build programs in class for many different tasks, from
simple things like doing word counts to building rule-based
part-of-speech taggers. This not only makes it easier to experiment
with ideas on the fly, but also allows students to replicate the
exercises easily outside of class. Graphical displays that show the
dispersion of terms throughout a text also give students excellent
examples of how a few simple statistics collected from a corpus can
provide useful and interesting views on a text---including seeing the
frequency with which various characters appear in a novel. This can in
turn be related to other resources like Google Trends, which shows the
frequency with which a term has been referenced in news reports or
been used in search terms over several years.


\subsection{Small Group Discussion}

animate this with a quiz, presented as a slide or a handout, giving code samples and asking what they do.

class exercise, e.g. counting uses of ``must'' in a spoken vs written language corpus
(deontic vs epistemic uses).

\subsection{Chatroom}

useful during intensive summer program; otherwise couldn't be staffed adequately

\section{Exercises, Assignments and Projects}
\label{sec:projects}

Many exercises are provided with the \NLTK\ documentation. These
exercises have the tremendous advantage of building on the \NLTK\
infrastructure--both code and documentation. For students who are
learning to program as part of a computational linguistics course, the
parallels between the examples in the documentation and the
requirements of the assignments is very helpful. As a result, they
were able use \NLTK\ to do far more complex tasks than they could
otherwise have hoped to do. The availability of online examples that
they could try out in interactive Python were a huge help for them.


\begin{figure*}
{\small
\begin{verbatim}
>>> nltk.FreqDist(nltk.corpus.brown.words())
\end{verbatim}}

{\small
\begin{verbatim}
>>> fd = nltk.FreqDist()
>>> for file in ???:
...     for word in nltk.tokenize(...):
...         fd.inc(word)
\end{verbatim}}
\medskip

{\small
\begin{verbatim}
>>> counts = {}
>>> for word in nltk.corpus.brown.words():
...     if word not in counts:
...         counts[word] = 0
...     counts += 1
\end{verbatim}}
\caption{Different ways to build up a frequency distribution of words in the Brown Corpus}
\label{fig:freqdist}
\end{figure*}


The exercises are also highly adaptable. It is common for instructors
to take them as a starting point in building homework assignments that
are tailored to their own students.  Some instructors prefer to
include exercises that do not allow students to take advantage of
built-in \NLTK\ functionality, e.g., using a Python dictionary to
count word frequencies in the Brown corpus rather than \NLTK 's
\code{FreqDist} (see Figure~\ref{fig:freqdist}).  This is an important
part of building facility with general text processing in Python,
since eventually students will have to play outside of the \NLTK\
sandbox, particularly when they start looking at larger
corpora. Nonetheless, students often use \NLTK\ functionality as part
of their solutions, e.g., for managing frequencies and
distributions. Again, this flexibility is a good thing: students learn
to work with resources they know how to use, and can branch out to new
exercises from that basis. When course content includes discussion of
Unix command line utilities for text processing, students can
furthermore gain a better appreciation of the pros and cons of writing
their own scripts versus using an appropriate Unix pipe.


\NLTK\ supports assignments of varying difficulty and scope. In the
simplest assignments, students experiment with existing components to
perform a wide variety of \NLP\ tasks. This may involve no programming
at all, in the case of the existing demonstrations, or simply changing
a line or two of program code. As students become more familiar with
the toolkit they can be asked to modify existing components or to
create complete systems out of existing components. \NLTK\ also provides
students with a flexible framework for advanced projects, such as
developing a multi-component system by integrating and extending \NLTK\
components, and adding on entirely new components. Here \NLTK\ helps by
providing standard implementations of all the basic data structures
and algorithms, interfaces to standard corpora, substantial corpus
samples, and a flexible and extensible architecture.


Group projects involving a mixture of linguists and CS students:
initial appeal is that a CS student will help the linguist student with programming,
and vice versa.  However, there's a
complex dynamic, unpredictable success, linguist probably won't get to program
in the interests of a good project mark.
Multi-stage project mandating stages that require linguistic and CS content: difficult
to foster continuous collaboration, more likely to get e.g. parser being developed by
a CS team member, then thrown over the wall to a linguist member to develop a grammar.

Instead, believe it is more productive in the context of a single-semester introductory
course to have students work on their own projects.  Devise distinct projects for
students depending on background.  Provide a list and give them the option of proposing
other projects.\footnote{\url{http://nltk.org/projects.html}}

Peer review (including code review) to improve quality of programming, and
emphasize the communicative dimension of programming.
(Even grade a student on the quality of his/her peer review of another student.)

\section{Conclusion}


\bibliographystyle{acl}
\bibliography{acl-08}

\end{document}
