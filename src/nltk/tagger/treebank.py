# Natural Language Toolkit: Treebank Tagged Tokenizer
#
# Copyright (C) 2004 University of Melbourne
# Author: Trevor Cohn <tacohn@cs.mu.oz.au>
# URL: <http://nltk.sf.net>
# For license information, see LICENSE.TXT
#
# $Id$

"""
Single tokenizer class for loading part-of-speech tagged data from the Penn
Treebank. This extends the L{TaggedTokenizer} to support the additional markup
present in these files.

@group Tokenizers: TreebankTaggedTokenizer
"""

from nltk.tagger import TaggedTokenReader
from nltk.chktype import chktype
from nltk.token import Token, SentIndexLocation, ParaIndexLocation
from nltk.token import TokenReaderI, SubtokenContextPointer
from nltk import PropertyIndirectionMixIn
from nltk.parser.chunk import ChunkedTaggedTokenReader
import re

class TreebankTaggedTokenReader(TokenReaderI, PropertyIndirectionMixIn):
    # [XX] This comment is out-of-date!!
    """

    A token reader that reas the treebank tagged-file format into a
    token.  In this format:

      - Paragraphs are separated by lines of C{'='} characters.
      - Sentences are separated by words tagged as sentence-final
        punctuation (e.g., C{'./.'}).
      - NP chunk structure is encoded with square brackets (C{[...]}).
      - Words are separated by whitespace or square brackets.
      - Each word has the form C{I{text}/i{tag}}, where C{I{text}}
        is the word's text, and C{I{tag}} is its tag.

    In the returned token:
    
      - The returned token describes a single document.
      - The document's C{PARAS} property contains a list of paragraph
        tokens.
        - Each paragraph token's C{SENTS} property contains a list of
          sentence tokens.
          - Each sentence token's C{WORDS} property contains a list of
            word tokens.
            - Each word token's C{TEXT} property contains the word's
              text.
            - Each word token's C{TAG} property contains the word's
              tag.
            - Depending on the arguments to the reader's constructor,
              each word token may also define the C{LOC} and
              C{CONTEXT} properties.
          - Each sentence token's C{TREE} property contains the
            chunk structures in the text.  In the case of the Treebank,
            these chunk structures were generated by a stochastic NP
            chunker as part of the PARTS preprocessor, and \"are best
            ignored.\"
    """
    def __init__(self, locs=True, contexts=True, **property_names):
        PropertyIndirectionMixIn.__init__(self, **property_names)
        self._locs = locs
        self._contexts = contexts

        # A token reader for processing sentences.
        self._sent_reader = ChunkedTaggedTokenReader(
            locs, contexts, top_node='S', chunk_node='NP_CHUNK',
            **property_names)

    def read_token(self, s, source=None):
        assert chktype(1, s, str)

        TEXT = self.property('TEXT')
        LOC = self.property('LOC')
        CONTEXT = self.property('CONTEXT')
        SENTS = self.property('SENTS')
        PARAS = self.property('PARAS')
        TREE = self.property('TREE')

        # Split into paragraphs.
        paragraphs = re.split(r'(?m)^={10,}$', s)
        # Strip whitespace, and discard empty paras.
        paragraphs = [para.strip() for para in paragraphs]
        paragraphs = [para for para in paragraphs if para]

        # Process each paragraph.
        para_toks = []
        for para_num, para_text in enumerate(paragraphs):
            # Create a location for this paragraph
            para_loc = ParaIndexLocation(para_num, source)
            # Split the paragraph into sentences.
            sentences = re.findall('(?s)\S.*?/\.', para_text)
            sent_toks = []
            for sent_num, sentence in enumerate(sentences):
                sent_loc = SentIndexLocation(sent_num, para_loc)
                sent_tok = self._sent_reader.read_token(sentence, sent_loc)
                sent_toks.append(sent_tok)
                print sent_tok
                print
            para_toks.append(Token(**{SENTS: sent_toks}))
            if self._locs:
                para_toks[-1][LOC] = para_loc

        # Create a token for the document
        tok = Token(**{PARAS: para_toks})

        # Add context pointers, if requested
        if self._contexts:
            for para_num, para_tok in enumerate(tok[PARAS]):
                para_tok[CONTEXT] = SubtokenContextPointer(tok, PARAS,
                                                           para_num)
                for sent_num, sent_tok in enumerate(para_tok[SENTS]):
                    sent_tok[CONTEXT] = SubtokenContextPointer(tok, SENTS,
                                                               sent_num)

        # Return the finished token.
        return tok
            
    def read_tokens(self, s, source=None):
        return [self.read_token(s, source)]
