# Natural Language Toolkit: Hidden Markov Model
#
# Copyright (C) 2003 University of Melbourne
# Author: Trevor Cohn <tacohn@cs.mu.oz.au>
# URL: <http://nltk.sf.net>
# For license information, see LICENSE.TXT
#
# $Id$

"""
Hidden Markov Models (HMMs) largely used to assign the correct label sequence
to sequential data or assess the probability of a given label and data
sequence. These models are finite state machines characterised by a number
of states, transitions between these states, and output symbols emitted
while in each state. The HMM is an extension to the Markov chain, where each
state corresponds deterministically to a given event. In the HMM the
observation is a probabilistic function of the state. HMMs share the Markov
chain's assumption, being that the probability of transition from one state to
another only depends on the current state - i.e. the series of states that
led to the current state are not used. They are also time invariant.

The HMM is a directed graph, with probability weighted edges (representing the
probability of a transition between the source and sink states) where each
vertex emits an output symbol when entered. The symbol (or observation) is
non-deterministically generated. For this reason, knowing that a sequence of
output observations was generated by a given HMM does not mean that the
corresponding sequence of states (and what the current state is) is known. This
is the 'hidden' in the hidden markov model.

Formally, a HMM can be characterised by:
    - the output observation alphabet. This is the set of symbols which may be
      observed as output of the system. 
    - the set of states. 
    - the transition probabilities M{a_{ij} = P(s_t = j | s_{t-1} = i)}. These
      represent the probability of transition to each state from a given
      state. 
    - the output probability matrix M{b_i(k) = P(X_t = o_k | s_t = i). These
      represent the probability of observing each symbol in a given state.
    - the initial state distribution. This gives the probability of starting
      in each state.

To ground this discussion, take a common NLP application, part-of-speech
(POS) tagging. An HMM is desirable for this task as the highest probability
tag sequence can be calculated for a given sequence of word forms. This differs
from other tagging techniques which often tag each word individually, seeking
to optimise each individual tagging greedily without regard to the optimal
combination of tags for a larger unit, such as a sentence. The HMM does this
with the Viterbi algorithm, which efficiently computing the optimal path
through the graph given the sequence of words forms.

In POS tagging the states usually have a 1:1 correspondence with the tag
alphabet - i.e. each state represents a single tag. The output observation
alphabet is the set of word forms (the lexicon), and the remaining three
parameters are derived by a training regime. With this information the
probability of a given sentence can be easily derived, by simply summing
the probability of each distinct path through the model. Similarly, the
highest probability tagging sequence can be derived with the Viterbi
algorithm, yielding a state sequence which can be mapped into a tag sequence.

This discussion assumes that the HMM has been trained. This is probably the
most difficult task with the model, and requires either MLE estimates of the
parameters or unsupervised learning using the Baum-Welch algorithm, a variant
of EM.
"""

#from nltk.probability import *
from Numeric import *

class HMM:
    """
    Based on HMM description in Chapter 8, Huang, Acero and Hon, Spoken
    Language Processing.
    """
    def __init__(self, states, transitions, symbols, outputs, priors):
        """
        Creates a hidden markov model parametised by the the states,
        transition probabilities, output probabilities and priors.

        @param  states:         a set of states representing state space
        @type   states:         seq of any
        @param  transitions:    transition probability matrix; entry (i, j)
                                is the probability of transition from state i
                                to state j for all i, j in C{range(len(states))}
        @type   transitions:    C{array}
        @param  symbols:        the set of output symbols (alphabet)
        @type   symbols:        (seq) of any
        @param  outputs:        output probability matrix; entry (i, k) is the
                                probability of emitting symbol k when entering
                                state i for all i in C{range(len(states))} and
                                k in C{range(len(symbols))}
        @type   outputs:        C{array}
        @param  priors:         initial state distribution; entry i is the
                                probability of starting in state i
        @type   priors:         C{array}
        """
        self._states = states
        self._transitions = transitions
        self._symbols = symbols
        self._outputs = outputs
        self._priors = priors
        self._symbol_map = {}
        N = 0
        for symbol in symbols:
            if not self._symbol_map.has_key(symbol):
                self._symbol_map[symbol] = N
                N += 1

        # TODO - allow for some of these parameters to be None, instead
        # assuming uniform probabilities

    def probability(self, symbols):
        """
        Returns the probability of the given symbol sequence. Uses the forward
        algorithm.
        """
        T = len(symbols)
        N = len(self._states)
        alpha = zeros((T, N), Float64)

        for t in range(T):
            symbol = symbols[t]
            b = self._symbol_map[symbol] # what if unseen?
            if t == 0:
                for i in range(N):
                    alpha[t, i] = self._priors[i] * self._outputs[i, b]
            else:
                for i in range(N):
                    for j in range(N):
                        alpha[t, i] += alpha[t-1, j] * self._transitions[i, j]
                    alpha[t, i] *= self._outputs[i, b]

        p = 0
        for i in range(N):
            p += alpha[T-1, i]

        return p

    def best_path(self, symbols):
        """
        Returns the state sequence of the optimal (most probable) path through
        the HMM. Uses the Viterbi algorithm.
        """
        T = len(symbols)
        N = len(self._states)
        V = zeros((T, N), Float64)
        B = zeros((T, N), Int)

        for t in range(T):
            symbol = symbols[t]
            b = self._symbol_map[symbol] # what if unseen?
            if t == 0:
                for i in range(N):
                    V[t, i] = self._priors[i] * self._outputs[i, b]
                    B[t, i] = -1
            else:
                for j in range(N):
                    best = None
                    for i in range(N):
                        va = V[t-1, i] * self._transitions[i, j]
                        if not best or va > best[0]:
                            best = (va, i)
                    V[t, j] = best[0] * self._outputs[j, b]
                    B[t, j] = best[1]

        #print 'V', V
        #print 'B', B

        best = None
        for i in range(N):
            val = V[T-1, i]
            if not best or val > best[0]:
                best = (val, i)

        #print 'best', best

        current = best[1]
        sequence = [current]
        for t in range(T-1, 0, -1):
            last = B[t, current]
            sequence.append(last)
            current = last

        sequence.reverse()
        #print 'sequence', sequence
        states = map(lambda s, mapping=self._states: mapping[s], sequence)
        return states

    def __repr__(self):
        return '<HMM %d states and %d output symbols>' \
                % (len(self._states), len(self._symbols))

class HMMTrainer:
    def __init__(self, states, symbols):
        """
        Creates an HMM trainer to induce an HMM with the given states and
        output symbol alphabet. A supervised and unsupervised training
        method may be used.
        """
        self._states = states
        self._symbols = symbols
        self._symbol_map = {}
        N = 0
        for symbol in symbols:
            if not self._symbol_map.has_key(symbol):
                self._symbol_map[symbol] = N
                N += 1
        self._state_map = {}
        N = 0
        for state in states:
            if not self._state_map.has_key(state):
                self._state_map[state] = N
                N += 1

    def train_baum_welch(self, symbol_sequence):
        """
        Trains the HMM using the Baum-Welch algorithm to maximise the
        probability of the data sequence. This is a variant of the EM
        algorithm, and is unsupervised in that it doesn't need the state
        sequences for the symbols.
        """

        N = len(self._states)
        M = len(self._symbols)
        T = len(symbol_sequence)
        backward = zeros((T, N), Float64)
        forward = zeros((T, N), Float64)

        # create a (uniform) HMM, which will be iteratively refined
        A = ones((N, N), Float64) / float(N)
        B = ones((N, M), Float64) / float(M)
        pi = ones(N, Float64) / float(N)
        gamma = zeros((T, N, N), Float64)

        while True: # needs a condition here
            # E-step - compute auxilliary function Q
            # --------------------------------------

            # initialise the backward values
            for i in range(N):
                backward[T-1, i] = 1.0 / n

            # inductively calculate remaining backward values
            for t in range(T-2, 1, -1):
                x = self._symbol_map[symbols[t+1]] # if unseen?
                for i in range(n):
                    backward[t, i] = 0
                    for j in range(n):
                        backward[t, i] += A[i, j] * B[j, x] * backward[t+1, j]

            # calculate the forward values
            for t in range(T):
                symbol = symbols[t]
                x = self._symbol_map[symbol] # if unseen?
                if t == 0:
                    # initialise the forward values
                    for i in range(N):
                        forward[t, i] = pi[i] * B[i, x]
                else:
                    for i in range(N):
                        for j in range(N):
                            forward[t, i] += forward[t-1, j] * A[i, j]
                        forward[t, i] *= B[i, x]

            # calculate gamma values
            norm = 0
            for t in range(T):
                for i in range(N):
                    norm += forward[t, i]

            for t in range(1, T):
                x = self._symbol_map[symbols[t]] # if unseen?
                for i in range(N):
                    for j in range(N):
                        gamma[t, i, j] = forward[t-1, i] * A[i, j] * B[j, x] \
                                         * backward[t, j] / norm

            # calculate Q
            #hmmm ... this gets tricky

            # M-step - compute new A, B, pi values

    def train_supervised(self, symbol_sequences, state_sequences, smoothing=1):
        """
        Supervised training maximising the joint probability of the symbol and
        state sequences. This is done with the MLE for the transition,
        emission and prior probability matrices.
        """

        # Think that this is broken - see the demo. FIXME

        N = len(self._states)
        M = len(self._symbols)
        starting = zeros(N, Int) + smoothing
        transitions = zeros((N, N), Int) + smoothing
        outputs = zeros((N, M), Int) + smoothing

        for symbols, states in zip(symbol_sequences, state_sequences):
            T = len(symbols)
            lasts = -1
            for i in range(T):
                b = self._symbol_map[symbols[i]] # if unseen?
                s = self._state_map[states[i]]   # ditto
                if i == 0:
                    starting[s] += 1
                else:
                    transitions[lasts, s] += 1

                outputs[s, b] += 1
                lasts = s

        # normalise
        pi = starting / float(add.reduce(starting))
        A = zeros((N, N), Float64)
        B = zeros((N, M), Float64)

        for i in range(N):
            sum = 0
            for j in range(N):
                sum += transitions[i, j]
            for j in range(N):
                if sum != 0:
                    A[i, j] = transitions[i, j] / float(sum)
                else:
                    A[i, j] = 1.0 / N

            sum = 0
            for k in range(M):
                sum += outputs[i, k]
            for k in range(M):
                if sum != 0:
                    B[i, k] = outputs[i, k] / float(sum)
                else:
                    B[i, k] = 1.0 / M

        #print 'output alphabet'
        #print self._symbols
        #print 'states (tags)'
        #print self._states
        #print 'priors'
        #print pi
        #print 'transitions'
        #print A
        #print 'output probabilities'
        #print B

        return HMM(self._states, A, self._symbols, B, pi)

def demo():
    # example taken from page 381, Huang et al
    omega = [1, 2, 3]
    A = array([[0.6, 0.2, 0.2], [0.5, 0.3, 0.2], [0.4, 0.1, 0.5]], Float64)
    #B = array([[1.0, 0, 0], [0, 1, 0], [0, 0, 1]], Float64)
    B = array([[0.7, 0.1, 0.2], [0.1, 0.6, 0.3], [0.3, 0.3, 0.4]], Float64)
    pi = array([0.5, 0.2, 0.3], Float64)
    O = ['up', 'down', 'unchanged']

    model = HMM(states=omega, transitions=A, outputs=B, symbols=O, priors=pi)

    for test in [['up'] * 2, ['up'] * 5, ['up', 'down', 'up'], ['down'] * 5, 
                ['unchanged'] * 5 + ['up']]:
        print test, model.probability(test)
        print test, model.best_path(test)

def _split_tagged_tokens(tagged_tokens):
    from nltk.set import MutableSet
    from nltk.stemmer.porter import *
    words = []
    ws = []
    word_set = MutableSet()
    tags = []
    ts = []
    tag_set = MutableSet()
    stemmer = PorterStemmer()
    for token in tagged_tokens:
        w = token.type().base().lower() # make them lower case
        w = stemmer.stem_word(w) # oh, and stem them too
        #w = token.type().base()
        t = token.type().tag()
        word_set.insert(w)
        tag_set.insert(t)
        ws.append(w)
        ts.append(t)
        if t == '.':
            words.append(ws)
            ws = []
            tags.append(ts)
            ts = []

    return words, word_set.elements(), tags, tag_set.elements()

def demo_pos_supervised():
    from nltk.corpus import brown
    from nltk.tagger import TaggedTokenizer
    print 'Loading data from Brown corpus...'
    tagged_tokens = []
    for item in brown.items()[:5]:
        tagged_tokens.extend(brown.tokenize(item))
        
    words, word_set, tags, tag_set = _split_tagged_tokens(tagged_tokens)

    word_set.sort()
    tag_set.sort()

    print 'output alphabet', `word_set`[:50], '...'
    print 'state labels   ', `tag_set`[:50], '...'

    print 'Training HMM...'

    #print 'training data:'
    #print zip(words[1:], tags[1:])

    trainer = HMMTrainer(tag_set, word_set)
    hmm = trainer.train_supervised(words[100:], tags[100:], 0.1)

    print hmm
    print 'Testing...'
    
    for ws, ts in zip(words[:3], tags[:3]):
        print ws
        print 'HMM >>>'
        print hmm.best_path(ws)
        print 'CORRECT >>>'
        print ts
        print '-' * 60

    count = correct = 0
    for ws, ts in zip(words[:100], tags[:100]):
        print '.',
        pts = hmm.best_path(ws)
        for t, pt in zip(ts, pts):
            count += 1
            if t == pt:
                correct += 1

    print 'accuracy over first', count, 'tokens %.1f' % (100.0 * correct / count)
    
if __name__ == '__main__':
    #demo()
    demo_pos_supervised()

