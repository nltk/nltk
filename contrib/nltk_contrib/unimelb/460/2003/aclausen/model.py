import nltk.probability
import nltk.tagger
from math import log, pow 


# read_frequent_words takes builds a dictionary from words listed
# in a frequent words file ('top.txt').  Count is the number
# of words to read from the file.

def read_frequent_words(count):
	f = open('top.txt', 'r')
	words = f.read()
	f.close()

	words = words.lower().split('\n')	
	dict = {}
	for word in words[:count]:
		dict[word] = 1
	

	return dict


# LanguageModelI provides the interface for classes implementing
# diferent types of language models. 

class LanguageModelI:

	# train the model using some machine learing technique
        def train(self, tokens):
                assert 0, "Abstract class"

	# derive the information content of a text (tokens) with
	# respect to this model.

	def info_content(self, tokens):
                assert 0, "Abstract class"

	# derive the relative entropy between a model and  the model
	# generated by a text (tokens). 

	def rel_entropy(self, tokens):
                assert 0, "Abstract class"

#---------------------------------------------------------------------------
# PartialContextTree provides a data structure for storing information
# about words and contexts in a language model. The root of the tree
# represents a zero order context, at depth one in the tree we have
# a first order context model etc.
# Each node of the tree is a frequency distribution of words that 
# have occured with that context.
#--------------------------------------------------------------------------- 

class PartialContextTree(nltk.probability.FreqDist):
        def __init__(self):
                nltk.probability.FreqDist.__init__(self)
                self.children = {}

        # records a sample at all levels of the tree.  "context" is the
        # remaining context left to give to the children.
	# Note: context is the context of word outcome reversed.
	# The context[0] is the word just seen before outcome.

        def inc(self, context, outcome, freq_word_list=None):
                nltk.probability.FreqDist.inc(self, outcome)
                if len(context) > 0:
			if freq_word_list is not None:
				if not freq_word_list.has_key(context[0]):
					return
			child = self.children.get(context[0])
			if not child:
				child = PartialContextTree()
				self.children[context[0]] = child
			child.inc(context[1:], outcome)


        # gets the deepest node that matches context.  This is akin to a
        # PATRICIA tree search.
        def get(self, context, depth=0):
                if len(context) == 0:
                        return (self, depth)
                child = self.children.get(context[0])
                if child:
                        return child.get(context[1:], depth + 1)
                return (self, depth)


# NGramModel implements an n-gram language model. order is the
# size of context considered.  The guts of the model is kept
# as a frequency distribution over n-grams of length order.

class NGramModel(LanguageModelI):
        def __init__(self, order):
                self.order = order
		self.ngram_fdist = nltk.probability.FreqDist() 

	# train the model using a sliding window of context.
	# Observations are built up in ngram_fdist

	def train(self, tokens):
		if len(tokens) < self.order:
			return  
		tokens = [t.type() for t in tokens]
		while len(tokens) >= self.order + 1:
                        context = tokens[-self.order - 1:]
                        context.reverse()
			self.ngram_fdist.inc(tuple(context))
                        tokens.pop()
		self.update_entropy()

	# updates the entropy of a model!
	# Entropy of P(X) = \sum_{x} -P(x)log(P(X))
	def update_entropy(self):
		self.entropy = 0
		for sample in self.ngram_fdist.samples():
			pr = self.ngram_pr(sample)
			self.entropy += pr * log(pr) / log(2)

	# Compute the relative entropy between the current model (self)
	# and another model, alt.
	# optimized for the case where "alt.ngram_fdist.samples()" is small.
	# Doesn't really compute relative entropy... in fact, relative
	# entropy *never* exists!  (See report for a discussion)

	def rel_entropy(self, alt):
		rel_ent = 0 
		for sample in alt.ngram_fdist.samples():
			s_pr = self.ngram_pr(sample)
			a_pr = alt.ngram_pr(sample)
			rel_ent += a_pr*(log(a_pr) - log(s_pr))/log(2) 
		return rel_ent

	# Compute the probability of a particular ngram in the current model
	# If the ngram hasn't been seen by the model before
	# computer the probability as that of backing off times the 
	# with additional cost of encoding the ngram in full.
	# We use PPM Method X to deal with this instance of the 
	# zero frequency problem. That is assume the probability
	# of an unseen ngram occuring as (1+Nr(1)/(1+Nr(1)+N)
	def ngram_pr(self, ngram):
		fdist = self.ngram_fdist
		ngram_count = fdist.count(ngram) 

		denominator = 1.0 + fdist.Nr(1) + fdist.N()

		if ngram_count > 0:
			return ngram_count/denominator
		else:
              		pr = (1 + fdist.Nr(1))/denominator
			ngram_len = 0		
			for str in ngram:
				ngram_len += len(str) 	
			return  pr * pow(1.0/8, ngram_len)	
	

# PartialContextModel builds a language model up based
# on a PartialContextTree.

class PartialContextModel(LanguageModelI):

	# freq_word_count is the number of words we take from
	# a frequent words list.  This effectively
	# controls how much the partial context tree can grow.

        def __init__(self, max_order, freq_word_count):
                self.max_order = max_order
		self.freq_word_list = read_frequent_words(freq_word_count)
                self.tree = PartialContextTree()

	# train the model by building up the PartialContextTree
        def train(self, tokens):
		tokens = [t.type() for t in tokens]
		while len(tokens) > 0:
                        context = tokens[-self.max_order-1:-1]
                        context.reverse()
                        outcome = tokens[-1]
                        self.tree.inc(context, outcome, self.freq_word_list)
                        tokens.pop()

	# Calculate the information content of a list of tokens
	# given the current model. Tokens are in reading order.

	def info_content(self, tokens):
		tokens = [t.type() for t in tokens]
		i = 0  
		context = []	
		for token in tokens:
			i += - log(self.sym_pr(context, token))/log(2)	
			context.insert(0, token)
			if len(context) > self.max_order:
				del context[-1]
		return i 

	# Calculate the probability of a word (outcome), with
	# a given context according to the current model.
	# Use PPM Method X to deal words (outcome) that haven't
	# been seen at any level of the PartialContextTree

	def sym_pr(self, context, outcome):
                (node, depth) = self.tree.get(context, 0)
		denominator = 1.0 + node.Nr(1) + node.N()
		if outcome in node:	# Seen in this context
			return node.count(outcome) / denominator
		elif depth == 0: 	# Never seen outcome before
              		pr = (1 + node.Nr(1)) / denominator
			return pr * pow(1.0/8, len(outcome))
		else:			# backoff up a level
              		pr = (1 + node.Nr(1)) / denominator
			return pr * self.sym_pr(context[:depth-1], outcome) 		

