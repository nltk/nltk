CIS530: Introduction to Computational Linguistics 
Final Project: Text Categorization Using KNN Algorithm

                                        Hsiao-chun Tang, tanghc@seas.upenn.edu

Problem Statement
=================
Classification is the task of assigning objects from a universe to two or more
classes. The goal in text categorization is to classify the topic or theme of
a document. In order to do the classification, we will need a training set of
texts, each labeled with one or more classes. After the training procedure, we
can classify texts in the test set, which includes a few unclassified texts, 
and check for prediction accuracy.

In this project I emphasize my works on classifying the articles from twenty
newsgroups in NLTK. First I choose a few newsgroups, pick some articles from
each of them as training set and some others from each of them as test set.
The articles should be classified to its newsgroup name. This is the special
case of text classification where each document only belongs to one class, and
there are more than two categories (i.e., binary classification is not very
suitable in this case).

K Nearest Neighbor classification is a remarkably simple algorithm which deals
the above problem surprisingly well. Its idea is easy: classify the unknown
according to the most similar classified document(s). 

The goal of this project is to implement the kNN algorithm with the existing
NLTK (1.4.2) framework, test its performance upon twenty newsgroups, observe
its behavior with different parameters, and compare the results with Naive
Bayes classification.

Algorithm
=========
[ Feature Selection ]
The complexity of kNN is in finding a good measure of similarity. One commonly
used technique is to measure cosine similarity in the vector space. Namely, we 
convert each document to a vector in a high-dimensional space where each
dimension represents a word (or a token). Then we can use inner product to
measure the cosine value of two vectors, representing the similarity of two
documents. This process is called the feature selection.
The question is how to weight words in the vector space model. One widely used
scheme is called tf-idf (term frequency and inversed document frequency). For
each term in a given document, we count its occurences (tf) in that document as
well as its occurrences in all documents (df), and use a function to combine 
those values together. The idea behind the algorithm is if a term occurs in a
document multiple times then it's a more important feature, however if a term
occurs in a lot of documents then it might just be a commonly used word (e.g. 
determiners, auxiliaries, etc.). There are a couple different approaches for
the function mentioned above including natural, logarithm and augmented schemes,
which are all implemented in the project.

[ k Nearest Neighbor Classification ]
After we represent each document in a vector, we can calculate the cosine value
to measure the pairwise similarity of all documents. The kNN algorithm is that
for each unclassified document, we find k documents in the training set which
has the greatest similarity with it, and picks the one category with the 
greatest summation of similarity values. The algorithm is written as:

    If we want to classify document y based on training set X,
    we fist find k classified tokens with greatest similarity:
        set A = {x | x in X, sim(x, y) > sim(x', y) for all x' in (X-A)  }
        with size(A) = k.
    The score for y classified to class C is:
        score(C) = sum { sim(x, y) | x in A, x is classified to C }
    We simply classify y to:
        class(y) = argmax score(C)

If k is set to be one, the most similar document in the training set single-
handedly decide the class of the unclassified document.

Implementation
==============
[ NLTK Classification Framework ]
NLTK classification framework has been changed a lot since 1.1, 1.2 to 1.4. 
In NLTK 1.4.2, there are primarily two packages dealing with classification
works:
  - nltk.feature: Includes feature detector and feature encoder classes which
                  does the feature selection. Several schemes were implemented.
  - nltk.classifier: Includes classifier trainer and classifier classes which
                     does the classification work. Only Naive Bayes and Maximum
                     Entropy algorithms were implemented.

The object used in the above framework is nltk.token.Token. There are different
properties in a Token including TEXT (representing the raw document), CLASS
(the given category for documents in training set), FEATURES (the detected
features), and FEATURE_VECTOR (the encoded features).

To be more clear, the classification flow should be as following:

      Read the document
              |
              | Token(TEXT)
              V
      Assign the category for training tokens
              |
              | Token(TEXT, [CLASS])
              V
      Tokenize the document, and do optional stemming
              |
              | Token(TEXT, [CLASS], SUBTOKENS)
              V
      Use FeatureDetector to get features
              |
              | Token(TEXT, [CLASS], FEATURES)
              V
      Use FeatureEncoder to create feature vector
              |
              | Token(TEXT, [CLASS], FEATURES, FEATURE_VECTOR)
              V
      Construct the ClassifierTrainer with training tokens
              |
              | Classifier
              V
      Classify the test tokens

[ Problems in NLTK 1.4.2 ]
Unfortunately, the existing classification framework in NLTK 1.4.2 does not 
work correctly, and the documentation is unclear either. 
First of all, module nltk.feature.document, which is supposed to do the feature
detection task for documents, cannot execute at all due to an undeclared 
variable. Second of all, module nltk.classifier.naivebayes runs incorrectly and
only supports word-level classification but not document-level classification
(i.e., it does not support bag-of-words features). Besides, some demo code in
those two packages are even not runnable due to NLTK version change.

Hence, before implementing kNN classifier, I needed to create a new module with
document feature detection and encoding work. And after implementing kNN 
classifier, because I wanted to compare the result with another classification
method, I implemented the Naive Bayes classifier for document classification 
as well.

I chose nltk.corpus.twenty_newsgroups as the target corpus. Unfortunately
enough, even that corpus cannot run correctly under existing NLTK code. So I
had to write another piece of code for reading the newsgroup documents.

[ Modules and Classes ]
There are four modules in the project:
  - documentfeature: handles the document-level feature selection.
    * class DocumentFeatureDetector implements nltk.feature.FeatureDetectorI:
      Use tokenizing and stemming to establish a bag of words as the features.
    * class TFIDFFeatureEncoder implments nltk.feature.FeatureEncoderI:
      Implement the tf-idf scheme to establish a feature vector as a sparse
      list from the bag of words.
  - knnclassifier: implements the kNN algorithm for text categorization.
    * class KNNClassifier implements nltk.classifier.ClassifierI:
      Deal with vector similarity measuring and nearst neighbor picking.
  - naivebayes: implements the Naive Bayes algorithm for text categorization.
    * class NaiveBayesDocumentClassifierTrainer implements 
        nltk.classifier.ClassifierTrainerI:
      Provide train method which helps creating the classifier.
    * class NaiveBayesDocumentClassifier implements nltk.classifier.ClassifierI:
      Classify the document according to probability.
  - newsgroups: demo program, reads articles from each newsgroup, create the
    training and test set, construct the classifier, and verify the results.

For detailed class/method description please refer to the source code comments.

Result Analysis
===============
This section examines the performance of the classification under different
parameter settings. I chose regular expression tokenizer which only accounts
for alphabets and numbers and Porter Stemmer to do the stemming.

[ tf-idf schemes ]
The first step of kNN classification is to choose the best tf-idf scheme. 
There are three term schemes: natural, logarithm, augmented and two document
schemes: natural, logarithm. Each combination is tested against different set
of newsgroups. 

Settings:
  - k = 3
  - 1000 training articles and 100 test articles gathered from each group.
Results:
  +-------------+-----------------+---------------------+--------------------+
  | term scheme | document scheme | first 10 newsgroups | last 10 newsgroups |
  +-------------+-----------------+---------------------+--------------------+
  |   natural   |     natural     |         67%         |         81%        |
  +-------------+-----------------+---------------------+--------------------+
  |   natural   |    logarithm    |         72%         |         78%        |
  +-------------+-----------------+---------------------+--------------------+
  |  logarithm  |     natural     |         66%         |         77%        |
  +-------------+-----------------+---------------------+--------------------+
  |  logarithm  |    logarithm    |         82%         |         88%        |
  +-------------+-----------------+---------------------+--------------------+
  |  augmented  |     natural     |         67%         |         79%        |
  +-------------+-----------------+---------------------+--------------------+
  |  augmented  |    logarithm    |         84%         |         89%        |
  +-------------+-----------------+---------------------+--------------------+

We can see that though results from different sets of newsgroups vary (because
some newsgroups are more close to each other), the behavior of each scheme show
the similar trend. Augmented term scheme (term frequency divided by maximum 
term frequency, then take average with 1) together with logarithm document
frequency (logarithm value of number of documents divided by the number of the
occurrences of the word) have the best results. Natural schemes do not work so
well because they value high occurrences too much.

[ Number of neighbors (k) ]
The second most important parameter of classification is to choose the k value
in the kNN classification. Some small prime numbers are tested on different sets
of newsgroups.

Settings:
  - scheme: augmented + logarithm
  - 1000 training articles and 100 test articles gathered from each group.
Results:
  +-------------+---------------------+--------------------+
  |      k      | first 10 newsgroups | last 10 newsgroups |
  +-------------+---------------------+--------------------+
  |      1      |         85%         |         84%        |
  +-------------+---------------------+--------------------+
  |      2      |         85%         |         84%        |
  +-------------+---------------------+--------------------+
  |      3      |         84%         |         89%        |
  +-------------+---------------------+--------------------+
  |      5      |         90%         |         90%        |
  +-------------+---------------------+--------------------+
  |      7      |         88%         |         89%        |
  +-------------+---------------------+--------------------+
  |     11      |         89%         |         92%        |
  +-------------+---------------------+--------------------+

As we can see, before k = 5, the accuracy grows with k. After k > 5, the 
accuracy stays at the similar level. Choosing k as 5 might be appropriate
for about 100 training documents in each category.

[ Number of training documents ]
The number of training documents influences the accuracy of classification.
Clearly, if there are too few training documents, then there's higher 
probability that the "most similar" documents are not accurate.

Settings:
  - scheme: augmented + logarithm
  - k = 5
  - The same 100 test articles are gathered from each group.
Resuls:
  +--------------------------+---------------------+--------------------+
  | total training documents | first 10 newsgroups | last 10 newsgroups |
  +--------------------------+---------------------+--------------------+
  | 100 (10 from each group) |         63%         |         67%        |
  +--------------------------+---------------------+--------------------+
  | 500 (50 from each group) |         81%         |         81%        |
  +--------------------------+---------------------+--------------------+
  | 1000(100 from each group)|         89%         |         87%        |
  +--------------------------+---------------------+--------------------+
  | 2000(200 from each group)|         86%         |         87%        |
  +--------------------------+---------------------+--------------------+

We can see from the result that if we need a good classification result, at
least 100 training documents from each group is required. However, supplying
more than that does not necessary increase the performance.

[ Performance vs. Naive Bayes ]
Finally it's time to compare the performance with Naive Bayes classification.

Settings:
  - scheme: augmented + logarithm
  - k = 5
  - 100 test articles are gathered from each group.
Resuls:
  +----------------------------------+-----------+-----------+-------------+
  |                                  |# training |    kNN    | Naive Bayes |
  |                                  | documents |           |             |
  +----------------------------------+-----------+-----------+-------------+
  | 2 similar newsgroups             |           |           |             |
  | (ibm.pc.hardware / mac.hardware) |    400    |    90%    |     82%     |
  +----------------------------------+-----------+-----------+-------------+
  | 2 diverse newsgroups             |           |           |             |
  | (alt.atheism / ms-windows.misc)  |    400    |    98%    |    100%     |
  +----------------------------------+-----------+-----------+-------------+
  | 5 random newsgroups              |   1000    |    85%    |     77%     |
  +----------------------------------+-----------+-----------+-------------+
  | 10 random newsgroups             |   1000    |    84%    |     69%     |
  +----------------------------------+-----------+-----------+-------------+
  | all 20 newsgroups                |   2000    |    82%    |     71%     |
  +----------------------------------+-----------+-----------+-------------+

As we can see, kNN almost beats Naive Bayes in all the experiments. Even if we
use all 20 newsgroups (which means random guessing would only have 5% accuracy),
kNN still achieves an impressive accuracy over 80%. However there are definitely
drawback in kNN: its speed. Each classification requires pairwise similairy
calculation. At the last experiment (100 training documents and 5 additional 
test documents from each newsgroup), Naive Bayes only took 8 seconds training
plus classifying, while kNN took 88 seconds for classification.

Conclusion
==========
Despite its simplicity, kNN is a surprisingly well classification method for 
text categorization. By choosing appropriate weighting scheme (tf-idf with 
augmented term frequency and logarithm document frequency) and k (set to 5),
the results can be pretty satisfying: over 90% accuracy for binary 
classification and over 80% for 20 newsgroups.

However this system is not very scalable: with increasing number of training
documents, both the required memory and the classification time are increased
linearly or even more. This system might not work very well with a very large
corpus.

There are a few ways to fix this: we can store some training documents' feature
vector information in a file or database, because we don't need them to be in
the memory at the same time because they are matched sequentially. Latent
Semantic Indexing (LSI) reduces dimensionality reduction and therefore can
lower memory requirement and accelerate matching. These are both possible
enhancements in the future.
